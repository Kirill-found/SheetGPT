"""
Simple GPT Processor v1.0.0 - –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SIMPLE GPT PROCESSOR v1.0              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  1. Schema Extraction (–ª–æ–∫–∞–ª—å–Ω–æ, 0 tokens)          ‚îÇ
‚îÇ     ‚Üí –¢–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫, —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è             ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  2. GPT-4o –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Pandas –∫–æ–¥ (–í–°–ï–ì–î–ê)           ‚îÇ
‚îÇ     ‚Üí –ë–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –±–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤              ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  3. Execute + Self-Correction (–¥–æ 3 –ø–æ–ø—ã—Ç–æ–∫)        ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  4. POST-VALIDATION                                 ‚îÇ
‚îÇ     ‚Üí GPT –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞            ‚îÇ
‚îÇ     ‚Üí –ï—Å–ª–∏ –Ω–µ—Ç ‚Üí retry —Å —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º                 ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- GPT –ø–æ–Ω–∏–º–∞–µ—Ç –ª—é–±—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- Self-correction –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
- Post-validation –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from openai import AsyncOpenAI
import os
import time
import logging
import re
import ast

from .schema_extractor import SchemaExtractor, get_schema_extractor

logger = logging.getLogger(__name__)


class SimpleGPTProcessor:
    """
    –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–∞ –±–∞–∑–µ GPT-4o.
    –í—Å—ë —á–µ—Ä–µ–∑ LLM, –±–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
    """

    MODEL = "gpt-4o"  # Best quality
    MAX_RETRIES = 2

    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏
    ALLOWED_IMPORTS = {'pandas', 'pd', 'numpy', 'np', 'datetime', 'timedelta', 're', 'math'}

    # –ó–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    FORBIDDEN_PATTERNS = [
        r'\bexec\b', r'\beval\b', r'\bcompile\b',
        r'\b__\w+__\b', r'\bopen\b', r'\bfile\b',
        r'\bos\b', r'\bsys\b', r'\bsubprocess\b',
        r'\brequests\b', r'\burllib\b', r'\bsocket\b', r'\bpickle\b',
    ]

    SYSTEM_PROMPT = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤ Python/Pandas. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞–≤–∞—Ç—å –ì–õ–£–ë–û–ö–ò–ô, –ò–°–ß–ï–†–ü–´–í–ê–Æ–©–ò–ô –∞–Ω–∞–ª–∏–∑.

–ó–ê–î–ê–ß–ê: –ù–∞–ø–∏—à–∏ Python –∫–æ–¥ –¥–ª—è –ü–û–õ–ù–û–ì–û –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ü–†–ê–í–ò–õ–ê:
1. DataFrame —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `df`
2. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û pandas, numpy, datetime, math
3. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–∏ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `result`
4. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–æ–∑–¥–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `explanation` —Å –î–ï–¢–ê–õ–¨–ù–´–ú –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ú –æ—Ç–≤–µ—Ç–æ–º
5. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π print(), –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏—Å–≤–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
6. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π NaN –∑–Ω–∞—á–µ–Ω–∏—è (dropna() –∏–ª–∏ fillna())
7. –î–ª—è —á–∏—Å–µ–ª: pd.to_numeric(df[col], errors='coerce')

‚ö†Ô∏è –ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û - –ì–õ–£–ë–ò–ù–ê –ê–ù–ê–õ–ò–ó–ê:
- –ù–ï –¥–∞–≤–∞–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π!
- –í–°–ï–ì–î–ê –≤–∫–ª—é—á–∞–π: —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –≤—ã–≤–æ–¥—ã
- –í–°–ï–ì–î–ê –æ–±—ä—è—Å–Ω—è–π –ß–¢–û —ç—Ç–æ –∑–Ω–∞—á–∏—Ç –∏ –ü–û–ß–ï–ú–£ —ç—Ç–æ –≤–∞–∂–Ω–æ
- –ú–∏–Ω–∏–º—É–º 5-7 –ø—É–Ω–∫—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ª—é–±–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞

–ö–†–ò–¢–ò–ß–ù–û - –§–û–†–ú–ê–¢ explanation (–°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –û–¢–í–ï–¢):
–ò—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏:
- **–ñ–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç** –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –≤—ã–≤–æ–¥–æ–≤
- –°–ø–∏—Å–∫–∏ —Å ‚Ä¢ –∏–ª–∏ —Ü–∏—Ñ—Ä–∞–º–∏ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–π
- –†–∞–∑–¥–µ–ª—è–π –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π
- –≠–º–æ–¥–∑–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–µ–∫—Ü–∏–π (üìäüìàüí°üîçüí∞üèÜ)

–®–ê–ë–õ–û–ù–´ explanation:

1. –î–ª—è –°–†–ê–í–ù–ï–ù–ò–Ø –ø–µ—Ä–∏–æ–¥–æ–≤/–∫–∞—Ç–µ–≥–æ—Ä–∏–π (—Å—Ä–∞–≤–Ω–∏, —Ä–∞–∑–Ω–∏—Ü–∞, vs):
```
**üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: [–ü–µ—Ä–∏–æ–¥1] vs [–ü–µ—Ä–∏–æ–¥2]**

üìà –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:
‚Ä¢ [–ü–µ—Ä–∏–æ–¥1]: [—Å—É–º–º–∞] —Ä—É–±. ([N] –æ–ø–µ—Ä–∞—Ü–∏–π)
‚Ä¢ [–ü–µ—Ä–∏–æ–¥2]: [—Å—É–º–º–∞] —Ä—É–±. ([N] –æ–ø–µ—Ä–∞—Ü–∏–π)

üìâ –î–∏–Ω–∞–º–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π:
‚Ä¢ –ê–±—Å–æ–ª—é—Ç–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞: [X] —Ä—É–±.
‚Ä¢ –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: [Y]% ([—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ])
‚Ä¢ –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞: [Z] —Ä—É–±.

üîç –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä:
‚Ä¢ –¢–æ–ø –ø–æ–∑–∏—Ü–∏—è [–ü–µ—Ä–∏–æ–¥1]: [–Ω–∞–∑–≤–∞–Ω–∏–µ] ‚Äî [—Å—É–º–º–∞]
‚Ä¢ –¢–æ–ø –ø–æ–∑–∏—Ü–∏—è [–ü–µ—Ä–∏–æ–¥2]: [–Ω–∞–∑–≤–∞–Ω–∏–µ] ‚Äî [—Å—É–º–º–∞]
‚Ä¢ –ù–∞–∏–±–æ–ª—å—à–∏–π —Ä–æ—Å—Ç –ø–æ–∫–∞–∑–∞–ª–∞: [–∫–∞—Ç–µ–≥–æ—Ä–∏—è] +[X]%
‚Ä¢ –ù–∞–∏–±–æ–ª—å—à–µ–µ –ø–∞–¥–µ–Ω–∏–µ: [–∫–∞—Ç–µ–≥–æ—Ä–∏—è] -[X]%

üí° –í—ã–≤–æ–¥—ã:
‚Ä¢ [–ì–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥ –æ —Ç—Ä–µ–Ω–¥–µ]
‚Ä¢ [–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π]
‚Ä¢ [–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è]
```

2. –î–ª—è "–∫—Ç–æ –ª—É—á—à–∏–π/—Ö—É–¥—à–∏–π" (—Ä–µ–π—Ç–∏–Ω–≥):
```
**üèÜ –õ–∏–¥–µ—Ä: [–ò–º—è]**

üìä –ü–æ–ª–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥:
1. [–ò–º—è1]: [—Å—É–º–º–∞] —Ä—É–±. ([N] —Å–¥–µ–ª–æ–∫, —Å—Ä. —á–µ–∫ [X])
2. [–ò–º—è2]: [—Å—É–º–º–∞] —Ä—É–±. ([N] —Å–¥–µ–ª–æ–∫, —Å—Ä. —á–µ–∫ [X])
3. [–ò–º—è3]: [—Å—É–º–º–∞] —Ä—É–±. ([N] —Å–¥–µ–ª–æ–∫, —Å—Ä. —á–µ–∫ [X])

üìà –ê–Ω–∞–ª–∏–∑ –ª–∏–¥–µ—Ä–∞:
‚Ä¢ –î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ: [X]%
‚Ä¢ –û—Ç—Ä—ã–≤ –æ—Ç 2-–≥–æ –º–µ—Å—Ç–∞: [Y] —Ä—É–±. ([Z]%)
‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: [X] —Ä—É–±. (vs —Å—Ä–µ–¥–Ω–∏–π –ø–æ –≤—Å–µ–º [Y])

üí° –ü–æ—á–µ–º—É –ª–∏–¥–∏—Ä—É–µ—Ç:
‚Ä¢ [–ü—Ä–∏—á–∏–Ω–∞ 1: –æ–±—ä—ë–º/—á–∞—Å—Ç–æ—Ç–∞/—Ä–∞–∑–º–µ—Ä —Å–¥–µ–ª–æ–∫]
‚Ä¢ [–ü—Ä–∏—á–∏–Ω–∞ 2: —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞]
```

3. –î–ª—è "–ø–æ—á–µ–º—É?" (–≥–ª—É–±–æ–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ):
```
**üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω: [—Ç–µ–º–∞]**

üìä –§–∞–∫—Ç—ã –∏ —Ü–∏—Ñ—Ä—ã:
‚Ä¢ [–§–∞–∫—Ç 1 —Å —á–∏—Å–ª–∞–º–∏]
‚Ä¢ [–§–∞–∫—Ç 2 —Å —á–∏—Å–ª–∞–º–∏]
‚Ä¢ [–§–∞–∫—Ç 3 —Å —á–∏—Å–ª–∞–º–∏]

üìà –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã:
‚Ä¢ [–§–∞–∫—Ç–æ—Ä 1]: –≤–∫–ª–∞–¥ [X]%
‚Ä¢ [–§–∞–∫—Ç–æ—Ä 2]: –≤–∫–ª–∞–¥ [X]%

üîé –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä:
‚Ä¢ [–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö]
‚Ä¢ [–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏]

üí° –ó–∞–∫–ª—é—á–µ–Ω–∏–µ:
‚Ä¢ [–ì–ª–∞–≤–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞]
‚Ä¢ [–ß—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å]
```

4. –î–ª—è "—Å–∫–æ–ª—å–∫–æ/–∫–∞–∫–∞—è —Å—É–º–º–∞" (–¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç):
```
**üí∞ –†–µ–∑—É–ª—å—Ç–∞—Ç: [–ß–∏—Å–ª–æ/–°—É–º–º–∞]**

üìã –ö–∞–∫ –ø–æ—Å—á–∏—Ç–∞–Ω–æ:
‚Ä¢ –ú–µ—Ç–æ–¥: [–æ–ø–∏—Å–∞–Ω–∏–µ]
‚Ä¢ –ó–∞–ø–∏—Å–µ–π —É—á—Ç–µ–Ω–æ: [N] –∏–∑ [M]

üìä –ö–æ–Ω—Ç–µ–∫—Å—Ç:
‚Ä¢ –î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ: [X]%
‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ —Å—Ä–µ–¥–Ω–∏–º: [+/-X]%

üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
‚Ä¢ [–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç –¥–ª—è –±–∏–∑–Ω–µ—Å–∞]
```

–í–ê–ñ–ù–û - –ü–û–ù–ò–ú–ê–ô –ù–ê–ú–ï–†–ï–ù–ò–ï:
- "—Å—Ä–∞–≤–Ω–∏/—Ä–∞–∑–Ω–∏—Ü–∞/vs" -> –î–ï–¢–ê–õ–¨–ù–û–ï —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏, –¥–∏–Ω–∞–º–∏–∫–æ–π –∏ –≤—ã–≤–æ–¥–∞–º–∏
- "–∫–∞–∫–∏–µ/–∫–∞–∫–æ–π/—á—Ç–æ" -> –°–ü–ò–°–û–ö —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
- "—Å–∫–æ–ª—å–∫–æ" -> —á–∏—Å–ª–æ + –∫–æ–Ω—Ç–µ–∫—Å—Ç + –¥–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ
- "–ø–æ—á–µ–º—É?" -> –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
- "—Ç–æ–ø N" -> —Ä–µ–π—Ç–∏–Ω–≥ + –∞–Ω–∞–ª–∏–∑ –ª–∏–¥–µ—Ä–æ–≤ + –≤—ã–≤–æ–¥—ã

–ü–†–ò–ú–ï–†–´:

–ó–∞–ø—Ä–æ—Å: "–ö–∞–∫–æ–π –º–µ–Ω–µ–¥–∂–µ—Ä —Å–∞–º—ã–π –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–π"
```python
sales = df.groupby('–ú–µ–Ω–µ–¥–∂–µ—Ä')['–°—É–º–º–∞'].sum().sort_values(ascending=False)
result = sales.idxmax()
top3 = sales.head(3)
explanation = f"**–û—Ç–≤–µ—Ç: {result}**

"
explanation += "üìä –ü—Ä–æ–¥–∞–∂–∏ –ø–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º:
"
for i, (name, val) in enumerate(top3.items(), 1):
    explanation += f"‚Ä¢ {name}: {val:,.0f} —Ä—É–±.
"
if len(sales) > 3:
    explanation += f"‚Ä¢ ...–∏ –µ—â—ë {len(sales)-3} –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
"
explanation += f"
üí° {result} –ª–∏–¥–∏—Ä—É–µ—Ç —Å –æ—Ç—Ä—ã–≤–æ–º {sales.iloc[0] - sales.iloc[1]:,.0f} —Ä—É–±. –æ—Ç –≤—Ç–æ—Ä–æ–≥–æ –º–µ—Å—Ç–∞."
```

–ó–∞–ø—Ä–æ—Å: "–ø–æ—á–µ–º—É?" (–ø–æ—Å–ª–µ –≤–æ–ø—Ä–æ—Å–∞ –æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
```python
sales = df.groupby('–ú–µ–Ω–µ–¥–∂–µ—Ä')['–°—É–º–º–∞'].sum().sort_values(ascending=False)
counts = df.groupby('–ú–µ–Ω–µ–¥–∂–µ—Ä').size()
leader = sales.index[0]
leader_sum = sales.iloc[0]
leader_count = counts[leader]
second = sales.index[1] if len(sales) > 1 else None
explanation = f"**{leader} –ª–∏–¥–∏—Ä—É–µ—Ç** –ø–æ—Ç–æ–º—É —á—Ç–æ:

"
explanation += "üìà –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã:
"
explanation += f"‚Ä¢ –û–±—â–∞—è —Å—É–º–º–∞ –ø—Ä–æ–¥–∞–∂: {leader_sum:,.0f} —Ä—É–±.
"
explanation += f"‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {leader_count}
"
if second:
    diff = leader_sum - sales.iloc[1]
    pct = diff / sales.iloc[1] * 100
    explanation += f"‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞ —Å {second}: +{diff:,.0f} —Ä—É–±. (+{pct:.0f}%)
"
explanation += f"
üí° –í—ã—Å–æ–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –æ–±–µ—Å–ø–µ—á–µ–Ω—ã –±–æ–ª—å—à–∏–º –æ–±—ä—ë–º–æ–º –∏/–∏–ª–∏ –∫—Ä—É–ø–Ω—ã–º–∏ —Å–¥–µ–ª–∫–∞–º–∏."
result = sales.to_dict()
```

–ó–∞–ø—Ä–æ—Å: "–°–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥–∞–∂ —É –ò–≤–∞–Ω–æ–≤–∞"
```python
ivanov = df[df['–ú–µ–Ω–µ–¥–∂–µ—Ä'].str.contains('–ò–≤–∞–Ω–æ–≤', case=False, na=False)]
result = len(ivanov)
total = ivanov['–°—É–º–º–∞'].sum()
avg = ivanov['–°—É–º–º–∞'].mean()
explanation = f"**{result} –ø—Ä–æ–¥–∞–∂**

"
explanation += "üìã –î–µ—Ç–∞–ª–∏:
"
explanation += f"‚Ä¢ –û–±—â–∞—è —Å—É–º–º–∞: {total:,.0f} —Ä—É–±.
"
explanation += f"‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: {avg:,.0f} —Ä—É–±.
"
```

–í–æ–∑–≤—Ä–∞—â–∞–π –¢–û–õ–¨–ö–û –∫–æ–¥ –≤–Ω—É—Ç—Ä–∏ ```python ... ```
"""

    VALIDATION_PROMPT = """–¢—ã –ø—Ä–æ–≤–µ—Ä—è–µ—à—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {query}

–†–ï–ó–£–õ–¨–¢–ê–¢: {result}

–ó–ê–î–ê–ß–ê: –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º:
- "OK" - –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- "BAD" - –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ù–ï –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö, –Ω–µ —Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç)

–ü—Ä–∏–º–µ—Ä—ã:
- –ó–∞–ø—Ä–æ—Å "–∫–∞–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã" ‚Üí –†–µ–∑—É–ª—å—Ç–∞—Ç ["–¢–µ–ª–µ—Ñ–æ–Ω", "–ù–æ—É—Ç–±—É–∫"] ‚Üí OK
- –ó–∞–ø—Ä–æ—Å "–∫–∞–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã" ‚Üí –†–µ–∑—É–ª—å—Ç–∞—Ç 5 (—á–∏—Å–ª–æ) ‚Üí BAD (–Ω—É–∂–µ–Ω —Å–ø–∏—Å–æ–∫, –Ω–µ —á–∏—Å–ª–æ)
- –ó–∞–ø—Ä–æ—Å "—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥–∞–∂" ‚Üí –†–µ–∑—É–ª—å—Ç–∞—Ç 42 ‚Üí OK
- –ó–∞–ø—Ä–æ—Å "—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥–∞–∂" ‚Üí –†–µ–∑—É–ª—å—Ç–∞—Ç ["–ø—Ä–æ–¥—É–∫—Ç1", "–ø—Ä–æ–¥—É–∫—Ç2"] ‚Üí BAD (–Ω—É–∂–Ω–æ —á–∏—Å–ª–æ)

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û "OK" –∏–ª–∏ "BAD":
"""

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º–∏ (–Ω–µ –∞–Ω–∞–ª–∏–∑, –∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è)
    SORT_KEYWORDS = ['–æ—Ç—Å–æ—Ä—Ç–∏—Ä—É–π', '—Å–æ—Ä—Ç–∏—Ä—É–π', '—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '—É–ø–æ—Ä—è–¥–æ—á—å', '—É–ø–æ—Ä—è–¥–æ—á–∏', 'sort', 'order by']
    SORT_DESC_KEYWORDS = ['—É–±—ã–≤–∞–Ω', 'desc', 'z-a', '—è-–∞', '–±–æ–ª—å—à –∫ –º–µ–Ω—å—à', '–æ—Ç –±–æ–ª—å—à–µ–≥–æ', '–ø–æ —É–±—ã–≤–∞–Ω–∏—é']
    SORT_ASC_KEYWORDS = ['–≤–æ–∑—Ä–∞—Å—Ç', 'asc', 'a-z', '–∞-—è', '–º–µ–Ω—å—à –∫ –±–æ–ª—å—à', '–æ—Ç –º–µ–Ω—å—à–µ–≥–æ', '–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é']

    # Freeze keywords
    FREEZE_KEYWORDS = ['–∑–∞–º–æ—Ä–æ–∑—å', '–∑–∞–º–æ—Ä–æ–∑–∏—Ç—å', '–∑–∞–∫—Ä–µ–ø–∏', '–∑–∞–∫—Ä–µ–ø–∏—Ç—å', 'freeze', 'pin']
    UNFREEZE_KEYWORDS = ['—Ä–∞–∑–º–æ—Ä–æ–∑—å', '—Ä–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å', '–æ—Ç–∫—Ä–µ–ø–∏', '–æ—Ç–∫—Ä–µ–ø–∏—Ç—å', 'unfreeze', 'unpin']

    # Format keywords
    FORMAT_BOLD_KEYWORDS = ['–∂–∏—Ä–Ω', 'bold', '–≤—ã–¥–µ–ª–∏ –∂–∏—Ä–Ω—ã–º']
    FORMAT_HEADER_KEYWORDS = ['–∑–∞–≥–æ–ª–æ–≤', 'header', '—à–∞–ø–∫', '–ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É']
    FORMAT_COLOR_KEYWORDS = ['—Ü–≤–µ—Ç', 'color', '–ø–æ–∫—Ä–∞—Å—å', '–∑–∞–∫—Ä–∞—Å—å']

    # Chart keywords
    CHART_KEYWORDS = ['–¥–∏–∞–≥—Ä–∞–º–º', '–≥—Ä–∞—Ñ–∏–∫', 'chart', 'graph', '–ø–æ—Å—Ç—Ä–æ–π', '–≤–∏–∑—É–∞–ª–∏–∑', 'plot']
    CHART_TYPES = {
        # Line charts
        '–ª–∏–Ω–µ–π–Ω': 'LINE', 'line': 'LINE', '–ª–∏–Ω–∏—è': 'LINE', '—Ç—Ä–µ–Ω–¥': 'LINE',
        # Bar charts (horizontal)
        '–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω': 'BAR', 'bar': 'BAR',
        # Column charts (vertical bars) - default
        '—Å—Ç–æ–ª–±—á': 'COLUMN', 'column': 'COLUMN', '—Å—Ç–æ–ª–±–∏–∫': 'COLUMN', '–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º': 'COLUMN',
        # Pie charts
        '–∫—Ä—É–≥–æ–≤': 'PIE', 'pie': 'PIE', '–ø–∏—Ä–æ–≥': 'PIE', '–¥–æ–ª–µ–π': 'PIE', '–ø—Ä–æ—Ü–µ–Ω—Ç': 'PIE', '–¥–æ–ª—è–º–∏': 'PIE', '–¥–æ–ª–∏': 'PIE',
        # Area charts
        '–æ–±–ª–∞—Å—Ç—å': 'AREA', 'area': 'AREA', '–∑–∞–ª–∏–≤–∫': 'AREA',
        # Scatter plots
        '—Ç–æ—á–µ—á–Ω': 'SCATTER', 'scatter': 'SCATTER', '—Ä–∞–∑–±—Ä–æ—Å': 'SCATTER', '–∫–æ—Ä—Ä–µ–ª—è—Ü': 'SCATTER',
        # Combo charts
        '–∫–æ–º–±–∏–Ω–∏—Ä': 'COMBO', 'combo': 'COMBO', '—Å–º–µ—à–∞–Ω': 'COMBO',
    }

    # Conditional formatting keywords
    CONDITIONAL_FORMAT_KEYWORDS = ['—É—Å–ª–æ–≤–Ω', 'conditional', '–≥–¥–µ –±–æ–ª—å—à–µ', '–≥–¥–µ –º–µ–Ω—å—à–µ', '–≥–¥–µ —Ä–∞–≤–Ω–æ',
                                    '–±–æ–ª—å—à–µ —á–µ–º', '–º–µ–Ω—å—à–µ —á–µ–º', '–µ—Å–ª–∏ –±–æ–ª—å—à–µ', '–µ—Å–ª–∏ –º–µ–Ω—å—à–µ',
                                    '–∫—Ä–∞—Å–Ω—ã–º –≥–¥–µ', '–∑–µ–ª—ë–Ω—ã–º –≥–¥–µ', '–∑–µ–ª–µ–Ω—ã–º –≥–¥–µ', '–∂—ë–ª—Ç—ã–º –≥–¥–µ', '–∂–µ–ª—Ç—ã–º –≥–¥–µ',
                                    '–≤—ã–¥–µ–ª–∏ –≥–¥–µ', '–ø–æ–∫—Ä–∞—Å—å –≥–¥–µ', '–æ—Ç–º–µ—Ç—å –≥–¥–µ',
                                    # Additional patterns for color-based formatting
                                    '–ø–æ–∫—Ä–∞—Å—å –∫—Ä–∞—Å–Ω', '–ø–æ–∫—Ä–∞—Å—å –∑–µ–ª–µ–Ω', '–ø–æ–∫—Ä–∞—Å—å –∂—ë–ª—Ç', '–ø–æ–∫—Ä–∞—Å—å –∂–µ–ª—Ç',
                                    '–≤—ã–¥–µ–ª–∏ –∫—Ä–∞—Å–Ω', '–≤—ã–¥–µ–ª–∏ –∑–µ–ª–µ–Ω', '–≤—ã–¥–µ–ª–∏ –∂—ë–ª—Ç', '–≤—ã–¥–µ–ª–∏ –∂–µ–ª—Ç',
                                    '–µ—Å–ª–∏ —Ü–µ–Ω–∞', '–µ—Å–ª–∏ —Å—É–º–º–∞', '–µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ',
                                    '–∫—Ä–∞—Å–Ω—ã–º —è—á–µ–π–∫–∏', '–∑–µ–ª—ë–Ω—ã–º —è—á–µ–π–∫–∏', '–∑–µ–ª–µ–Ω—ã–º —è—á–µ–π–∫–∏',
                                    '–ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', '–ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏', '–∂–µ–ª—Ç—ã–º –ø—É—Å—Ç', '–∂—ë–ª—Ç—ã–º –ø—É—Å—Ç']

    # Pivot table / grouping keywords
    PIVOT_KEYWORDS = ['—Å–≤–æ–¥–Ω', 'pivot', '–≥—Ä—É–ø–ø–∏—Ä', 'group by', '–∞–≥—Ä–µ–≥–∏—Ä', '–∏—Ç–æ–≥–∏ –ø–æ', '—Å—É–º–º—ã –ø–æ',
                      '–ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏', '–ø–æ –º–µ–Ω–µ–¥–∂–µ—Ä', '–ø–æ —Ä–µ–≥–∏–æ–Ω', '–ø–æ –º–µ—Å—è—Ü', '–ø–æ –≥–æ–¥',
                      '—Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ', '–≤ —Ä–∞–∑—Ä–µ–∑–µ']

    # Aggregation functions
    AGG_FUNCTIONS = {
        '—Å—É–º–º': 'sum', 'sum': 'sum', '–∏—Ç–æ–≥': 'sum',
        '—Å—Ä–µ–¥–Ω': 'mean', 'avg': 'mean', 'average': 'mean',
        '–∫–æ–ª–∏—á–µ—Å—Ç–≤': 'count', 'count': 'count', '—á–∏—Å–ª–æ': 'count',
        '–º–∞–∫—Å': 'max', 'max': 'max', '–º–∞–∫—Å–∏–º—É–º': 'max',
        '–º–∏–Ω': 'min', 'min': 'min', '–º–∏–Ω–∏–º—É–º': 'min'
    }

    # Data cleaning keywords
    CLEAN_KEYWORDS = ['–æ—á–∏—Å—Ç', 'clean', '—É–¥–∞–ª–∏ –¥—É–±–ª–∏–∫', 'remove duplicate', '–¥—É–±–ª–∏–∫–∞—Ç',
                      '—É–¥–∞–ª–∏ –ø—É—Å—Ç', 'remove empty', '–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫', 'empty row',
                      '–∑–∞–ø–æ–ª–Ω–∏ –ø—É—Å—Ç', 'fill empty', 'fill blank', 'fillna',
                      '—É–±–µ—Ä–∏ –ø—Ä–æ–±–µ–ª', 'trim', 'strip', '–ø—Ä–æ–±–µ–ª—ã',
                      '–Ω–æ—Ä–º–∞–ª–∏–∑', 'normalize', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑',
                      # Additional patterns
                      '—É–±–µ—Ä–∏ –¥—É–±–ª–∏–∫', '—É–±–µ—Ä–∏ –ø–æ–≤—Ç–æ—Ä', '—É–±–µ—Ä–∏ –ø—É—Å—Ç', '—É–±–µ—Ä–∏ —Å—Ç—Ä–æ–∫–∏',
                      '—É–¥–∞–ª–∏ –ø–æ–≤—Ç–æ—Ä', '—É–¥–∞–ª–∏ —Å—Ç—Ä–æ–∫–∏']

    # CSV Split / Text-to-columns keywords
    CSV_SPLIT_KEYWORDS = ['—Ä–∞–∑–±–µ–π', '—Ä–∞–∑–±–∏—Ç—å', 'split', '—Ä–∞–∑–¥–µ–ª–∏—Ç—å', '—Ä–∞–∑–¥–µ–ª—è–π', 
                          '–ø–æ —è—á–µ–π–∫–∞–º', 'text to columns', '—Ç–µ–∫—Å—Ç –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º',
                          'csv', '–ø–æ –∫–æ–ª–æ–Ω–∫–∞–º', '–ø–æ —Å—Ç–æ–ª–±—Ü–∞–º', '—Ä–∞—Å–ø–∞—Ä—Å–∏', '–ø–∞—Ä—Å–∏–Ω–≥',
                          '—Ä–∞–∑–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã–µ', '—Ä–∞–∑–±–µ–π –¥–∞–Ω–Ω—ã–µ', '—Ä–∞–∑–±–µ–π csv', '—Ä–∞–∑–±–µ–π —Ç–µ–∫—Å—Ç']

    # Cleaning operation types
    CLEAN_OPERATIONS = {
        'duplicate': ['–¥—É–±–ª–∏–∫', 'duplicate', '–ø–æ–≤—Ç–æ—Ä', '–æ–¥–∏–Ω–∞–∫–æ–≤', '–¥—É–±–ª'],
        'empty_rows': ['–ø—É—Å—Ç', 'empty', 'blank', 'nan', 'null'],
        'trim': ['–ø—Ä–æ–±–µ–ª', 'trim', 'strip', 'whitespace'],
        'fill': ['–∑–∞–ø–æ–ª–Ω', 'fill', '–∑–∞–º–µ–Ω', 'replace'],
    }

    # Data validation keywords
    VALIDATION_KEYWORDS = ['–≤–∞–ª–∏–¥–∞—Ü', 'validation', '–≤—ã–ø–∞–¥–∞—é—â', 'dropdown', '—Å–ø–∏—Å–æ–∫',
                           '–æ–≥—Ä–∞–Ω–∏—á—å', 'restrict', '–¥–æ–ø—É—Å—Ç–∏–º',
                           '—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω', 'allowed', '–≤—ã–±–æ—Ä –∏–∑', 'select from']

    # Filter keywords
    FILTER_KEYWORDS = ['—Ñ–∏–ª—å—Ç—Ä', 'filter', '–æ—Ç—Ñ–∏–ª—å—Ç—Ä', '–ø–æ–∫–∞–∂–∏ —Ç–æ–ª—å–∫–æ', 'show only',
                       '–≥–¥–µ ', 'where ', '–≤—ã–±–µ—Ä–∏ –≥–¥–µ', 'select where', '—Å—Ç—Ä–æ–∫–∏ –≥–¥–µ',
                       'rows where', '–æ—Ç–±–µ—Ä–∏', '–≤—ã–±–µ—Ä–∏ —Å—Ç—Ä–æ–∫–∏']

    # Filter operators
    FILTER_OPERATORS = {
        '>=': ['>=', '‚â•', '–±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ', '–Ω–µ –º–µ–Ω—å—à–µ'],
        '<=': ['<=', '‚â§', '–º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ', '–Ω–µ –±–æ–ª—å—à–µ'],
        '!=': ['!=', '‚â†', '<>', '–Ω–µ —Ä–∞–≤–Ω–æ', '–Ω–µ —Ä–∞–≤–µ–Ω', '–∫—Ä–æ–º–µ'],
        '>': ['>', '–±–æ–ª—å—à–µ', '–≤—ã—à–µ', 'more than', 'greater', '–±–æ–ª–µ–µ'],
        '<': ['<', '–º–µ–Ω—å—à–µ', '–Ω–∏–∂–µ', 'less than', 'lower', '–º–µ–Ω–µ–µ'],
        '==': ['=', '==', '—Ä–∞–≤–Ω–æ', '—Ä–∞–≤–µ–Ω', 'equals', 'is'],
        'contains': ['—Å–æ–¥–µ—Ä–∂–∏—Ç', 'contains', '–≤–∫–ª—é—á–∞–µ—Ç', 'includes'],
        'startswith': ['–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è', 'starts with', '–Ω–∞—á–∏–Ω–∞–µ—Ç'],
        'endswith': ['–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è', 'ends with', '–æ–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è'],
        'empty': ['–ø—É—Å—Ç', 'empty', 'null', 'nan', '–Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è'],
        'not_empty': ['–Ω–µ –ø—É—Å—Ç', 'not empty', '–∑–∞–ø–æ–ª–Ω–µ–Ω', '–µ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ'],
    }

    # Color keywords for conditional formatting
    CONDITION_COLORS = {
        '–∫—Ä–∞—Å–Ω': {'red': 1, 'green': 0.8, 'blue': 0.8},      # Light red
        'red': {'red': 1, 'green': 0.8, 'blue': 0.8},
        '–∑–µ–ª–µ–Ω': {'red': 0.85, 'green': 0.95, 'blue': 0.85}, # Light green
        'green': {'red': 0.85, 'green': 0.95, 'blue': 0.85},
        '–∂—ë–ª—Ç': {'red': 1, 'green': 1, 'blue': 0.7},         # Light yellow
        '–∂–µ–ª—Ç': {'red': 1, 'green': 1, 'blue': 0.7},
        'yellow': {'red': 1, 'green': 1, 'blue': 0.7},
        '–æ—Ä–∞–Ω–∂': {'red': 1, 'green': 0.9, 'blue': 0.8},      # Light orange
        'orange': {'red': 1, 'green': 0.9, 'blue': 0.8},
        '—Å–∏–Ω–∏–π': {'red': 0.85, 'green': 0.9, 'blue': 1},     # Light blue
        'blue': {'red': 0.85, 'green': 0.9, 'blue': 1},
        '–≥–æ–ª—É–±': {'red': 0.85, 'green': 0.95, 'blue': 1},    # Light cyan
    }

    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            # Try loading from .env
            from pathlib import Path
            env_path = Path(__file__).parent.parent.parent / ".env"
            if env_path.exists():
                with open(env_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith("OPENAI_API_KEY="):
                            api_key = line.split("=", 1)[1].strip()
                            os.environ["OPENAI_API_KEY"] = api_key
                            break

        self.client = AsyncOpenAI(api_key=api_key)
        self.schema_extractor = get_schema_extractor()

    def _detect_sort_action(self, query: str, column_names: List[str]) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏–ª–∏ None.
        """
        query_lower = query.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        is_sort_query = any(kw in query_lower for kw in self.SORT_KEYWORDS)
        if not is_sort_query:
            return None

        logger.info(f"[SimpleGPT] Sort action detected: {query}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        is_descending = any(kw in query_lower for kw in self.SORT_DESC_KEYWORDS)
        is_ascending = any(kw in query_lower for kw in self.SORT_ASC_KEYWORDS)

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é, –µ—Å–ª–∏ —è–≤–Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ —É–±—ã–≤–∞–Ω–∏–µ
        sort_order = "DESCENDING" if is_descending and not is_ascending else "ASCENDING"

        # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ
        sort_column = None
        sort_column_index = None

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞
        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ
            if col_lower in query_lower or col_name in query:
                sort_column = col_name
                sort_column_index = idx
                logger.info(f"[SimpleGPT] Found sort column: '{col_name}' at index {idx}")
                break

        # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        if not sort_column:
            for idx, col_name in enumerate(column_names):
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞ —Å–ª–æ–≤–∞
                col_words = col_name.lower().split()
                for word in col_words:
                    if len(word) > 2 and word in query_lower:
                        sort_column = col_name
                        sort_column_index = idx
                        logger.info(f"[SimpleGPT] Found sort column by partial match: '{col_name}' at index {idx}")
                        break
                if sort_column:
                    break

        if not sort_column:
            logger.warning(f"[SimpleGPT] Sort column not found in query. Available columns: {column_names}")
            return None

        return {
            "action_type": "sort",
            "column_name": sort_column,
            "column_index": sort_column_index,
            "sort_order": sort_order,
            "message": f"–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–æ–Ω–∫–µ '{sort_column}' ({('–ø–æ —É–±—ã–≤–∞–Ω–∏—é' if sort_order == 'DESCENDING' else '–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é')})"
        }

    def _detect_freeze_action(self, query: str) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π –∑–∞–º–æ—Ä–æ–∑–∫–∏ —Å—Ç—Ä–æ–∫/—Å—Ç–æ–ª–±—Ü–æ–≤.
        """
        query_lower = query.lower()

        # Check for unfreeze first
        is_unfreeze = any(kw in query_lower for kw in self.UNFREEZE_KEYWORDS)
        if is_unfreeze:
            logger.info(f"[SimpleGPT] Unfreeze action detected: {query}")
            return {
                "action_type": "freeze",
                "freeze_rows": 0,
                "freeze_columns": 0,
                "message": "–ó–∞–∫—Ä–µ–ø–ª–µ–Ω–∏–µ —Å–Ω—è—Ç–æ"
            }

        # Check for freeze
        is_freeze = any(kw in query_lower for kw in self.FREEZE_KEYWORDS)
        if not is_freeze:
            return None

        logger.info(f"[SimpleGPT] Freeze action detected: {query}")

        # Determine what to freeze
        freeze_rows = 0
        freeze_columns = 0

        # Check for row freeze
        if any(word in query_lower for word in ['—Å—Ç—Ä–æ–∫', '—Å—Ç—Ä–æ–∫—É', 'row', '–ø–µ—Ä–≤—É—é', '—à–∞–ø–∫—É', '–∑–∞–≥–æ–ª–æ–≤']):
            # Try to find number
            import re
            numbers = re.findall(r'(\d+)\s*(?:—Å—Ç—Ä–æ–∫|—Å—Ç—Ä–æ–∫—É|row)', query_lower)
            if numbers:
                freeze_rows = int(numbers[0])
            else:
                freeze_rows = 1  # Default to 1 row (header)

        # Check for column freeze
        if any(word in query_lower for word in ['—Å—Ç–æ–ª–±', '–∫–æ–ª–æ–Ω–∫', 'column', '–ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±', '–ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫']):
            import re
            numbers = re.findall(r'(\d+)\s*(?:—Å—Ç–æ–ª–±|–∫–æ–ª–æ–Ω–∫|column)', query_lower)
            if numbers:
                freeze_columns = int(numbers[0])
            else:
                freeze_columns = 1  # Default to 1 column

        # If nothing specific mentioned, freeze first row
        if freeze_rows == 0 and freeze_columns == 0:
            freeze_rows = 1

        message_parts = []
        if freeze_rows > 0:
            message_parts.append(f"{freeze_rows} —Å—Ç—Ä–æ–∫" if freeze_rows > 1 else "–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞")
        if freeze_columns > 0:
            message_parts.append(f"{freeze_columns} —Å—Ç–æ–ª–±—Ü–æ–≤" if freeze_columns > 1 else "–ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü")

        return {
            "action_type": "freeze",
            "freeze_rows": freeze_rows,
            "freeze_columns": freeze_columns,
            "message": f"–ó–∞–∫—Ä–µ–ø–ª–µ–Ω–æ: {', '.join(message_parts)}"
        }

    def _detect_format_action(self, query: str) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        """
        query_lower = query.lower()

        # Check for bold formatting
        is_bold = any(kw in query_lower for kw in self.FORMAT_BOLD_KEYWORDS)
        is_header = any(kw in query_lower for kw in self.FORMAT_HEADER_KEYWORDS)

        if not (is_bold or is_header):
            return None

        logger.info(f"[SimpleGPT] Format action detected: {query}")

        # Determine format type
        format_type = "bold_header" if (is_bold and is_header) or is_header else "bold"

        # Check for color
        color = None
        if any(kw in query_lower for kw in self.FORMAT_COLOR_KEYWORDS):
            # Try to detect color
            color_map = {
                '–∫—Ä–∞—Å–Ω': '#FF0000', 'red': '#FF0000',
                '—Å–∏–Ω–∏–π': '#0000FF', 'blue': '#0000FF',
                '–∑–µ–ª–µ–Ω': '#00FF00', 'green': '#00FF00',
                '–∂–µ–ª—Ç': '#FFFF00', 'yellow': '#FFFF00',
                '–æ—Ä–∞–Ω–∂': '#FFA500', 'orange': '#FFA500',
                '—Å–µ—Ä—ã–π': '#808080', '—Å–µ—Ä': '#808080', 'gray': '#808080', 'grey': '#808080',
            }
            for color_word, color_code in color_map.items():
                if color_word in query_lower:
                    color = color_code
                    break

        return {
            "action_type": "format",
            "format_type": format_type,
            "target_row": 1,  # First row (header)
            "bold": is_bold or is_header,
            "background_color": color,
            "message": f"–ó–∞–≥–æ–ª–æ–≤–∫–∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã" + (f" (—Ü–≤–µ—Ç: {color})" if color else "")
        }

    def _detect_chart_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã.
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Å–µ–π.
        """
        query_lower = query.lower()

        # Check for chart keywords
        is_chart_query = any(kw in query_lower for kw in self.CHART_KEYWORDS)
        if not is_chart_query:
            return None

        logger.info(f"[SimpleGPT] Chart action detected: {query}")

        # Determine chart type
        chart_type = 'COLUMN'  # Default
        for type_keyword, type_value in self.CHART_TYPES.items():
            if type_keyword in query_lower:
                chart_type = type_value
                logger.info(f"[SimpleGPT] Chart type detected: {type_value}")
                break

        # Analyze columns to find best X and Y axes
        numeric_cols = []
        categorical_cols = []
        date_cols = []

        for idx, col in enumerate(column_names):
            if idx >= len(df.columns):
                continue
            col_data = df.iloc[:, idx]

            # Check if column is numeric
            try:
                numeric_data = pd.to_numeric(col_data, errors='coerce')
                non_null_ratio = numeric_data.notna().sum() / len(numeric_data) if len(numeric_data) > 0 else 0
                if non_null_ratio > 0.5:
                    numeric_cols.append({'name': col, 'index': idx})
                    continue
            except:
                pass

            # Check if column is date-like
            col_lower = col.lower()
            if any(d in col_lower for d in ['–¥–∞—Ç–∞', 'date', '–º–µ—Å—è—Ü', 'month', '–≥–æ–¥', 'year', '–¥–µ–Ω—å', 'day', '–ø–µ—Ä–∏–æ–¥', 'time']):
                date_cols.append({'name': col, 'index': idx})
                continue

            # Otherwise it's categorical
            categorical_cols.append({'name': col, 'index': idx})

        logger.info(f"[SimpleGPT] Column analysis: numeric={[c['name'] for c in numeric_cols]}, "
                   f"categorical={[c['name'] for c in categorical_cols]}, date={[c['name'] for c in date_cols]}")

        # Find columns mentioned in query
        mentioned_cols = []
        for idx, col in enumerate(column_names):
            col_lower = col.lower()
            # Check if column name or any significant word from it is in query
            if col_lower in query_lower or col in query:
                mentioned_cols.append({'name': col, 'index': idx})
            else:
                # Check for partial match
                for word in col_lower.split():
                    if len(word) > 2 and word in query_lower:
                        mentioned_cols.append({'name': col, 'index': idx})
                        break

        logger.info(f"[SimpleGPT] Columns mentioned in query: {[c['name'] for c in mentioned_cols]}")

        # Determine X and Y axes
        x_column = None
        y_columns = []

        # Priority for X axis: mentioned categorical > date > first categorical
        # If user explicitly mentions a categorical column, use it
        for cat in categorical_cols:
            if cat in mentioned_cols:
                x_column = cat
                logger.info(f"[SimpleGPT] Using mentioned categorical column for X axis: {cat['name']}")
                break

        # If no mentioned categorical, use date column for time series
        if not x_column and date_cols:
            x_column = date_cols[0]
            logger.info(f"[SimpleGPT] Using date column for X axis: {x_column['name']}")

        # Fallback to first categorical
        if not x_column and categorical_cols:
            x_column = categorical_cols[0]
            logger.info(f"[SimpleGPT] Using first categorical column for X axis: {x_column['name']}")

        # Y axis: mentioned numeric columns, or all numeric if none mentioned
        for num in numeric_cols:
            if num in mentioned_cols:
                y_columns.append(num)

        if not y_columns and numeric_cols:
            # Take first 1-3 numeric columns
            y_columns = numeric_cols[:3]

        # For PIE charts, we need exactly one Y column and one X column
        if chart_type == 'PIE' and y_columns:
            y_columns = [y_columns[0]]

        # Generate title from query or columns
        title = ""
        if y_columns and x_column:
            y_names = ", ".join([c['name'] for c in y_columns])
            title = f"{y_names} –ø–æ {x_column['name']}"
        elif y_columns:
            title = ", ".join([c['name'] for c in y_columns])

        # Build chart spec
        chart_spec = {
            "chart_type": chart_type,
            "title": title,
            "x_column_index": x_column['index'] if x_column else 0,
            "x_column_name": x_column['name'] if x_column else column_names[0],
            "y_column_indices": [c['index'] for c in y_columns] if y_columns else [1] if len(column_names) > 1 else [0],
            "y_column_names": [c['name'] for c in y_columns] if y_columns else [column_names[1] if len(column_names) > 1 else column_names[0]],
            "row_count": len(df),
            "col_count": len(column_names)
        }

        chart_type_names = {
            'LINE': '–ª–∏–Ω–µ–π–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫',
            'BAR': '–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É—é –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É',
            'COLUMN': '—Å—Ç–æ–ª–±—á–∞—Ç—É—é –¥–∏–∞–≥—Ä–∞–º–º—É',
            'PIE': '–∫—Ä—É–≥–æ–≤—É—é –¥–∏–∞–≥—Ä–∞–º–º—É',
            'AREA': '–¥–∏–∞–≥—Ä–∞–º–º—É —Å –æ–±–ª–∞—Å—Ç—è–º–∏',
            'SCATTER': '—Ç–æ—á–µ—á–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É',
            'COMBO': '–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫'
        }

        message = f"–°–æ–∑–¥–∞—é {chart_type_names.get(chart_type, '–¥–∏–∞–≥—Ä–∞–º–º—É')}: {title}"

        return {
            "action_type": "chart",
            "chart_spec": chart_spec,
            "message": message
        }

    def _detect_conditional_format_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π —É—Å–ª–æ–≤–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        –ü—Ä–∏–º–µ—Ä—ã:
        - "–≤—ã–¥–µ–ª–∏ –∫—Ä–∞—Å–Ω—ã–º –≥–¥–µ —Å—É–º–º–∞ –±–æ–ª—å—à–µ 10000"
        - "–∑–µ–ª—ë–Ω—ã–º –≥–¥–µ –ø—Ä–∏–±—ã–ª—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è"
        - "—É—Å–ª–æ–≤–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –∂—ë–ª—Ç—ã–º –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏"
        """
        query_lower = query.lower()

        # Check for conditional format keywords
        is_conditional = any(kw in query_lower for kw in self.CONDITIONAL_FORMAT_KEYWORDS)
        if not is_conditional:
            return None

        logger.info(f"[SimpleGPT] Conditional format action detected: {query}")

        # Detect color
        format_color = {'red': 1, 'green': 1, 'blue': 0.7}  # Default yellow
        color_name = "–∂—ë–ª—Ç—ã–π"
        for color_kw, color_value in self.CONDITION_COLORS.items():
            if color_kw in query_lower:
                format_color = color_value
                color_name = color_kw
                break

        # Find column mentioned in query
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        # If no column found, try to find numeric column
        if not target_column:
            for idx, col_name in enumerate(column_names):
                if idx < len(df.columns):
                    try:
                        numeric_data = pd.to_numeric(df.iloc[:, idx], errors='coerce')
                        if numeric_data.notna().sum() / len(numeric_data) > 0.5:
                            target_column = col_name
                            target_column_index = idx
                            break
                    except:
                        pass

        # Detect condition type and value
        condition_type = "GREATER_THAN"  # Default
        condition_value = None

        # Patterns for conditions
        import re

        # "–±–æ–ª—å—à–µ X" / "> X"
        greater_match = re.search(r'(?:–±–æ–ª—å—à–µ|>|–±–æ–ª–µ–µ)\s*(?:—á–µ–º\s*)?(\d+(?:[.,]\d+)?)', query_lower)
        if greater_match:
            condition_type = "NUMBER_GREATER"
            condition_value = float(greater_match.group(1).replace(',', '.'))

        # "–º–µ–Ω—å—à–µ X" / "< X"
        less_match = re.search(r'(?:–º–µ–Ω—å—à–µ|<|–º–µ–Ω–µ–µ)\s*(?:—á–µ–º\s*)?(\d+(?:[.,]\d+)?)', query_lower)
        if less_match:
            condition_type = "NUMBER_LESS"
            condition_value = float(less_match.group(1).replace(',', '.'))

        # "—Ä–∞–≤–Ω–æ X" / "= X"
        equal_match = re.search(r'(?:—Ä–∞–≤–Ω–æ|=|—Ä–∞–≤–µ–Ω)\s*(\d+(?:[.,]\d+)?)', query_lower)
        if equal_match:
            condition_type = "NUMBER_EQ"
            condition_value = float(equal_match.group(1).replace(',', '.'))

        # "–ø—É—Å—Ç–æ" / "–ø—É—Å—Ç—ã–µ"
        if any(w in query_lower for w in ['–ø—É—Å—Ç', 'empty', 'blank', '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö']):
            condition_type = "BLANK"
            condition_value = None

        # "–Ω–µ –ø—É—Å—Ç–æ" / "–∑–∞–ø–æ–ª–Ω–µ–Ω–æ"
        if any(w in query_lower for w in ['–Ω–µ –ø—É—Å—Ç', 'not empty', '–∑–∞–ø–æ–ª–Ω–µ–Ω', '–µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ']):
            condition_type = "NOT_BLANK"
            condition_value = None

        # "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω" / "—É–±—ã—Ç–æ–∫"
        if any(w in query_lower for w in ['–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω', '—É–±—ã—Ç', 'negative', '–º–∏–Ω—É—Å']):
            condition_type = "NUMBER_LESS"
            condition_value = 0

        # "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω" / "–ø—Ä–∏–±—ã–ª—å"
        if any(w in query_lower for w in ['–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω', '–ø—Ä–∏–±—ã–ª', 'positive', '–ø–ª—é—Å']):
            condition_type = "NUMBER_GREATER"
            condition_value = 0

        # Build the conditional format rule
        rule = {
            "column_index": target_column_index if target_column_index is not None else 0,
            "column_name": target_column or column_names[0] if column_names else "A",
            "condition_type": condition_type,
            "condition_value": condition_value,
            "format_color": format_color
        }

        # Generate message
        condition_text = ""
        if condition_type == "NUMBER_GREATER":
            condition_text = f"> {condition_value}"
        elif condition_type == "NUMBER_LESS":
            condition_text = f"< {condition_value}"
        elif condition_type == "NUMBER_EQ":
            condition_text = f"= {condition_value}"
        elif condition_type == "BLANK":
            condition_text = "–ø—É—Å—Ç—ã–µ"
        elif condition_type == "NOT_BLANK":
            condition_text = "–Ω–µ–ø—É—Å—Ç—ã–µ"

        message = f"–£—Å–ª–æ–≤–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {target_column or '–∫–æ–ª–æ–Ω–∫–∞'} {condition_text} ‚Üí {color_name}"

        return {
            "action_type": "conditional_format",
            "rule": rule,
            "message": message
        }

    def _detect_pivot_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã.
        –ü—Ä–∏–º–µ—Ä—ã:
        - "—Å–≤–æ–¥–Ω–∞—è –ø–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º"
        - "–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–¥–∞–∂ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º"
        - "—Å—É–º–º—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"
        """
        query_lower = query.lower()

        # Check for pivot keywords
        is_pivot = any(kw in query_lower for kw in self.PIVOT_KEYWORDS)
        if not is_pivot:
            return None

        logger.info(f"[SimpleGPT] Pivot action detected: {query}")

        # Analyze columns
        numeric_cols = []
        categorical_cols = []

        for idx, col_name in enumerate(column_names):
            if idx >= len(df.columns):
                continue
            col_data = df.iloc[:, idx]

            # Check if numeric
            try:
                numeric_data = pd.to_numeric(col_data, errors='coerce')
                non_null_ratio = numeric_data.notna().sum() / len(numeric_data) if len(numeric_data) > 0 else 0
                if non_null_ratio > 0.5:
                    numeric_cols.append({'name': col_name, 'index': idx})
                    continue
            except:
                pass

            # Otherwise categorical
            unique_count = col_data.nunique()
            if unique_count <= len(col_data) * 0.7:  # Up to 70% unique = categorical
                categorical_cols.append({'name': col_name, 'index': idx})

        logger.info(f"[SimpleGPT] Columns: numeric={[c['name'] for c in numeric_cols]}, categorical={[c['name'] for c in categorical_cols]}")

        # Find grouping column (categorical mentioned in query)
        group_column = None
        for cat in categorical_cols:
            cat_lower = cat['name'].lower()
            if cat_lower in query_lower or cat['name'] in query:
                group_column = cat
                break
            # Partial match - check if column name stem is in query
            for word in cat_lower.split():
                if len(word) > 2 and word in query_lower:
                    group_column = cat
                    break
                # Check if word stem (first 4+ chars) is in query for Russian word forms
                if len(word) >= 4:
                    word_stem = word[:max(4, len(word) - 2)]  # Get stem (at least 4 chars)
                    if word_stem in query_lower:
                        group_column = cat
                        logger.info(f"[SimpleGPT] Found pivot column by stem: '{word_stem}' in '{word}' -> {cat['name']}")
                        break
            if group_column:
                break

        # If not found, use first categorical
        if not group_column and categorical_cols:
            group_column = categorical_cols[0]

        # Find value column (numeric mentioned in query or first numeric)
        value_column = None
        for num in numeric_cols:
            num_lower = num['name'].lower()
            if num_lower in query_lower or num['name'] in query:
                value_column = num
                break
            for word in num_lower.split():
                if len(word) > 2 and word in query_lower:
                    value_column = num
                    break
            if value_column:
                break

        if not value_column and numeric_cols:
            value_column = numeric_cols[0]

        # Detect aggregation function
        agg_func = 'sum'  # Default
        for kw, func in self.AGG_FUNCTIONS.items():
            if kw in query_lower:
                agg_func = func
                break

        if not group_column or not value_column:
            logger.warning(f"[SimpleGPT] Cannot create pivot: group_column={group_column}, value_column={value_column}")
            return None

        # Create pivot table using pandas
        try:
            pivot_df = df.groupby(df.iloc[:, group_column['index']]).agg({
                df.columns[value_column['index']]: agg_func
            }).reset_index()

            # Rename columns
            pivot_df.columns = [group_column['name'], f"{agg_func.upper()}({value_column['name']})"]

            # Sort by value descending
            pivot_df = pivot_df.sort_values(by=pivot_df.columns[1], ascending=False)

            # Convert to structured data
            pivot_data = {
                "headers": list(pivot_df.columns),
                "rows": pivot_df.to_dict(orient='records')
            }

            agg_names = {
                'sum': '–°—É–º–º–∞',
                'mean': '–°—Ä–µ–¥–Ω–µ–µ',
                'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
                'max': '–ú–∞–∫—Å–∏–º—É–º',
                'min': '–ú–∏–Ω–∏–º—É–º'
            }

            message = f"–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞: {agg_names.get(agg_func, agg_func)} {value_column['name']} –ø–æ {group_column['name']}"

            return {
                "action_type": "pivot_table",
                "pivot_data": pivot_data,
                "group_column": group_column['name'],
                "value_column": value_column['name'],
                "agg_func": agg_func,
                "message": message
            }

        except Exception as e:
            logger.error(f"[SimpleGPT] Error creating pivot: {e}")
            return None

    def _detect_csv_split_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π —Ä–∞–∑–±–∏–µ–Ω–∏—è CSV/—Ç–µ–∫—Å—Ç–∞ –ø–æ —è—á–µ–π–∫–∞–º.
        –ü—Ä–∏–º–µ—Ä—ã:
        - "—Ä–∞–∑–±–µ–π –¥–∞–Ω–Ω—ã–µ –ø–æ —è—á–µ–π–∫–∞–º"
        - "—Ä–∞–∑–¥–µ–ª–∏ csv –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º"
        - "text to columns"
        """
        query_lower = query.lower()
        
        # Check for CSV split keywords
        is_csv_split = any(kw in query_lower for kw in self.CSV_SPLIT_KEYWORDS)
        if not is_csv_split:
            return None
        
        logger.info(f"[SimpleGPT] CSV split action detected: {query}")
        
        # Detect delimiter from data
        delimiter = None
        first_row = df.iloc[0, 0] if len(df) > 0 and len(df.columns) > 0 else ''
        first_row_str = str(first_row)
        
        # Check common delimiters
        if ';' in first_row_str:
            delimiter = ';'
        elif ',' in first_row_str:
            delimiter = ','
        elif '	' in first_row_str:
            delimiter = '	'
        elif '|' in first_row_str:
            delimiter = '|'
        
        if not delimiter:
            logger.warning(f"[SimpleGPT] Could not detect delimiter in data")
            return None
        
        logger.info(f"[SimpleGPT] Detected delimiter: '{delimiter}'")
        
        # Split data
        try:
            import io
            # Combine all data into a single string
            all_data = []
            for idx, row in df.iterrows():
                row_str = str(row.iloc[0]) if len(row) > 0 else ''
                all_data.append(row_str)
            
            csv_text = chr(10).join(all_data)

            # Parse CSV - first row becomes headers (standard CSV format)
            split_df = pd.read_csv(io.StringIO(csv_text), sep=delimiter, header=0, dtype=str)

            # First row is used as column names
            headers = split_df.columns.tolist()

            # Remaining rows become data
            rows = split_df.fillna('').to_dict('records')
            
            structured_data = {
                'headers': headers,
                'rows': rows
            }
            
            message = f"""**‚úÖ –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–±–∏—Ç—ã –ø–æ —è—á–µ–π–∫–∞–º**

üìã –†–µ–∑—É–ª—å—Ç–∞—Ç:
‚Ä¢ –ö–æ–ª–æ–Ω–æ–∫: {len(headers)}
‚Ä¢ –°—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö: {len(rows)}
‚Ä¢ –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{delimiter}'
‚Ä¢ –ö–æ–ª–æ–Ω–∫–∏: {', '.join(headers[:5])}{'...' if len(headers) > 5 else ''}

üí° –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã –∑–∞–º–µ–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü–µ."""
            
            return {
                'structured_data': structured_data,
                'original_rows': len(df),
                'new_rows': len(rows),
                'new_cols': len(headers),
                'delimiter': delimiter,
                'message': message
            }
            
        except Exception as e:
            logger.error(f"[SimpleGPT] CSV split error: {e}")
            return None

    def _detect_clean_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
        –ü—Ä–∏–º–µ—Ä—ã:
        - "—É–¥–∞–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã"
        - "—É–¥–∞–ª–∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏"
        - "–∑–∞–ø–æ–ª–Ω–∏ –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏ –Ω—É–ª—è–º–∏"
        - "–æ—á–∏—Å—Ç–∏ –¥–∞–Ω–Ω—ã–µ"
        """
        query_lower = query.lower()

        # Check for clean keywords
        is_clean = any(kw in query_lower for kw in self.CLEAN_KEYWORDS)
        if not is_clean:
            return None

        logger.info(f"[SimpleGPT] Clean action detected: {query}")

        # Determine operation type
        operations = []

        # Check for duplicate removal
        if any(kw in query_lower for kw in self.CLEAN_OPERATIONS['duplicate']):
            operations.append('remove_duplicates')

        # Check for empty row removal
        if any(kw in query_lower for kw in self.CLEAN_OPERATIONS['empty_rows']):
            # Distinguish between "—É–¥–∞–ª–∏ –ø—É—Å—Ç—ã–µ" vs "–∑–∞–ø–æ–ª–Ω–∏ –ø—É—Å—Ç—ã–µ"
            if any(w in query_lower for w in ['—É–¥–∞–ª–∏', '—É–±–µ—Ä–∏', 'remove', 'delete']):
                operations.append('remove_empty_rows')
            elif any(w in query_lower for w in ['–∑–∞–ø–æ–ª–Ω', 'fill', '–∑–∞–º–µ–Ω']):
                operations.append('fill_empty')

        # Check for trimming whitespace
        if any(kw in query_lower for kw in self.CLEAN_OPERATIONS['trim']):
            operations.append('trim_whitespace')

        # Check for fill operation (if not already detected)
        if 'fill_empty' not in operations and any(kw in query_lower for kw in self.CLEAN_OPERATIONS['fill']):
            operations.append('fill_empty')

        # Default to all common operations if just "–æ—á–∏—Å—Ç–∏ –¥–∞–Ω–Ω—ã–µ"
        if not operations and any(w in query_lower for w in ['–æ—á–∏—Å—Ç–∏', 'clean']):
            operations = ['remove_duplicates', 'remove_empty_rows', 'trim_whitespace']

        if not operations:
            return None

        # Detect fill value if applicable
        fill_value = None
        if 'fill_empty' in operations:
            # Check for specific fill values
            import re

            # "–Ω—É–ª—è–º–∏" / "0" / "zeros"
            if any(w in query_lower for w in ['–Ω—É–ª', 'zero', '0']):
                fill_value = 0
            # "–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π" / ""
            elif any(w in query_lower for w in ['—Å—Ç—Ä–æ–∫', 'string', '—Ç–µ–∫—Å—Ç']):
                fill_value = ""
            # "—Å—Ä–µ–¥–Ω–∏–º" / "mean" / "average"
            elif any(w in query_lower for w in ['—Å—Ä–µ–¥–Ω', 'mean', 'average', 'avg']):
                fill_value = "mean"
            # "–º–µ–¥–∏–∞–Ω–æ–π" / "median"
            elif any(w in query_lower for w in ['–º–µ–¥–∏–∞–Ω', 'median']):
                fill_value = "median"
            # "–ø—Ä–µ–¥—ã–¥—É—â–∏–º" / "forward fill"
            elif any(w in query_lower for w in ['–ø—Ä–µ–¥—ã–¥—É—â', 'forward', 'ffill', '–ø–æ—Å–ª–µ–¥–Ω']):
                fill_value = "ffill"
            # Specific number
            number_match = re.search(r'(\d+(?:[.,]\d+)?)', query_lower)
            if number_match and fill_value is None:
                fill_value = float(number_match.group(1).replace(',', '.'))

            # Default to 0 if not specified
            if fill_value is None:
                fill_value = 0

        # Find target column if specified
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        # Execute cleaning and get preview
        try:
            cleaned_df = df.copy()
            original_rows = len(cleaned_df)
            changes = []

            for op in operations:
                if op == 'remove_duplicates':
                    before = len(cleaned_df)
                    if target_column:
                        cleaned_df = cleaned_df.drop_duplicates(subset=[cleaned_df.columns[target_column_index]])
                    else:
                        cleaned_df = cleaned_df.drop_duplicates()
                    removed = before - len(cleaned_df)
                    if removed > 0:
                        changes.append(f"—É–¥–∞–ª–µ–Ω–æ {removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

                elif op == 'remove_empty_rows':
                    before = len(cleaned_df)
                    if target_column:
                        cleaned_df = cleaned_df.dropna(subset=[cleaned_df.columns[target_column_index]])
                    else:
                        cleaned_df = cleaned_df.dropna(how='all')
                    removed = before - len(cleaned_df)
                    if removed > 0:
                        changes.append(f"—É–¥–∞–ª–µ–Ω–æ {removed} –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫")

                elif op == 'trim_whitespace':
                    # Trim string columns
                    str_cols = cleaned_df.select_dtypes(include=['object']).columns
                    for col in str_cols:
                        cleaned_df[col] = cleaned_df[col].apply(
                            lambda x: x.strip() if isinstance(x, str) else x
                        )
                    if len(str_cols) > 0:
                        changes.append(f"—É–±—Ä–∞–Ω—ã –ø—Ä–æ–±–µ–ª—ã –≤ {len(str_cols)} –∫–æ–ª–æ–Ω–∫–∞—Ö")

                elif op == 'fill_empty':
                    if target_column:
                        col = cleaned_df.columns[target_column_index]
                        empty_count = cleaned_df[col].isna().sum()
                        if fill_value == "mean":
                            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mean())
                        elif fill_value == "median":
                            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())
                        elif fill_value == "ffill":
                            cleaned_df[col] = cleaned_df[col].fillna(method='ffill')
                        else:
                            cleaned_df[col] = cleaned_df[col].fillna(fill_value)
                        if empty_count > 0:
                            changes.append(f"–∑–∞–ø–æ–ª–Ω–µ–Ω–æ {empty_count} –ø—É—Å—Ç—ã—Ö —è—á–µ–µ–∫ –≤ '{target_column}'")
                    else:
                        # Fill all numeric columns
                        num_cols = cleaned_df.select_dtypes(include=[np.number]).columns
                        total_filled = 0
                        for col in num_cols:
                            empty_count = cleaned_df[col].isna().sum()
                            total_filled += empty_count
                            if fill_value == "mean":
                                cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mean())
                            elif fill_value == "median":
                                cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())
                            elif fill_value == "ffill":
                                cleaned_df[col] = cleaned_df[col].fillna(method='ffill')
                            else:
                                cleaned_df[col] = cleaned_df[col].fillna(fill_value)
                        if total_filled > 0:
                            changes.append(f"–∑–∞–ø–æ–ª–Ω–µ–Ω–æ {total_filled} –ø—É—Å—Ç—ã—Ö —è—á–µ–µ–∫")

            final_rows = len(cleaned_df)

            # Prepare result data
            cleaned_data = {
                "headers": list(cleaned_df.columns),
                "rows": cleaned_df.to_dict(orient='records')
            }

            # Build message
            if changes:
                message = "–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: " + ", ".join(changes)
            else:
                message = "–î–∞–Ω–Ω—ã–µ —É–∂–µ —á–∏—Å—Ç—ã–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è"

            return {
                "action_type": "clean_data",
                "operations": operations,
                "fill_value": fill_value,
                "target_column": target_column,
                "original_rows": original_rows,
                "final_rows": final_rows,
                "cleaned_data": cleaned_data,
                "changes": changes,
                "message": message
            }

        except Exception as e:
            logger.error(f"[SimpleGPT] Error cleaning data: {e}")
            return None

    def _detect_validation_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö (–≤—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫).
        –ü—Ä–∏–º–µ—Ä—ã:
        - "—Å–æ–∑–¥–∞–π –≤—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –≤ –∫–æ–ª–æ–Ω–∫–µ –°—Ç–∞—Ç—É—Å"
        - "–¥–æ–±–∞–≤—å –≤–∞–ª–∏–¥–∞—Ü–∏—é: —Ç–æ–ª—å–∫–æ –î–∞/–ù–µ—Ç"
        - "–æ–≥—Ä–∞–Ω–∏—á—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–µ –ö–∞—Ç–µ–≥–æ—Ä–∏—è"
        """
        query_lower = query.lower()

        # Check for validation keywords
        is_validation = any(kw in query_lower for kw in self.VALIDATION_KEYWORDS)
        if not is_validation:
            return None

        logger.info(f"[SimpleGPT] Validation action detected: {query}")

        # Find target column
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        # Extract allowed values from query
        allowed_values = []

        # Pattern 1: "—Ç–æ–ª—å–∫–æ X/Y/Z" or "—Ç–æ–ª—å–∫–æ X, Y, Z"
        import re
        only_match = re.search(r'—Ç–æ–ª—å–∫–æ\s+([^.!?]+)', query_lower)
        if only_match:
            values_str = only_match.group(1)
            # Split by / or , or "–∏–ª–∏"
            values = re.split(r'[/,]|\s–∏–ª–∏\s|\sor\s', values_str)
            allowed_values = [v.strip() for v in values if v.strip()]

        # Pattern 2: "–∑–Ω–∞—á–µ–Ω–∏—è: X, Y, Z" or "–≤–∞—Ä–∏–∞–Ω—Ç—ã: X, Y, Z"
        values_match = re.search(r'(?:–∑–Ω–∞—á–µ–Ω–∏—è|–≤–∞—Ä–∏–∞–Ω—Ç—ã|options|values)[:\s]+([^.!?]+)', query_lower)
        if values_match and not allowed_values:
            values_str = values_match.group(1)
            values = re.split(r'[/,]|\s–∏–ª–∏\s|\sor\s', values_str)
            allowed_values = [v.strip() for v in values if v.strip()]

        # Pattern 3: "–î–∞/–ù–µ—Ç" style in query
        if not allowed_values:
            # Look for slash-separated values
            slash_match = re.search(r'([–∞-—è—ëa-z0-9]+(?:/[–∞-—è—ëa-z0-9]+)+)', query_lower)
            if slash_match:
                allowed_values = slash_match.group(1).split('/')

        # If still no values and we have a target column, extract unique values from data
        if not allowed_values and target_column and target_column_index is not None:
            try:
                unique_values = df.iloc[:, target_column_index].dropna().unique()
                # Only use if reasonable number of unique values (< 20)
                if len(unique_values) <= 20:
                    allowed_values = [str(v) for v in unique_values]
                    logger.info(f"[SimpleGPT] Auto-extracted {len(allowed_values)} unique values from column")
            except Exception as e:
                logger.warning(f"[SimpleGPT] Could not extract unique values: {e}")

        if not target_column:
            # Try to find first categorical column
            for idx, col_name in enumerate(column_names):
                if idx < len(df.columns):
                    try:
                        unique_count = df.iloc[:, idx].nunique()
                        total_count = len(df)
                        # Categorical if less than 50% unique values and <= 20 unique
                        if unique_count <= 20 and unique_count < total_count * 0.5:
                            target_column = col_name
                            target_column_index = idx
                            if not allowed_values:
                                allowed_values = [str(v) for v in df.iloc[:, idx].dropna().unique()]
                            break
                    except:
                        pass

        if not target_column:
            logger.warning(f"[SimpleGPT] No target column found for validation")
            return None

        if not allowed_values:
            logger.warning(f"[SimpleGPT] No allowed values found for validation")
            return None

        # Capitalize first letter of each value for display
        allowed_values = [v.strip().capitalize() if v.strip() else v for v in allowed_values]

        # Build validation rule
        rule = {
            "column_index": target_column_index,
            "column_name": target_column,
            "validation_type": "ONE_OF_LIST",
            "allowed_values": allowed_values,
            "show_dropdown": True,
            "strict": True  # Reject invalid input
        }

        values_preview = ", ".join(allowed_values[:5])
        if len(allowed_values) > 5:
            values_preview += f" (+{len(allowed_values) - 5})"

        message = f"–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è '{target_column}': {values_preview}"

        return {
            "action_type": "data_validation",
            "rule": rule,
            "message": message
        }

    def _detect_filter_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–º–∞–Ω–¥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.
        –ü—Ä–∏–º–µ—Ä—ã:
        - "–ø–æ–∫–∞–∂–∏ —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –°—Ç–∞—Ç—É—Å = –ê–∫—Ç–∏–≤–Ω—ã–π"
        - "–æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–π –ø–æ –¶–µ–Ω–∞ > 1000"
        - "–Ω–∞–π–¥–∏ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –î–∞—Ç–∞ –ø—É—Å—Ç–∞—è"
        """
        query_lower = query.lower()

        # Check for filter keywords
        is_filter = any(kw in query_lower for kw in self.FILTER_KEYWORDS)
        if not is_filter:
            return None

        logger.info(f"[SimpleGPT] Filter action detected: {query}")

        # Find target column
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        if not target_column:
            logger.warning(f"[SimpleGPT] No target column found for filter")
            return None

        # Detect operator and value
        import re
        operator = '=='
        filter_value = None

        # Check operators in order of specificity (longer patterns first)
        for op, patterns in self.FILTER_OPERATORS.items():
            for pattern in patterns:
                if pattern in query_lower:
                    operator = op
                    break
            if operator != '==':
                break

        # Extract value based on operator
        if operator in ['empty', 'not_empty']:
            filter_value = None
        else:
            # Try to extract numeric value
            number_match = re.search(r'(\d+(?:[.,]\d+)?)', query_lower)
            if number_match:
                filter_value = float(number_match.group(1).replace(',', '.'))
            else:
                # Try to extract text value after operator patterns
                value_patterns = [
                    r'(?:—Ä–∞–≤–Ω–æ|=|—Ä–∞–≤–µ–Ω|is)\s+["\']?([^"\'.,!?]+)["\']?',
                    r'(?:—Å–æ–¥–µ—Ä–∂–∏—Ç|contains)\s+["\']?([^"\'.,!?]+)["\']?',
                    r'(?:–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è|starts)\s+(?:—Å|with)?\s*["\']?([^"\'.,!?]+)["\']?',
                ]
                for vp in value_patterns:
                    value_match = re.search(vp, query_lower)
                    if value_match:
                        filter_value = value_match.group(1).strip()
                        break

                # If still no value, try to find value after column name
                if filter_value is None:
                    col_pattern = re.escape(target_column.lower())
                    after_col_match = re.search(
                        rf'{col_pattern}\s*(?:[=<>!]+|—Ä–∞–≤–Ω–æ|–±–æ–ª—å—à–µ|–º–µ–Ω—å—à–µ|—Å–æ–¥–µ—Ä–∂–∏—Ç)\s*["\']?([^\s"\'.,!?]+)',
                        query_lower
                    )
                    if after_col_match:
                        filter_value = after_col_match.group(1).strip()

        # Execute filter and get preview
        try:
            filtered_df = df.copy()
            original_rows = len(filtered_df)
            col = filtered_df.columns[target_column_index]

            if operator == 'empty':
                filtered_df = filtered_df[filtered_df[col].isna() | (filtered_df[col] == '')]
            elif operator == 'not_empty':
                filtered_df = filtered_df[filtered_df[col].notna() & (filtered_df[col] != '')]
            elif operator == 'contains' and filter_value:
                filtered_df = filtered_df[
                    filtered_df[col].astype(str).str.lower().str.contains(str(filter_value).lower(), na=False)
                ]
            elif operator == 'startswith' and filter_value:
                filtered_df = filtered_df[
                    filtered_df[col].astype(str).str.lower().str.startswith(str(filter_value).lower())
                ]
            elif operator == 'endswith' and filter_value:
                filtered_df = filtered_df[
                    filtered_df[col].astype(str).str.lower().str.endswith(str(filter_value).lower())
                ]
            elif filter_value is not None:
                # Numeric or exact match
                try:
                    numeric_val = float(filter_value) if isinstance(filter_value, (int, float, str)) and str(filter_value).replace('.', '').replace('-', '').isdigit() else None
                    if numeric_val is not None:
                        col_numeric = pd.to_numeric(filtered_df[col], errors='coerce')
                        if operator == '>':
                            filtered_df = filtered_df[col_numeric > numeric_val]
                        elif operator == '<':
                            filtered_df = filtered_df[col_numeric < numeric_val]
                        elif operator == '>=':
                            filtered_df = filtered_df[col_numeric >= numeric_val]
                        elif operator == '<=':
                            filtered_df = filtered_df[col_numeric <= numeric_val]
                        elif operator == '!=':
                            filtered_df = filtered_df[col_numeric != numeric_val]
                        else:  # ==
                            filtered_df = filtered_df[col_numeric == numeric_val]
                    else:
                        # String comparison
                        str_col = filtered_df[col].astype(str).str.lower()
                        str_val = str(filter_value).lower()
                        if operator == '!=':
                            filtered_df = filtered_df[str_col != str_val]
                        else:
                            filtered_df = filtered_df[str_col == str_val]
                except Exception as e:
                    logger.warning(f"[SimpleGPT] Filter comparison error: {e}")
                    # Fallback to string comparison
                    str_col = filtered_df[col].astype(str).str.lower()
                    str_val = str(filter_value).lower()
                    filtered_df = filtered_df[str_col == str_val]

            filtered_rows = len(filtered_df)

            # Prepare result data
            filtered_data = {
                "headers": list(filtered_df.columns),
                "rows": filtered_df.to_dict(orient='records')
            }

            # Build operator display
            op_display = {
                '==': '=', '!=': '‚â†', '>': '>', '<': '<', '>=': '‚â•', '<=': '‚â§',
                'contains': '—Å–æ–¥–µ—Ä–∂–∏—Ç', 'startswith': '–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å', 'endswith': '–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞',
                'empty': '–ø—É—Å—Ç–æ', 'not_empty': '–Ω–µ –ø—É—Å—Ç–æ'
            }

            if operator in ['empty', 'not_empty']:
                condition_str = f"{target_column} {op_display.get(operator, operator)}"
            else:
                condition_str = f"{target_column} {op_display.get(operator, operator)} {filter_value}"

            message = f"–§–∏–ª—å—Ç—Ä: {condition_str} ‚Üí {filtered_rows} –∏–∑ {original_rows} —Å—Ç—Ä–æ–∫"

            return {
                "action_type": "filter_data",
                "column_name": target_column,
                "column_index": target_column_index,
                "operator": operator,
                "filter_value": filter_value,
                "original_rows": original_rows,
                "filtered_rows": filtered_rows,
                "filtered_data": filtered_data,
                "condition_str": condition_str,
                "message": message
            }

        except Exception as e:
            logger.error(f"[SimpleGPT] Error filtering data: {e}")
            return None

    async def process(
        self,
        query: str,
        df: pd.DataFrame,
        column_names: List[str],
        custom_context: Optional[str] = None,
        history: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞.
        """
        start_time = time.time()

        try:
            # 0. Check for direct actions (sort, format, etc.) - no GPT needed
            sort_action = self._detect_sort_action(query, column_names)
            if sort_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning sort action: {sort_action}")
                return {
                    "success": True,
                    "action_type": "sort",
                    "result_type": "action",
                    "sort_column": sort_action["column_name"],
                    "sort_column_index": sort_action["column_index"],
                    "sort_order": sort_action["sort_order"],
                    "summary": sort_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for freeze action
            freeze_action = self._detect_freeze_action(query)
            if freeze_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning freeze action: {freeze_action}")
                return {
                    "success": True,
                    "action_type": "freeze",
                    "result_type": "action",
                    "freeze_rows": freeze_action["freeze_rows"],
                    "freeze_columns": freeze_action["freeze_columns"],
                    "summary": freeze_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for format action
            format_action = self._detect_format_action(query)
            if format_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning format action: {format_action}")
                return {
                    "success": True,
                    "action_type": "format",
                    "result_type": "action",
                    "format_type": format_action["format_type"],
                    "target_row": format_action["target_row"],
                    "bold": format_action["bold"],
                    "background_color": format_action["background_color"],
                    "summary": format_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for chart action (needs df for column analysis)
            chart_action = self._detect_chart_action(query, column_names, df)
            if chart_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Chart action detected: {chart_action}")
                chart_result = {
                    "success": True,
                    "action_type": "chart",
                    "result_type": "action",
                    "chart_spec": chart_action["chart_spec"],
                    "summary": chart_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }
                logger.info(f"[SimpleGPT] Returning chart result with keys: {list(chart_result.keys())}")
                logger.info(f"[SimpleGPT] chart_result['chart_spec']: {chart_result.get('chart_spec')}")
                return chart_result

            # Check for conditional formatting action
            conditional_action = self._detect_conditional_format_action(query, column_names, df)
            if conditional_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning conditional format action: {conditional_action}")
                return {
                    "success": True,
                    "action_type": "conditional_format",
                    "result_type": "action",
                    "rule": conditional_action["rule"],
                    "summary": conditional_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for pivot table action
            pivot_action = self._detect_pivot_action(query, column_names, df)
            if pivot_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning pivot table action: {pivot_action}")
                return {
                    "success": True,
                    "action_type": "pivot_table",
                    "result_type": "action",
                    "pivot_data": pivot_action["pivot_data"],
                    "group_column": pivot_action["group_column"],
                    "value_column": pivot_action["value_column"],
                    "agg_func": pivot_action["agg_func"],
                    "summary": pivot_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for CSV split action (text to columns)
            csv_split_action = self._detect_csv_split_action(query, column_names, df)
            if csv_split_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning CSV split action")
                return {
                    "success": True,
                    "action_type": "csv_split",
                    "result_type": "action",
                    "structured_data": csv_split_action["structured_data"],
                    "original_rows": csv_split_action["original_rows"],
                    "new_rows": csv_split_action["new_rows"],
                    "new_cols": csv_split_action["new_cols"],
                    "delimiter": csv_split_action["delimiter"],
                    "summary": csv_split_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for data cleaning action
            clean_action = self._detect_clean_action(query, column_names, df)
            if clean_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning clean data action: {clean_action}")
                return {
                    "success": True,
                    "action_type": "clean_data",
                    "result_type": "action",
                    "operations": clean_action["operations"],
                    "fill_value": clean_action["fill_value"],
                    "target_column": clean_action["target_column"],
                    "original_rows": clean_action["original_rows"],
                    "final_rows": clean_action["final_rows"],
                    "cleaned_data": clean_action["cleaned_data"],
                    "changes": clean_action["changes"],
                    "summary": clean_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for data validation action
            validation_action = self._detect_validation_action(query, column_names, df)
            if validation_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning data validation action: {validation_action}")
                return {
                    "success": True,
                    "action_type": "data_validation",
                    "result_type": "action",
                    "rule": validation_action["rule"],
                    "summary": validation_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for filter action
            filter_action = self._detect_filter_action(query, column_names, df)
            if filter_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning filter action: {filter_action}")
                return {
                    "success": True,
                    "action_type": "filter_data",
                    "result_type": "action",
                    "column_name": filter_action["column_name"],
                    "column_index": filter_action["column_index"],
                    "operator": filter_action["operator"],
                    "filter_value": filter_action["filter_value"],
                    "original_rows": filter_action["original_rows"],
                    "filtered_rows": filter_action["filtered_rows"],
                    "filtered_data": filter_action["filtered_data"],
                    "condition_str": filter_action["condition_str"],
                    "summary": filter_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # 1. Schema extraction
            logger.info(f"[SimpleGPT] Processing: {query[:50]}...")
            schema = self.schema_extractor.extract_schema(df)
            schema_prompt = self.schema_extractor.schema_to_prompt(schema)
            logger.info(f"[SimpleGPT] Schema: {schema['column_count']} cols, {schema['row_count']} rows")

            # 2. Generate and execute code (with retries)
            result = await self._generate_and_execute(
                query=query,
                df=df,
                schema_prompt=schema_prompt,
                custom_context=custom_context,
                history=history
            )

            if not result["success"]:
                return self._create_error_response(result.get("error", "Unknown error"), time.time() - start_time)

            # 3. Post-validation
            validation = await self._validate_result(query, result["result"])

            if validation == "BAD":
                logger.warning(f"[SimpleGPT] Post-validation failed, retrying with clarification...")
                # Retry with explicit clarification
                result = await self._generate_and_execute(
                    query=query,
                    df=df,
                    schema_prompt=schema_prompt,
                    custom_context=custom_context,
                    history=history,
                    clarification="–ü—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—É. –£–±–µ–¥–∏—Å—å —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—à—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: —Å–ø–∏—Å–æ–∫ –¥–ª—è '–∫–∞–∫–∏–µ', —á–∏—Å–ª–æ –¥–ª—è '—Å–∫–æ–ª—å–∫–æ', DataFrame –¥–ª—è '–ø–æ–∫–∞–∂–∏'."
                )

            # 4. Format response
            elapsed = time.time() - start_time
            formatted_result = self._format_result(result["result"])
            result_type = self._get_result_type(result["result"])

            # Use explanation from code if available, otherwise generate summary
            explanation = result.get("explanation", "")
            if explanation:
                summary = explanation
                logger.info(f"[SimpleGPT] Using explanation from code: {explanation[:100]}...")
            else:
                summary = self._generate_summary(result["result"], result_type, query)

            response = {
                "success": True,
                "result": formatted_result,
                "result_type": result_type,
                "summary": summary,
                "code": result.get("code"),
                "processing_time": f"{elapsed:.2f}s",
                "processor": "SimpleGPT v1.0",
                "validation": validation
            }

            # Check if this is a highlight query
            query_lower = query.lower()
            is_highlight_query = any(kw in query_lower for kw in ['–≤—ã–¥–µ–ª–∏', '–≤—ã–¥–µ–ª–∏—Ç—å', '–ø–æ–¥—Å–≤–µ—Ç–∏', '–ø–æ–¥—Å–≤–µ—Ç–∏—Ç—å', 'highlight', 'mark'])

            if is_highlight_query:
                logger.info(f"[SimpleGPT] Highlight query detected: {query[:50]}")
                # Extract row indices from the result for highlighting
                highlight_rows = self._extract_highlight_rows(result["result"])
                if highlight_rows:
                    response["highlight_rows"] = highlight_rows
                    response["highlighted_count"] = len(highlight_rows)
                    response["highlight_color"] = "#FFFF00"  # Yellow
                    response["highlight_message"] = f"–í—ã–¥–µ–ª–µ–Ω–æ {len(highlight_rows)} —Å—Ç—Ä–æ–∫"
                    response["result_type"] = "highlight"
                    logger.info(f"[SimpleGPT] Generated highlight_rows: {highlight_rows[:10]}... (total: {len(highlight_rows)})")

            # Add structured_data for tables/lists (only if NOT highlight query)
            if not is_highlight_query and result_type == "table" and isinstance(formatted_result, list):
                # Extract headers from first row keys (rows are dicts from DataFrame)
                headers = list(formatted_result[0].keys()) if formatted_result else []
                response["structured_data"] = {
                    "headers": headers,
                    "rows": formatted_result,
                    "display_mode": "sidebar_only" if len(formatted_result) <= 20 else "create_sheet"
                }
            elif result_type == "list" and isinstance(formatted_result, list):
                response["key_findings"] = formatted_result

            return response

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"[SimpleGPT] Error: {str(e)}")
            return self._create_error_response(str(e), elapsed)

    async def _generate_and_execute(
        self,
        query: str,
        df: pd.DataFrame,
        schema_prompt: str,
        custom_context: Optional[str] = None,
        history: List[Dict[str, Any]] = None,
        clarification: Optional[str] = None,
        previous_error: Optional[str] = None
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–¥ —Å retry."""

        for attempt in range(self.MAX_RETRIES + 1):
            # Generate code
            code = await self._generate_code(
                query=query,
                schema_prompt=schema_prompt,
                custom_context=custom_context,
                history=history,
                clarification=clarification,
                previous_error=previous_error
            )

            if not code:
                return {"success": False, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥"}

            # Validate code safety
            is_safe, safety_error = self._validate_code_safety(code)
            if not is_safe:
                previous_error = f"–ù–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–¥: {safety_error}"
                continue

            # Execute code
            try:
                exec_result = self._execute_code(code, df)
                return {"success": True, "result": exec_result['result'], "explanation": exec_result.get('explanation', ''), "code": code}
            except Exception as e:
                previous_error = f"{type(e).__name__}: {str(e)}"
                logger.warning(f"[SimpleGPT] Attempt {attempt + 1} failed: {previous_error}")
                continue

        return {"success": False, "error": previous_error}

    async def _generate_code(
        self,
        query: str,
        schema_prompt: str,
        custom_context: Optional[str] = None,
        history: List[Dict[str, Any]] = None,
        clarification: Optional[str] = None,
        previous_error: Optional[str] = None
    ) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Pandas –∫–æ–¥ —á–µ—Ä–µ–∑ GPT-4o."""

        user_prompt = f"""–°–•–ï–ú–ê –î–ê–ù–ù–´–•:
{schema_prompt}

–ó–ê–ü–†–û–°: {query}
"""

        # Build history context if available
        history_context = ""
        if history and len(history) > 0:
            history_context = "\n–ò–°–¢–û–†–ò–Ø –†–ê–ó–ì–û–í–û–†–ê (–ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã):\n"
            for i, item in enumerate(history[-5:], 1):
                prev_query = item.get('query', '')
                prev_response = item.get('response', '')
                if prev_query:
                    history_context += f"{i}. –í–æ–ø—Ä–æ—Å: {prev_query}\n"
                    if prev_response:
                        resp_str = str(prev_response)
                        history_context += f"   –û—Ç–≤–µ—Ç: {resp_str[:150]}...\n" if len(resp_str) > 150 else f"   –û—Ç–≤–µ—Ç: {resp_str}\n"
            history_context += "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –∏—Å—Ç–æ—Ä–∏—é —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ '–ø–æ—á–µ–º—É?' –∏–ª–∏ '–∞ –ü–µ—Ç—Ä–æ–≤?'\n"
            logger.info(f"[SimpleGPT] Added conversation history: {len(history)} messages")

        user_prompt = f"""–°–•–ï–ú–ê –î–ê–ù–ù–´–•:
{schema_prompt}
{history_context}
–ó–ê–ü–†–û–°: {query}
"""
        if custom_context:
            user_prompt += f"""
–†–û–õ–¨ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {custom_context}
–í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π —Ä–æ–ª—å –≤ explanation! –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –º–µ—Ç—Ä–∏–∫–∞—Ö –≤–∞–∂–Ω—ã—Ö –¥–ª—è —ç—Ç–æ–π —Ä–æ–ª–∏.
"""

        if clarification:
            user_prompt += f"\n–í–ê–ñ–ù–û: {clarification}\n"

        if previous_error:
            user_prompt += f"\n–ü–†–ï–î–´–î–£–©–ê–Ø –û–®–ò–ë–ö–ê (–∏–∑–±–µ–≥–∞–π –µ—ë): {previous_error}\n"

        try:
            response = await self.client.chat.completions.create(
                model=self.MODEL,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )

            content = response.choices[0].message.content

            # Extract code from markdown
            code_match = re.search(r'```python\s*(.*?)\s*```', content, re.DOTALL)
            if code_match:
                return code_match.group(1).strip()

            # Try without markdown
            if 'result' in content and '=' in content:
                return content.strip()

            return None

        except Exception as e:
            logger.error(f"[SimpleGPT] Code generation error: {e}")
            return None

    async def _validate_result(self, query: str, result: Any) -> str:
        """Post-validation: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""

        # Format result for validation
        result_str = self._format_for_validation(result)

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",  # Cheaper model for validation
                messages=[
                    {"role": "user", "content": self.VALIDATION_PROMPT.format(
                        query=query,
                        result=result_str
                    )}
                ],
                temperature=0,
                max_tokens=10
            )

            answer = response.choices[0].message.content.strip().upper()
            return "OK" if "OK" in answer else "BAD"

        except Exception as e:
            logger.warning(f"[SimpleGPT] Validation error: {e}")
            return "OK"  # Default to OK if validation fails

    def _validate_code_safety(self, code: str) -> tuple:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∫–æ–¥–∞."""
        for pattern in self.FORBIDDEN_PATTERNS:
            if re.search(pattern, code, re.IGNORECASE):
                return False, f"Forbidden pattern: {pattern}"

        # Check AST
        try:
            ast.parse(code)
        except SyntaxError as e:
            return False, f"Syntax error: {e}"

        return True, None

    def _execute_code(self, code: str, df: pd.DataFrame) -> dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–¥ –≤ sandbox. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å result –∏ explanation."""

        # Create safe namespace
        namespace = {
            'df': df.copy(),
            'pd': pd,
            'np': np,
            'result': None,
            'explanation': None,
            'datetime': __import__('datetime'),
            'timedelta': __import__('datetime').timedelta,
            're': __import__('re'),
            'math': __import__('math'),
        }

        exec(code, namespace)

        result = namespace.get('result')
        if result is None:
            raise ValueError("–ö–æ–¥ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç (result = None)")

        explanation = namespace.get('explanation', '')

        return {'result': result, 'explanation': explanation}

    def _format_result(self, result: Any) -> Any:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è JSON."""

        if isinstance(result, pd.DataFrame):
            # Convert to list of dicts
            return result.to_dict(orient='records')
        elif isinstance(result, pd.Series):
            return result.tolist()
        elif isinstance(result, np.ndarray):
            return result.tolist()
        elif isinstance(result, (np.integer, np.floating)):
            return float(result)
        elif isinstance(result, list):
            return result
        else:
            return result

    def _format_for_validation(self, result: Any) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""

        if isinstance(result, pd.DataFrame):
            if len(result) > 5:
                return f"DataFrame —Å {len(result)} —Å—Ç—Ä–æ–∫–∞–º–∏. –ü–µ—Ä–≤—ã–µ 3: {result.head(3).to_dict(orient='records')}"
            return str(result.to_dict(orient='records'))
        elif isinstance(result, (list, pd.Series)):
            items = list(result)[:10]
            return f"–°–ø–∏—Å–æ–∫: {items}" + (f" (–≤—Å–µ–≥–æ {len(result)})" if len(result) > 10 else "")
        elif isinstance(result, (int, float, np.integer, np.floating)):
            return f"–ß–∏—Å–ª–æ: {result}"
        else:
            return str(result)[:500]

    def _get_result_type(self, result: Any) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""

        if isinstance(result, pd.DataFrame):
            return "table"
        elif isinstance(result, (list, pd.Series)):
            return "list"
        elif isinstance(result, (int, float, np.integer, np.floating)):
            return "number"
        else:
            return "text"

    def _generate_summary(self, result: Any, result_type: str, query: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""

        if result_type == "number":
            # –î–ª—è —á–∏—Å–µ–ª - –ø—Ä–æ—Å—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ
            if isinstance(result, float):
                return f"{result:,.2f}".replace(",", " ")
            return str(result)

        elif result_type == "list":
            # –î–ª—è —Å–ø–∏—Å–∫–æ–≤ - –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            items = list(result) if isinstance(result, pd.Series) else result
            if len(items) == 0:
                return "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            elif len(items) <= 5:
                return ", ".join(str(item) for item in items)
            else:
                first_items = ", ".join(str(item) for item in items[:5])
                return f"{first_items} (–∏ –µ—â—ë {len(items) - 5})"

        elif result_type == "table":
            # –î–ª—è —Ç–∞–±–ª–∏—Ü - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            if isinstance(result, pd.DataFrame):
                return f"–ù–∞–π–¥–µ–Ω–æ {len(result)} –∑–∞–ø–∏—Å–µ–π"
            elif isinstance(result, list):
                return f"–ù–∞–π–¥–µ–Ω–æ {len(result)} –∑–∞–ø–∏—Å–µ–π"
            return "–¢–∞–±–ª–∏—Ü–∞ –¥–∞–Ω–Ω—ã—Ö"

        else:
            # –¢–µ–∫—Å—Ç
            return str(result)[:200] if result else "–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω"

    def _extract_highlight_rows(self, result: Any) -> List[int]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç list[int] —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫ (1-based –¥–ª—è Google Sheets, +1 –¥–ª—è header).
        """
        try:
            if isinstance(result, pd.DataFrame):
                # Get original DataFrame indices and convert to Google Sheets row numbers
                # +2 because: +1 for 1-based indexing, +1 for header row
                indices = result.index.tolist()
                row_numbers = [int(idx) + 2 for idx in indices]
                logger.info(f"[SimpleGPT] Extracted {len(row_numbers)} row indices from DataFrame")
                return row_numbers
            elif isinstance(result, pd.Series):
                # Series with row indices
                indices = result.index.tolist()
                row_numbers = [int(idx) + 2 for idx in indices]
                return row_numbers
            elif isinstance(result, list):
                # If result is a list of row numbers
                if all(isinstance(x, (int, np.integer)) for x in result):
                    return [int(x) + 2 for x in result]
                # If result is list of dicts (from DataFrame.to_dict), can't extract indices
                return []
            else:
                return []
        except Exception as e:
            logger.error(f"[SimpleGPT] Error extracting highlight rows: {e}")
            return []

    def _create_error_response(self, error: str, elapsed: float) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—ë—Ç –æ—Ç–≤–µ—Ç –æ–± –æ—à–∏–±–∫–µ."""
        return {
            "success": False,
            "error": error,
            "processing_time": f"{elapsed:.2f}s",
            "processor": "SimpleGPT v1.0"
        }


# Singleton
_processor = None

def get_simple_gpt_processor() -> SimpleGPTProcessor:
    global _processor
    if _processor is None:
        _processor = SimpleGPTProcessor()
    return _processor
