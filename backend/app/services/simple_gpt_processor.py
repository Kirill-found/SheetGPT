"""
Simple GPT Processor v1.0.0 - Ð£Ð¿Ñ€Ð¾Ñ‰Ñ‘Ð½Ð½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð±ÐµÐ· Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð²

ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SIMPLE GPT PROCESSOR v1.0              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  1. Schema Extraction (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾, 0 tokens)          â”‚
â”‚     â†’ Ð¢Ð¸Ð¿Ñ‹ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº, ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ             â”‚
â”‚                                                     â”‚
â”‚  2. GPT-4o Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Pandas ÐºÐ¾Ð´ (Ð’Ð¡Ð•Ð“Ð”Ð)           â”‚
â”‚     â†’ Ð‘ÐµÐ· ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸, Ð±ÐµÐ· Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð²              â”‚
â”‚                                                     â”‚
â”‚  3. Execute + Self-Correction (Ð´Ð¾ 3 Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº)        â”‚
â”‚                                                     â”‚
â”‚  4. POST-VALIDATION                                 â”‚
â”‚     â†’ GPT Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð°            â”‚
â”‚     â†’ Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ â†’ retry Ñ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸ÐµÐ¼                 â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ÐŸÑ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð°:
- ÐÐµÑ‚ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð²
- GPT Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð»ÑŽÐ±Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹
- Self-correction Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ…
- Post-validation Ð´Ð»Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from openai import AsyncOpenAI
import os
import time
import logging
import re
import ast

from .schema_extractor import SchemaExtractor, get_schema_extractor

logger = logging.getLogger(__name__)


class SimpleGPTProcessor:
    """
    Ð£Ð¿Ñ€Ð¾Ñ‰Ñ‘Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ Ð½Ð° Ð±Ð°Ð·Ðµ GPT-4o.
    Ð’ÑÑ‘ Ñ‡ÐµÑ€ÐµÐ· LLM, Ð±ÐµÐ· Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð² Ð¸ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸.
    """

    MODEL = "gpt-4o"  # Best quality
    MAX_RETRIES = 2

    # Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ: Ñ€Ð°Ð·Ñ€ÐµÑˆÑ‘Ð½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÑƒÐ»Ð¸
    ALLOWED_IMPORTS = {'pandas', 'pd', 'numpy', 'np', 'datetime', 'timedelta', 're', 'math'}

    # Ð—Ð°Ð¿Ñ€ÐµÑ‰Ñ‘Ð½Ð½Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹
    FORBIDDEN_PATTERNS = [
        r'\bexec\b', r'\beval\b', r'\bcompile\b',
        r'\b__\w+__\b', r'\bopen\b', r'\bfile\b',
        r'\bos\b', r'\bsys\b', r'\bsubprocess\b',
        r'\brequests\b', r'\burllib\b', r'\bsocket\b', r'\bpickle\b',
    ]

    SYSTEM_PROMPT = """Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚-Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Python/Pandas. Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° - Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð“Ð›Ð£Ð‘ÐžÐšÐ˜Ð™, Ð˜Ð¡Ð§Ð•Ð ÐŸÐ«Ð’ÐÐ®Ð©Ð˜Ð™ Ð°Ð½Ð°Ð»Ð¸Ð·.

Ð—ÐÐ”ÐÐ§Ð: ÐÐ°Ð¿Ð¸ÑˆÐ¸ Python ÐºÐ¾Ð´ Ð´Ð»Ñ ÐŸÐžÐ›ÐÐžÐ“Ðž Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.

ÐŸÐ ÐÐ’Ð˜Ð›Ð:
1. DataFrame ÑƒÐ¶Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð² Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ `df`
2. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž pandas, numpy, datetime, math
3. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸ Ð² Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ `result`
4. ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž ÑÐ¾Ð·Ð´Ð°Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ `explanation` Ñ Ð”Ð•Ð¢ÐÐ›Ð¬ÐÐ«Ðœ Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð˜Ð ÐžÐ’ÐÐÐÐ«Ðœ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼
5. ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ print(), Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€Ð¸ÑÐ²Ð¾Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
6. ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ð¹ NaN Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ (dropna() Ð¸Ð»Ð¸ fillna())
7. Ð”Ð»Ñ Ñ‡Ð¸ÑÐµÐ»: pd.to_numeric(df[col], errors='coerce')

âš ï¸ Ð“Ð›ÐÐ’ÐÐžÐ• ÐŸÐ ÐÐ’Ð˜Ð›Ðž - Ð“Ð›Ð£Ð‘Ð˜ÐÐ ÐÐÐÐ›Ð˜Ð—Ð:
- ÐÐ• Ð´Ð°Ð²Ð°Ð¹ Ð¿Ð¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚Ð½Ñ‹Ñ… Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð¸Ð· 2-3 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹!
- Ð’Ð¡Ð•Ð“Ð”Ð Ð²ÐºÐ»ÑŽÑ‡Ð°Ð¹: Ñ†Ð¸Ñ„Ñ€Ñ‹, Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñ‹, ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ, Ð²Ñ‹Ð²Ð¾Ð´Ñ‹
- Ð’Ð¡Ð•Ð“Ð”Ð Ð¾Ð±ÑŠÑÑÐ½ÑÐ¹ Ð§Ð¢Ðž ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð¸ ÐŸÐžÐ§Ð•ÐœÐ£ ÑÑ‚Ð¾ Ð²Ð°Ð¶Ð½Ð¾
- ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 5-7 Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð² Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð´Ð»Ñ Ð»ÑŽÐ±Ð¾Ð³Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°

ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž - Ð¤ÐžÐ ÐœÐÐ¢ explanation (Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð˜Ð ÐžÐ’ÐÐÐÐ«Ð™ ÐžÐ¢Ð’Ð•Ð¢):
Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ñ‡Ð¸Ñ‚Ð°Ð±ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸:
- **Ð–Ð¸Ñ€Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚** Ð´Ð»Ñ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð²
- Ð¡Ð¿Ð¸ÑÐºÐ¸ Ñ â€¢ Ð¸Ð»Ð¸ Ñ†Ð¸Ñ„Ñ€Ð°Ð¼Ð¸ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹
- Ð Ð°Ð·Ð´ÐµÐ»ÑÐ¹ Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð±Ð»Ð¾ÐºÐ¸ Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹
- Ð­Ð¼Ð¾Ð´Ð·Ð¸ Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ ÑÐµÐºÑ†Ð¸Ð¹ (ðŸ“ŠðŸ“ˆðŸ’¡ðŸ”ðŸ’°ðŸ†)

Ð¨ÐÐ‘Ð›ÐžÐÐ« explanation:

1. Ð”Ð»Ñ Ð¡Ð ÐÐ’ÐÐ•ÐÐ˜Ð¯ Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð¾Ð²/ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹ (ÑÑ€Ð°Ð²Ð½Ð¸, Ñ€Ð°Ð·Ð½Ð¸Ñ†Ð°, vs):
```
**ðŸ“Š Ð¡Ñ€Ð°Ð²Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·: [ÐŸÐµÑ€Ð¸Ð¾Ð´1] vs [ÐŸÐµÑ€Ð¸Ð¾Ð´2]**

ðŸ“ˆ ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»Ð¸:
â€¢ [ÐŸÐµÑ€Ð¸Ð¾Ð´1]: [ÑÑƒÐ¼Ð¼Ð°] Ñ€ÑƒÐ±. ([N] Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹)
â€¢ [ÐŸÐµÑ€Ð¸Ð¾Ð´2]: [ÑÑƒÐ¼Ð¼Ð°] Ñ€ÑƒÐ±. ([N] Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹)

ðŸ“‰ Ð”Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ° Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹:
â€¢ ÐÐ±ÑÐ¾Ð»ÑŽÑ‚Ð½Ð°Ñ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ð°: [X] Ñ€ÑƒÐ±.
â€¢ ÐžÑ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ: [Y]% ([Ñ€Ð¾ÑÑ‚/Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ])
â€¢ Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ ÑÑ€ÐµÐ´Ð½ÐµÐ³Ð¾ Ñ‡ÐµÐºÐ°: [Z] Ñ€ÑƒÐ±.

ðŸ” Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð±Ð¾Ñ€:
â€¢ Ð¢Ð¾Ð¿ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ñ [ÐŸÐµÑ€Ð¸Ð¾Ð´1]: [Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ] â€” [ÑÑƒÐ¼Ð¼Ð°]
â€¢ Ð¢Ð¾Ð¿ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ñ [ÐŸÐµÑ€Ð¸Ð¾Ð´2]: [Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ] â€” [ÑÑƒÐ¼Ð¼Ð°]
â€¢ ÐÐ°Ð¸Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¹ Ñ€Ð¾ÑÑ‚ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð°: [ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ] +[X]%
â€¢ ÐÐ°Ð¸Ð±Ð¾Ð»ÑŒÑˆÐµÐµ Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ: [ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ] -[X]%

ðŸ’¡ Ð’Ñ‹Ð²Ð¾Ð´Ñ‹:
â€¢ [Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´ Ð¾ Ñ‚Ñ€ÐµÐ½Ð´Ðµ]
â€¢ [Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ñ‹ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹]
â€¢ [Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ]
```

2. Ð”Ð»Ñ "ÐºÑ‚Ð¾ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹/Ñ…ÑƒÐ´ÑˆÐ¸Ð¹" (Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³):
```
**ðŸ† Ð›Ð¸Ð´ÐµÑ€: [Ð˜Ð¼Ñ]**

ðŸ“Š ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³:
1. [Ð˜Ð¼Ñ1]: [ÑÑƒÐ¼Ð¼Ð°] Ñ€ÑƒÐ±. ([N] ÑÐ´ÐµÐ»Ð¾Ðº, ÑÑ€. Ñ‡ÐµÐº [X])
2. [Ð˜Ð¼Ñ2]: [ÑÑƒÐ¼Ð¼Ð°] Ñ€ÑƒÐ±. ([N] ÑÐ´ÐµÐ»Ð¾Ðº, ÑÑ€. Ñ‡ÐµÐº [X])
3. [Ð˜Ð¼Ñ3]: [ÑÑƒÐ¼Ð¼Ð°] Ñ€ÑƒÐ±. ([N] ÑÐ´ÐµÐ»Ð¾Ðº, ÑÑ€. Ñ‡ÐµÐº [X])

ðŸ“ˆ ÐÐ½Ð°Ð»Ð¸Ð· Ð»Ð¸Ð´ÐµÑ€Ð°:
â€¢ Ð”Ð¾Ð»Ñ Ð¾Ñ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾: [X]%
â€¢ ÐžÑ‚Ñ€Ñ‹Ð² Ð¾Ñ‚ 2-Ð³Ð¾ Ð¼ÐµÑÑ‚Ð°: [Y] Ñ€ÑƒÐ±. ([Z]%)
â€¢ Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹ Ñ‡ÐµÐº: [X] Ñ€ÑƒÐ±. (vs ÑÑ€ÐµÐ´Ð½Ð¸Ð¹ Ð¿Ð¾ Ð²ÑÐµÐ¼ [Y])

ðŸ’¡ ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚:
â€¢ [ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ð° 1: Ð¾Ð±ÑŠÑ‘Ð¼/Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð°/Ñ€Ð°Ð·Ð¼ÐµÑ€ ÑÐ´ÐµÐ»Ð¾Ðº]
â€¢ [ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ð° 2: ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸ÐºÐ°]
```

3. Ð”Ð»Ñ "Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ?" (Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ):
```
**ðŸ” ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½: [Ñ‚ÐµÐ¼Ð°]**

ðŸ“Š Ð¤Ð°ÐºÑ‚Ñ‹ Ð¸ Ñ†Ð¸Ñ„Ñ€Ñ‹:
â€¢ [Ð¤Ð°ÐºÑ‚ 1 Ñ Ñ‡Ð¸ÑÐ»Ð°Ð¼Ð¸]
â€¢ [Ð¤Ð°ÐºÑ‚ 2 Ñ Ñ‡Ð¸ÑÐ»Ð°Ð¼Ð¸]
â€¢ [Ð¤Ð°ÐºÑ‚ 3 Ñ Ñ‡Ð¸ÑÐ»Ð°Ð¼Ð¸]

ðŸ“ˆ ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹:
â€¢ [Ð¤Ð°ÐºÑ‚Ð¾Ñ€ 1]: Ð²ÐºÐ»Ð°Ð´ [X]%
â€¢ [Ð¤Ð°ÐºÑ‚Ð¾Ñ€ 2]: Ð²ÐºÐ»Ð°Ð´ [X]%

ðŸ”Ž Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð±Ð¾Ñ€:
â€¢ [Ð“Ð»ÑƒÐ±Ð¾ÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð´Ð°Ð½Ð½Ñ‹Ñ…]
â€¢ [ÐšÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ð¸ Ð¸ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸]

ðŸ’¡ Ð—Ð°ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ:
â€¢ [Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°]
â€¢ [Ð§Ñ‚Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ]
```

4. Ð”Ð»Ñ "ÑÐºÐ¾Ð»ÑŒÐºÐ¾/ÐºÐ°ÐºÐ°Ñ ÑÑƒÐ¼Ð¼Ð°" (Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°ÑÑ‡Ñ‘Ñ‚):
```
**ðŸ’° Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: [Ð§Ð¸ÑÐ»Ð¾/Ð¡ÑƒÐ¼Ð¼Ð°]**

ðŸ“‹ ÐšÐ°Ðº Ð¿Ð¾ÑÑ‡Ð¸Ñ‚Ð°Ð½Ð¾:
â€¢ ÐœÐµÑ‚Ð¾Ð´: [Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ]
â€¢ Ð—Ð°Ð¿Ð¸ÑÐµÐ¹ ÑƒÑ‡Ñ‚ÐµÐ½Ð¾: [N] Ð¸Ð· [M]

ðŸ“Š ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:
â€¢ Ð”Ð¾Ð»Ñ Ð¾Ñ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾: [X]%
â€¢ Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ ÑÐ¾ ÑÑ€ÐµÐ´Ð½Ð¸Ð¼: [+/-X]%

ðŸ’¡ Ð˜Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ:
â€¢ [Ð§Ñ‚Ð¾ ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð´Ð»Ñ Ð±Ð¸Ð·Ð½ÐµÑÐ°]
```

Ð’ÐÐ–ÐÐž - ÐŸÐžÐÐ˜ÐœÐÐ™ ÐÐÐœÐ•Ð Ð•ÐÐ˜Ð•:
- "ÑÑ€Ð°Ð²Ð½Ð¸/Ñ€Ð°Ð·Ð½Ð¸Ñ†Ð°/vs" -> Ð”Ð•Ð¢ÐÐ›Ð¬ÐÐžÐ• ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ñ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð°Ð¼Ð¸, Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¾Ð¹ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´Ð°Ð¼Ð¸
- "ÐºÐ°ÐºÐ¸Ðµ/ÐºÐ°ÐºÐ¾Ð¹/Ñ‡Ñ‚Ð¾" -> Ð¡ÐŸÐ˜Ð¡ÐžÐš Ñ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ°Ð¼Ð¸ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
- "ÑÐºÐ¾Ð»ÑŒÐºÐ¾" -> Ñ‡Ð¸ÑÐ»Ð¾ + ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ + Ð´Ð¾Ð»Ñ Ð¾Ñ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾
- "Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ?" -> Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¸ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑÐ¼Ð¸
- "Ñ‚Ð¾Ð¿ N" -> Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³ + Ð°Ð½Ð°Ð»Ð¸Ð· Ð»Ð¸Ð´ÐµÑ€Ð¾Ð² + Ð²Ñ‹Ð²Ð¾Ð´Ñ‹
- "Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹/Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°ÐºÐ°Ð·" -> Ð½Ð°Ð¹Ñ‚Ð¸ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ñ max/min Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»Ð¸
- "ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð¾ Ð³Ð¾Ñ€Ð¾Ð´Ð°Ð¼/Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼" -> Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ñ Ð¿Ð¾Ð´ÑÑ‡Ñ‘Ñ‚Ð¾Ð¼

ÐŸÐ Ð˜ÐœÐ•Ð Ð«:

Ð—Ð°Ð¿Ñ€Ð¾Ñ: "Ð¢Ð¾Ð¿ 5 Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² Ð¿Ð¾ ÑÑƒÐ¼Ð¼Ðµ"
```python
# Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ð¼ Ð¸ ÑÑƒÐ¼Ð¼Ð¸Ñ€ÑƒÐµÐ¼
product_col = None
sum_col = None
for col in df.columns:
    col_lower = col.lower()
    if any(x in col_lower for x in ['Ñ‚Ð¾Ð²Ð°Ñ€', 'product', 'Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ', 'name', 'Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ']):
        product_col = col
    if any(x in col_lower for x in ['ÑÑƒÐ¼Ð¼', 'sum', 'total', 'amount']):
        sum_col = col

if product_col and sum_col:
    top5 = df.groupby(product_col)[sum_col].sum().sort_values(ascending=False).head(5)
    result = top5.to_dict()
    explanation = f"**ðŸ† Ð¢Ð¾Ð¿ 5 Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² Ð¿Ð¾ ÑÑƒÐ¼Ð¼Ðµ:**\n\n"
    for i, (name, val) in enumerate(top5.items(), 1):
        explanation += f"{i}. {name}: {val:,.0f} Ñ€ÑƒÐ±.\n"
    total = top5.sum()
    explanation += f"\nðŸ’° Ð˜Ñ‚Ð¾Ð³Ð¾ Ñ‚Ð¾Ð¿-5: {total:,.0f} Ñ€ÑƒÐ±."
else:
    result = "ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ð¼Ð¸ Ð¸ ÑÑƒÐ¼Ð¼Ð°Ð¼Ð¸"
    explanation = result
```

Ð—Ð°Ð¿Ñ€Ð¾Ñ: "ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°ÐºÐ°Ð·Ð¾Ð² Ð¿Ð¾ Ð³Ð¾Ñ€Ð¾Ð´Ð°Ð¼" Ð¸Ð»Ð¸ "Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð·Ð°ÐºÐ°Ð·Ð¾Ð² Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¼ Ð³Ð¾Ñ€Ð¾Ð´Ðµ"
```python
# ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ Ñ Ð³Ð¾Ñ€Ð¾Ð´Ð°Ð¼Ð¸
city_col = None
for col in df.columns:
    col_lower = col.lower()
    if any(x in col_lower for x in ['Ð³Ð¾Ñ€Ð¾Ð´', 'city', 'Ñ€ÐµÐ³Ð¸Ð¾Ð½', 'region', 'Ð»Ð¾ÐºÐ°Ñ†']):
        city_col = col
        break

if city_col:
    city_counts = df[city_col].value_counts().sort_values(ascending=False)
    result = city_counts.to_dict()
    explanation = f"**ðŸ“ ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°ÐºÐ°Ð·Ð¾Ð² Ð¿Ð¾ Ð³Ð¾Ñ€Ð¾Ð´Ð°Ð¼:**\n\n"
    for city, count in city_counts.head(10).items():
        explanation += f"â€¢ {city}: {count}\n"
    if len(city_counts) > 10:
        explanation += f"â€¢ ...Ð¸ ÐµÑ‰Ñ‘ {len(city_counts) - 10} Ð³Ð¾Ñ€Ð¾Ð´Ð¾Ð²\n"
    explanation += f"\nðŸ“Š Ð’ÑÐµÐ³Ð¾ Ð³Ð¾Ñ€Ð¾Ð´Ð¾Ð²: {len(city_counts)}"
else:
    result = "ÐšÐ¾Ð»Ð¾Ð½ÐºÐ° Ñ Ð³Ð¾Ñ€Ð¾Ð´Ð°Ð¼Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°"
    explanation = result
```

Ð—Ð°Ð¿Ñ€Ð¾Ñ: "ÐŸÑ€Ð¾Ð´Ð°Ð¶Ð¸ Ð¿Ð¾ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼" Ð¸Ð»Ð¸ "Ð¡ÑƒÐ¼Ð¼Ð° Ð¿Ð¾ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼"
```python
# ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ Ñ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼Ð¸
manager_col = None
sum_col = None
for col in df.columns:
    col_lower = col.lower()
    if any(x in col_lower for x in ['Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€', 'manager', 'Ð¿Ñ€Ð¾Ð´Ð°Ð²ÐµÑ†', 'seller', 'ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ðº']):
        manager_col = col
    if any(x in col_lower for x in ['ÑÑƒÐ¼Ð¼', 'sum', 'total', 'amount']):
        sum_col = col

if manager_col and sum_col:
    sales = df.groupby(manager_col)[sum_col].sum().sort_values(ascending=False)
    result = sales.to_dict()
    explanation = f"**ðŸ“Š ÐŸÑ€Ð¾Ð´Ð°Ð¶Ð¸ Ð¿Ð¾ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼:**\n\n"
    for manager, total in sales.items():
        pct = (total / sales.sum()) * 100
        explanation += f"â€¢ {manager}: {total:,.0f} Ñ€ÑƒÐ±. ({pct:.1f}%)\n"
    explanation += f"\nðŸ’° ÐžÐ±Ñ‰Ð°Ñ ÑÑƒÐ¼Ð¼Ð°: {sales.sum():,.0f} Ñ€ÑƒÐ±."
else:
    result = "ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼Ð¸ Ð¸ ÑÑƒÐ¼Ð¼Ð°Ð¼Ð¸"
    explanation = result
```


Ð—Ð°Ð¿Ñ€Ð¾Ñ: "ÐšÐ°ÐºÐ¾Ð¹ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ ÑÐ°Ð¼Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹"
```python
sales = df.groupby('ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€')['Ð¡ÑƒÐ¼Ð¼Ð°'].sum().sort_values(ascending=False)
result = sales.idxmax()
top3 = sales.head(3)
explanation = f"**ÐžÑ‚Ð²ÐµÑ‚: {result}**

"
explanation += "ðŸ“Š ÐŸÑ€Ð¾Ð´Ð°Ð¶Ð¸ Ð¿Ð¾ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼:
"
for i, (name, val) in enumerate(top3.items(), 1):
    explanation += f"â€¢ {name}: {val:,.0f} Ñ€ÑƒÐ±.
"
if len(sales) > 3:
    explanation += f"â€¢ ...Ð¸ ÐµÑ‰Ñ‘ {len(sales)-3} Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð¾Ð²
"
explanation += f"
ðŸ’¡ {result} Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚ Ñ Ð¾Ñ‚Ñ€Ñ‹Ð²Ð¾Ð¼ {sales.iloc[0] - sales.iloc[1]:,.0f} Ñ€ÑƒÐ±. Ð¾Ñ‚ Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ð¼ÐµÑÑ‚Ð°."
```

Ð—Ð°Ð¿Ñ€Ð¾Ñ: "Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ?" (Ð¿Ð¾ÑÐ»Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸)
```python
sales = df.groupby('ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€')['Ð¡ÑƒÐ¼Ð¼Ð°'].sum().sort_values(ascending=False)
counts = df.groupby('ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€').size()
leader = sales.index[0]
leader_sum = sales.iloc[0]
leader_count = counts[leader]
second = sales.index[1] if len(sales) > 1 else None
explanation = f"**{leader} Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚** Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡Ñ‚Ð¾:

"
explanation += "ðŸ“ˆ ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹:
"
explanation += f"â€¢ ÐžÐ±Ñ‰Ð°Ñ ÑÑƒÐ¼Ð¼Ð° Ð¿Ñ€Ð¾Ð´Ð°Ð¶: {leader_sum:,.0f} Ñ€ÑƒÐ±.
"
explanation += f"â€¢ ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ´ÐµÐ»Ð¾Ðº: {leader_count}
"
if second:
    diff = leader_sum - sales.iloc[1]
    pct = diff / sales.iloc[1] * 100
    explanation += f"â€¢ Ð Ð°Ð·Ð½Ð¸Ñ†Ð° Ñ {second}: +{diff:,.0f} Ñ€ÑƒÐ±. (+{pct:.0f}%)
"
explanation += f"
ðŸ’¡ Ð’Ñ‹ÑÐ¾ÐºÐ¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ñ‹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼ Ð¾Ð±ÑŠÑ‘Ð¼Ð¾Ð¼ Ð¸/Ð¸Ð»Ð¸ ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ð¼Ð¸ ÑÐ´ÐµÐ»ÐºÐ°Ð¼Ð¸."
result = sales.to_dict()
```

Ð—Ð°Ð¿Ñ€Ð¾Ñ: "Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¾Ð´Ð°Ð¶ Ñƒ Ð˜Ð²Ð°Ð½Ð¾Ð²Ð°"
```python
ivanov = df[df['ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€'].str.contains('Ð˜Ð²Ð°Ð½Ð¾Ð²', case=False, na=False)]
result = len(ivanov)
total = ivanov['Ð¡ÑƒÐ¼Ð¼Ð°'].sum()
avg = ivanov['Ð¡ÑƒÐ¼Ð¼Ð°'].mean()
explanation = f"**{result} Ð¿Ñ€Ð¾Ð´Ð°Ð¶**

"
explanation += "ðŸ“‹ Ð”ÐµÑ‚Ð°Ð»Ð¸:
"
explanation += f"â€¢ ÐžÐ±Ñ‰Ð°Ñ ÑÑƒÐ¼Ð¼Ð°: {total:,.0f} Ñ€ÑƒÐ±.
"
explanation += f"â€¢ Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹ Ñ‡ÐµÐº: {avg:,.0f} Ñ€ÑƒÐ±.
"
```

Ð—Ð°Ð¿Ñ€Ð¾Ñ: "ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°ÐºÐ°Ð·" Ð¸Ð»Ð¸ "ÐšÐ°ÐºÐ¾Ð¹ ÑÐ°Ð¼Ñ‹Ð¹ ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ð¹ Ð·Ð°ÐºÐ°Ð·"
```python
# ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑƒÐ¼Ð¼Ð¾Ð¹
sum_col = 'Ð¡ÑƒÐ¼Ð¼Ð°'  # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ ÑÑƒÐ¼Ð¼Ð¾Ð¹ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸
if sum_col not in df.columns:
    # Ð˜Ñ‰ÐµÐ¼ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ Ñ ÑÑƒÐ¼Ð¼Ð¾Ð¹ Ð¿Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ
    for col in df.columns:
        if any(x in col.lower() for x in ['ÑÑƒÐ¼Ð¼', 'sum', 'total', 'amount', 'Ñ†ÐµÐ½Ð°', 'price']):
            sum_col = col
            break

numeric_col = pd.to_numeric(df[sum_col], errors='coerce')
max_idx = numeric_col.idxmax()
max_row = df.loc[max_idx]
max_value = numeric_col.max()

# ÐÐ°Ð¹Ñ‚Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ñ‚Ð¾Ð²Ð°Ñ€Ð°/Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°
product_col = None
for col in df.columns:
    if any(x in col.lower() for x in ['Ñ‚Ð¾Ð²Ð°Ñ€', 'product', 'Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ', 'name', 'Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ']):
        product_col = col
        break

product_name = max_row[product_col] if product_col else "N/A"

result = {"max_value": max_value, "product": product_name, "row": max_row.to_dict()}
explanation = f"**ðŸ’° ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°ÐºÐ°Ð·: {max_value:,.0f} Ñ€ÑƒÐ±.**\n\n"
explanation += f"ðŸ“¦ Ð¢Ð¾Ð²Ð°Ñ€: {product_name}\n"
explanation += f"ðŸ“Š Ð”ÐµÑ‚Ð°Ð»Ð¸ Ð·Ð°ÐºÐ°Ð·Ð°:\n"
for col, val in max_row.items():
    if pd.notna(val) and str(val).strip():
        explanation += f"â€¢ {col}: {val}\n"
```

Ð—Ð°Ð¿Ñ€Ð¾Ñ: "ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°ÐºÐ°Ð·" Ð¸Ð»Ð¸ "ÐšÐ°ÐºÐ¾Ð¹ ÑÐ°Ð¼Ñ‹Ð¹ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹ Ð·Ð°ÐºÐ°Ð·"
```python
# ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ñ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑƒÐ¼Ð¼Ð¾Ð¹
sum_col = 'Ð¡ÑƒÐ¼Ð¼Ð°'
if sum_col not in df.columns:
    for col in df.columns:
        if any(x in col.lower() for x in ['ÑÑƒÐ¼Ð¼', 'sum', 'total', 'amount', 'Ñ†ÐµÐ½Ð°', 'price']):
            sum_col = col
            break

numeric_col = pd.to_numeric(df[sum_col], errors='coerce')
# Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ NaN Ð¸ Ð½ÑƒÐ»Ð¸
valid_mask = (numeric_col > 0) & numeric_col.notna()
min_idx = numeric_col[valid_mask].idxmin()
min_row = df.loc[min_idx]
min_value = numeric_col[valid_mask].min()

product_col = None
for col in df.columns:
    if any(x in col.lower() for x in ['Ñ‚Ð¾Ð²Ð°Ñ€', 'product', 'Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ', 'name', 'Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ']):
        product_col = col
        break

product_name = min_row[product_col] if product_col else "N/A"

result = {"min_value": min_value, "product": product_name, "row": min_row.to_dict()}
explanation = f"**ðŸ’° ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°ÐºÐ°Ð·: {min_value:,.0f} Ñ€ÑƒÐ±.**\n\n"
explanation += f"ðŸ“¦ Ð¢Ð¾Ð²Ð°Ñ€: {product_name}\n"
explanation += f"ðŸ“Š Ð”ÐµÑ‚Ð°Ð»Ð¸ Ð·Ð°ÐºÐ°Ð·Ð°:\n"
for col, val in min_row.items():
    if pd.notna(val) and str(val).strip():
        explanation += f"â€¢ {col}: {val}\n"
```

Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž ÐºÐ¾Ð´ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ```python ... ```
"""

    VALIDATION_PROMPT = """Ð¢Ñ‹ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑˆÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.

Ð—ÐÐŸÐ ÐžÐ¡ ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¯: {query}

Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢: {result}

Ð—ÐÐ”ÐÐ§Ð: ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð¾Ð´Ð½Ð¸Ð¼ ÑÐ»Ð¾Ð²Ð¾Ð¼:
- "OK" - ÐµÑÐ»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð½Ð° Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
- "BAD" - ÐµÑÐ»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÐÐ• Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð½Ð° Ð·Ð°Ð¿Ñ€Ð¾Ñ (Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð½Ðµ Ñ‚Ð° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ, Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚)

ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
- Ð—Ð°Ð¿Ñ€Ð¾Ñ "ÐºÐ°ÐºÐ¸Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹" â†’ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ["Ð¢ÐµÐ»ÐµÑ„Ð¾Ð½", "ÐÐ¾ÑƒÑ‚Ð±ÑƒÐº"] â†’ OK
- Ð—Ð°Ð¿Ñ€Ð¾Ñ "ÐºÐ°ÐºÐ¸Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹" â†’ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ 5 (Ñ‡Ð¸ÑÐ»Ð¾) â†’ BAD (Ð½ÑƒÐ¶ÐµÐ½ ÑÐ¿Ð¸ÑÐ¾Ðº, Ð½Ðµ Ñ‡Ð¸ÑÐ»Ð¾)
- Ð—Ð°Ð¿Ñ€Ð¾Ñ "ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¾Ð´Ð°Ð¶" â†’ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ 42 â†’ OK
- Ð—Ð°Ð¿Ñ€Ð¾Ñ "ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¾Ð´Ð°Ð¶" â†’ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ["Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚1", "Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚2"] â†’ BAD (Ð½ÑƒÐ¶Ð½Ð¾ Ñ‡Ð¸ÑÐ»Ð¾)

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð¢ÐžÐ›Ð¬ÐšÐž "OK" Ð¸Ð»Ð¸ "BAD":
"""

    # ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° Ð´Ð»Ñ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð½Ð°Ð´ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ (Ð½Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·, Ð° Ð¼Ð¾Ð´Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ)
    SORT_KEYWORDS = ['Ð¾Ñ‚ÑÐ¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐ¹', 'ÑÐ¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐ¹', 'ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°', 'ÑƒÐ¿Ð¾Ñ€ÑÐ´Ð¾Ñ‡ÑŒ', 'ÑƒÐ¿Ð¾Ñ€ÑÐ´Ð¾Ñ‡Ð¸', 'sort', 'order by']
    SORT_DESC_KEYWORDS = ['ÑƒÐ±Ñ‹Ð²Ð°Ð½', 'desc', 'z-a', 'Ñ-Ð°', 'Ð±Ð¾Ð»ÑŒÑˆ Ðº Ð¼ÐµÐ½ÑŒÑˆ', 'Ð¾Ñ‚ Ð±Ð¾Ð»ÑŒÑˆÐµÐ³Ð¾', 'Ð¿Ð¾ ÑƒÐ±Ñ‹Ð²Ð°Ð½Ð¸ÑŽ']
    SORT_ASC_KEYWORDS = ['Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚', 'asc', 'a-z', 'Ð°-Ñ', 'Ð¼ÐµÐ½ÑŒÑˆ Ðº Ð±Ð¾Ð»ÑŒÑˆ', 'Ð¾Ñ‚ Ð¼ÐµÐ½ÑŒÑˆÐµÐ³Ð¾', 'Ð¿Ð¾ Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚Ð°Ð½Ð¸ÑŽ']

    # Freeze keywords
    FREEZE_KEYWORDS = ['Ð·Ð°Ð¼Ð¾Ñ€Ð¾Ð·ÑŒ', 'Ð·Ð°Ð¼Ð¾Ñ€Ð¾Ð·Ð¸Ñ‚ÑŒ', 'Ð·Ð°ÐºÑ€ÐµÐ¿Ð¸', 'Ð·Ð°ÐºÑ€ÐµÐ¿Ð¸Ñ‚ÑŒ', 'freeze', 'pin']
    UNFREEZE_KEYWORDS = ['Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð·ÑŒ', 'Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð·Ð¸Ñ‚ÑŒ', 'Ð¾Ñ‚ÐºÑ€ÐµÐ¿Ð¸', 'Ð¾Ñ‚ÐºÑ€ÐµÐ¿Ð¸Ñ‚ÑŒ', 'unfreeze', 'unpin']

    # Format keywords
    FORMAT_BOLD_KEYWORDS = ['Ð¶Ð¸Ñ€Ð½', 'bold', 'Ð²Ñ‹Ð´ÐµÐ»Ð¸ Ð¶Ð¸Ñ€Ð½Ñ‹Ð¼']
    FORMAT_HEADER_KEYWORDS = ['Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²', 'header', 'ÑˆÐ°Ð¿Ðº', 'Ð¿ÐµÑ€Ð²ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ']
    FORMAT_COLOR_KEYWORDS = ['Ñ†Ð²ÐµÑ‚', 'color', 'Ð¿Ð¾ÐºÑ€Ð°ÑÑŒ', 'Ð·Ð°ÐºÑ€Ð°ÑÑŒ']

    # Chart keywords
    CHART_KEYWORDS = ['Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼', 'Ð³Ñ€Ð°Ñ„Ð¸Ðº', 'chart', 'graph', 'Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¹', 'Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·', 'plot']
    CHART_TYPES = {
        # Line charts
        'Ð»Ð¸Ð½ÐµÐ¹Ð½': 'LINE', 'line': 'LINE', 'Ð»Ð¸Ð½Ð¸Ñ': 'LINE', 'Ñ‚Ñ€ÐµÐ½Ð´': 'LINE',
        # Bar charts (horizontal)
        'Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ð°Ð»ÑŒÐ½': 'BAR', 'bar': 'BAR',
        # Column charts (vertical bars) - default
        'ÑÑ‚Ð¾Ð»Ð±Ñ‡': 'COLUMN', 'column': 'COLUMN', 'ÑÑ‚Ð¾Ð»Ð±Ð¸Ðº': 'COLUMN', 'Ð³Ð¸ÑÑ‚Ð¾Ð³Ñ€Ð°Ð¼Ð¼': 'COLUMN',
        # Pie charts
        'ÐºÑ€ÑƒÐ³Ð¾Ð²': 'PIE', 'pie': 'PIE', 'Ð¿Ð¸Ñ€Ð¾Ð³': 'PIE', 'Ð´Ð¾Ð»ÐµÐ¹': 'PIE', 'Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚': 'PIE', 'Ð´Ð¾Ð»ÑÐ¼Ð¸': 'PIE', 'Ð´Ð¾Ð»Ð¸': 'PIE',
        # Area charts
        'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'AREA', 'area': 'AREA', 'Ð·Ð°Ð»Ð¸Ð²Ðº': 'AREA',
        # Scatter plots
        'Ñ‚Ð¾Ñ‡ÐµÑ‡Ð½': 'SCATTER', 'scatter': 'SCATTER', 'Ñ€Ð°Ð·Ð±Ñ€Ð¾Ñ': 'SCATTER', 'ÐºÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†': 'SCATTER',
        # Combo charts
        'ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€': 'COMBO', 'combo': 'COMBO', 'ÑÐ¼ÐµÑˆÐ°Ð½': 'COMBO',
    }

    # Conditional formatting keywords
    CONDITIONAL_FORMAT_KEYWORDS = ['ÑƒÑÐ»Ð¾Ð²Ð½', 'conditional', 'Ð³Ð´Ðµ Ð±Ð¾Ð»ÑŒÑˆÐµ', 'Ð³Ð´Ðµ Ð¼ÐµÐ½ÑŒÑˆÐµ', 'Ð³Ð´Ðµ Ñ€Ð°Ð²Ð½Ð¾',
                                    'Ð±Ð¾Ð»ÑŒÑˆÐµ Ñ‡ÐµÐ¼', 'Ð¼ÐµÐ½ÑŒÑˆÐµ Ñ‡ÐµÐ¼', 'ÐµÑÐ»Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐµ', 'ÐµÑÐ»Ð¸ Ð¼ÐµÐ½ÑŒÑˆÐµ',
                                    'ÐºÑ€Ð°ÑÐ½Ñ‹Ð¼ Ð³Ð´Ðµ', 'Ð·ÐµÐ»Ñ‘Ð½Ñ‹Ð¼ Ð³Ð´Ðµ', 'Ð·ÐµÐ»ÐµÐ½Ñ‹Ð¼ Ð³Ð´Ðµ', 'Ð¶Ñ‘Ð»Ñ‚Ñ‹Ð¼ Ð³Ð´Ðµ', 'Ð¶ÐµÐ»Ñ‚Ñ‹Ð¼ Ð³Ð´Ðµ',
                                    'Ð²Ñ‹Ð´ÐµÐ»Ð¸ Ð³Ð´Ðµ', 'Ð¿Ð¾ÐºÑ€Ð°ÑÑŒ Ð³Ð´Ðµ', 'Ð¾Ñ‚Ð¼ÐµÑ‚ÑŒ Ð³Ð´Ðµ',
                                    # Additional patterns for color-based formatting
                                    'Ð¿Ð¾ÐºÑ€Ð°ÑÑŒ ÐºÑ€Ð°ÑÐ½', 'Ð¿Ð¾ÐºÑ€Ð°ÑÑŒ Ð·ÐµÐ»ÐµÐ½', 'Ð¿Ð¾ÐºÑ€Ð°ÑÑŒ Ð¶Ñ‘Ð»Ñ‚', 'Ð¿Ð¾ÐºÑ€Ð°ÑÑŒ Ð¶ÐµÐ»Ñ‚',
                                    'Ð²Ñ‹Ð´ÐµÐ»Ð¸ ÐºÑ€Ð°ÑÐ½', 'Ð²Ñ‹Ð´ÐµÐ»Ð¸ Ð·ÐµÐ»ÐµÐ½', 'Ð²Ñ‹Ð´ÐµÐ»Ð¸ Ð¶Ñ‘Ð»Ñ‚', 'Ð²Ñ‹Ð´ÐµÐ»Ð¸ Ð¶ÐµÐ»Ñ‚',
                                    'ÐµÑÐ»Ð¸ Ñ†ÐµÐ½Ð°', 'ÐµÑÐ»Ð¸ ÑÑƒÐ¼Ð¼Ð°', 'ÐµÑÐ»Ð¸ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ',
                                    'ÐºÑ€Ð°ÑÐ½Ñ‹Ð¼ ÑÑ‡ÐµÐ¹ÐºÐ¸', 'Ð·ÐµÐ»Ñ‘Ð½Ñ‹Ð¼ ÑÑ‡ÐµÐ¹ÐºÐ¸', 'Ð·ÐµÐ»ÐµÐ½Ñ‹Ð¼ ÑÑ‡ÐµÐ¹ÐºÐ¸',
                                    'Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ', 'Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‡ÐµÐ¹ÐºÐ¸', 'Ð¶ÐµÐ»Ñ‚Ñ‹Ð¼ Ð¿ÑƒÑÑ‚', 'Ð¶Ñ‘Ð»Ñ‚Ñ‹Ð¼ Ð¿ÑƒÑÑ‚']

    # Pivot table / grouping keywords
    PIVOT_KEYWORDS = ['ÑÐ²Ð¾Ð´Ð½', 'pivot', 'Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€', 'group by', 'Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€', 'Ð¸Ñ‚Ð¾Ð³Ð¸ Ð¿Ð¾', 'ÑÑƒÐ¼Ð¼Ñ‹ Ð¿Ð¾',
                      'Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸', 'Ð¿Ð¾ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€', 'Ð¿Ð¾ Ñ€ÐµÐ³Ð¸Ð¾Ð½', 'Ð¿Ð¾ Ð¼ÐµÑÑÑ†', 'Ð¿Ð¾ Ð³Ð¾Ð´',
                      'Ñ€Ð°Ð·Ð±Ð¸Ð²ÐºÐ° Ð¿Ð¾', 'Ð² Ñ€Ð°Ð·Ñ€ÐµÐ·Ðµ']

    # Aggregation functions
    AGG_FUNCTIONS = {
        'ÑÑƒÐ¼Ð¼': 'sum', 'sum': 'sum', 'Ð¸Ñ‚Ð¾Ð³': 'sum',
        'ÑÑ€ÐµÐ´Ð½': 'mean', 'avg': 'mean', 'average': 'mean',
        'ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²': 'count', 'count': 'count', 'Ñ‡Ð¸ÑÐ»Ð¾': 'count',
        'Ð¼Ð°ÐºÑ': 'max', 'max': 'max', 'Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼': 'max',
        'Ð¼Ð¸Ð½': 'min', 'min': 'min', 'Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼': 'min'
    }

    # Synonyms for value columns (business terms -> likely column names)
    VALUE_COLUMN_SYNONYMS = {
        'Ð¿Ñ€Ð¾Ð´Ð°Ð¶': ['ÑÑƒÐ¼Ð¼Ð°', 'ÑÑƒÐ¼Ð¼', 'Ð²Ñ‹Ñ€ÑƒÑ‡ÐºÐ°', 'revenue', 'sales', 'amount', 'total'],
        'Ð´Ð¾Ñ…Ð¾Ð´': ['ÑÑƒÐ¼Ð¼Ð°', 'Ð²Ñ‹Ñ€ÑƒÑ‡ÐºÐ°', 'Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒ', 'revenue', 'income'],
        'Ð²Ñ‹Ñ€ÑƒÑ‡Ðº': ['ÑÑƒÐ¼Ð¼Ð°', 'Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð¸', 'revenue', 'sales'],
        'Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚': ['ÑÑƒÐ¼Ð¼Ð°', 'Ð²Ñ‹Ñ€ÑƒÑ‡ÐºÐ°', 'revenue'],
        'Ð¿Ñ€Ð¸Ð±Ñ‹Ð»': ['Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒ', 'Ð¼Ð°Ñ€Ð¶Ð°', 'profit', 'margin'],
        'Ð·Ð°Ñ‚Ñ€Ð°Ñ‚': ['ÑÐµÐ±ÐµÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ', 'Ñ€Ð°ÑÑ…Ð¾Ð´Ñ‹', 'cost', 'expenses'],
        'Ñ€Ð°ÑÑ…Ð¾Ð´': ['ÑÐµÐ±ÐµÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ', 'Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ñ‹', 'cost', 'expenses'],
    }


    # Data cleaning keywords
    CLEAN_KEYWORDS = ['Ð¾Ñ‡Ð¸ÑÑ‚', 'clean', 'ÑƒÐ´Ð°Ð»Ð¸ Ð´ÑƒÐ±Ð»Ð¸Ðº', 'remove duplicate', 'Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚',
                      'ÑƒÐ´Ð°Ð»Ð¸ Ð¿ÑƒÑÑ‚', 'remove empty', 'Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‚Ñ€Ð¾Ðº', 'empty row',
                      'Ð·Ð°Ð¿Ð¾Ð»Ð½Ð¸ Ð¿ÑƒÑÑ‚', 'fill empty', 'fill blank', 'fillna',
                      'ÑƒÐ±ÐµÑ€Ð¸ Ð¿Ñ€Ð¾Ð±ÐµÐ»', 'trim', 'strip', 'Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹',
                      'Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·', 'normalize', 'ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð¸Ð·',
                      # Additional patterns
                      'ÑƒÐ±ÐµÑ€Ð¸ Ð´ÑƒÐ±Ð»Ð¸Ðº', 'ÑƒÐ±ÐµÑ€Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€', 'ÑƒÐ±ÐµÑ€Ð¸ Ð¿ÑƒÑÑ‚', 'ÑƒÐ±ÐµÑ€Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ¸',
                      'ÑƒÐ´Ð°Ð»Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€', 'ÑƒÐ´Ð°Ð»Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ¸']

    # CSV Split / Text-to-columns keywords
    CSV_SPLIT_KEYWORDS = ['Ñ€Ð°Ð·Ð±ÐµÐ¹', 'Ñ€Ð°Ð·Ð±Ð¸Ñ‚ÑŒ', 'split', 'Ñ€Ð°Ð·Ð´ÐµÐ»Ð¸Ñ‚ÑŒ', 'Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐ¹', 
                          'Ð¿Ð¾ ÑÑ‡ÐµÐ¹ÐºÐ°Ð¼', 'text to columns', 'Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾ ÑÑ‚Ð¾Ð»Ð±Ñ†Ð°Ð¼',
                          'csv', 'Ð¿Ð¾ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°Ð¼', 'Ð¿Ð¾ ÑÑ‚Ð¾Ð»Ð±Ñ†Ð°Ð¼', 'Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸', 'Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³',
                          'Ñ€Ð°Ð·Ð´ÐµÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ', 'Ñ€Ð°Ð·Ð±ÐµÐ¹ Ð´Ð°Ð½Ð½Ñ‹Ðµ', 'Ñ€Ð°Ð·Ð±ÐµÐ¹ csv', 'Ñ€Ð°Ð·Ð±ÐµÐ¹ Ñ‚ÐµÐºÑÑ‚']

    # Cleaning operation types
    CLEAN_OPERATIONS = {
        'duplicate': ['Ð´ÑƒÐ±Ð»Ð¸Ðº', 'duplicate', 'Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€', 'Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²', 'Ð´ÑƒÐ±Ð»'],
        'empty_rows': ['Ð¿ÑƒÑÑ‚', 'empty', 'blank', 'nan', 'null'],
        'trim': ['Ð¿Ñ€Ð¾Ð±ÐµÐ»', 'trim', 'strip', 'whitespace'],
        'fill': ['Ð·Ð°Ð¿Ð¾Ð»Ð½', 'fill', 'Ð·Ð°Ð¼ÐµÐ½', 'replace'],
    }

    # Data validation keywords
    VALIDATION_KEYWORDS = ['Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†', 'validation', 'Ð²Ñ‹Ð¿Ð°Ð´Ð°ÑŽÑ‰', 'dropdown', 'ÑÐ¿Ð¸ÑÐ¾Ðº',
                           'Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÑŒ', 'restrict', 'Ð´Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼',
                           'Ñ€Ð°Ð·Ñ€ÐµÑˆÑ‘Ð½Ð½', 'allowed', 'Ð²Ñ‹Ð±Ð¾Ñ€ Ð¸Ð·', 'select from']

    # Highlight keywords - MUST be checked BEFORE filter keywords
    HIGHLIGHT_KEYWORDS = ['Ð²Ñ‹Ð´ÐµÐ»Ð¸', 'Ð²Ñ‹Ð´ÐµÐ»Ð¸Ñ‚ÑŒ', 'Ð¿Ð¾Ð´ÑÐ²ÐµÑ‚Ð¸', 'Ð¿Ð¾Ð´ÑÐ²ÐµÑ‚Ð¸Ñ‚ÑŒ', 'Ð¿Ð¾Ð´ÑÐ²ÐµÑ‚ÑŒ',
                          'highlight', 'mark', 'Ð¿Ð¾ÐºÑ€Ð°ÑÑŒ', 'Ð¿Ð¾ÐºÑ€Ð°ÑÐ¸Ñ‚ÑŒ', 'Ñ€Ð°ÑÐºÑ€Ð°ÑÑŒ',
                          'Ð¾Ñ‚Ð¼ÐµÑ‚ÑŒ', 'Ð¾Ñ‚Ð¼ÐµÑ‚Ð¸Ñ‚ÑŒ', 'Ð¿Ð¾Ð¼ÐµÑ‚ÑŒ', 'Ð¿Ð¾Ð¼ÐµÑ‚Ð¸Ñ‚ÑŒ']

    # Highlight colors mapping (hex)
    HIGHLIGHT_COLORS = {
        'ÐºÑ€Ð°ÑÐ½': '#FF6B6B',
        'red': '#FF6B6B',
        'Ð·ÐµÐ»ÐµÐ½': '#69DB7C',
        'Ð·ÐµÐ»Ñ‘Ð½': '#69DB7C',
        'green': '#69DB7C',
        'Ð¶Ñ‘Ð»Ñ‚': '#FFE066',
        'Ð¶ÐµÐ»Ñ‚': '#FFE066',
        'yellow': '#FFE066',
        'Ð¾Ñ€Ð°Ð½Ð¶': '#FFA94D',
        'orange': '#FFA94D',
        'ÑÐ¸Ð½Ð¸Ð¹': '#74C0FC',
        'blue': '#74C0FC',
        'Ð³Ð¾Ð»ÑƒÐ±': '#99E9F2',
    }

    # Filter keywords
    FILTER_KEYWORDS = ['Ñ„Ð¸Ð»ÑŒÑ‚Ñ€', 'filter', 'Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€', 'Ð¿Ð¾ÐºÐ°Ð¶Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾', 'show only',
                       'Ð³Ð´Ðµ ', 'where ', 'Ð²Ñ‹Ð±ÐµÑ€Ð¸ Ð³Ð´Ðµ', 'select where', 'ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð³Ð´Ðµ',
                       'rows where', 'Ð¾Ñ‚Ð±ÐµÑ€Ð¸', 'Ð²Ñ‹Ð±ÐµÑ€Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ¸']

    # Filter operators
    FILTER_OPERATORS = {
        '>=': ['>=', 'â‰¥', 'Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¸Ð»Ð¸ Ñ€Ð°Ð²Ð½Ð¾', 'Ð½Ðµ Ð¼ÐµÐ½ÑŒÑˆÐµ'],
        '<=': ['<=', 'â‰¤', 'Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¸Ð»Ð¸ Ñ€Ð°Ð²Ð½Ð¾', 'Ð½Ðµ Ð±Ð¾Ð»ÑŒÑˆÐµ'],
        '!=': ['!=', 'â‰ ', '<>', 'Ð½Ðµ Ñ€Ð°Ð²Ð½Ð¾', 'Ð½Ðµ Ñ€Ð°Ð²ÐµÐ½', 'ÐºÑ€Ð¾Ð¼Ðµ'],
        '>': ['>', 'Ð±Ð¾Ð»ÑŒÑˆÐµ', 'Ð²Ñ‹ÑˆÐµ', 'more than', 'greater', 'Ð±Ð¾Ð»ÐµÐµ'],
        '<': ['<', 'Ð¼ÐµÐ½ÑŒÑˆÐµ', 'Ð½Ð¸Ð¶Ðµ', 'less than', 'lower', 'Ð¼ÐµÐ½ÐµÐµ'],
        '==': ['=', '==', 'Ñ€Ð°Ð²Ð½Ð¾', 'Ñ€Ð°Ð²ÐµÐ½', 'equals', 'is'],
        'contains': ['ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚', 'contains', 'Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚', 'includes'],
        'startswith': ['Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ', 'starts with', 'Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚'],
        'endswith': ['Ð·Ð°ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°ÐµÑ‚ÑÑ', 'ends with', 'Ð¾ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°ÐµÑ‚ÑÑ'],
        'empty': ['Ð¿ÑƒÑÑ‚', 'empty', 'null', 'nan', 'Ð½ÐµÑ‚ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ'],
        'not_empty': ['Ð½Ðµ Ð¿ÑƒÑÑ‚', 'not empty', 'Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½', 'ÐµÑÑ‚ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ'],
    }

    # Color keywords for conditional formatting
    CONDITION_COLORS = {
        'ÐºÑ€Ð°ÑÐ½': {'red': 1, 'green': 0.8, 'blue': 0.8},      # Light red
        'red': {'red': 1, 'green': 0.8, 'blue': 0.8},
        'Ð·ÐµÐ»ÐµÐ½': {'red': 0.85, 'green': 0.95, 'blue': 0.85}, # Light green
        'green': {'red': 0.85, 'green': 0.95, 'blue': 0.85},
        'Ð¶Ñ‘Ð»Ñ‚': {'red': 1, 'green': 1, 'blue': 0.7},         # Light yellow
        'Ð¶ÐµÐ»Ñ‚': {'red': 1, 'green': 1, 'blue': 0.7},
        'yellow': {'red': 1, 'green': 1, 'blue': 0.7},
        'Ð¾Ñ€Ð°Ð½Ð¶': {'red': 1, 'green': 0.9, 'blue': 0.8},      # Light orange
        'orange': {'red': 1, 'green': 0.9, 'blue': 0.8},
        'ÑÐ¸Ð½Ð¸Ð¹': {'red': 0.85, 'green': 0.9, 'blue': 1},     # Light blue
        'blue': {'red': 0.85, 'green': 0.9, 'blue': 1},
        'Ð³Ð¾Ð»ÑƒÐ±': {'red': 0.85, 'green': 0.95, 'blue': 1},    # Light cyan
    }

    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            # Try loading from .env
            from pathlib import Path
            env_path = Path(__file__).parent.parent.parent / ".env"
            if env_path.exists():
                with open(env_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith("OPENAI_API_KEY="):
                            api_key = line.split("=", 1)[1].strip()
                            os.environ["OPENAI_API_KEY"] = api_key
                            break

        self.client = AsyncOpenAI(api_key=api_key)
        self.schema_extractor = get_schema_extractor()

    def _detect_sort_action(self, query: str, column_names: List[str]) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¸.
        Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¸Ð»Ð¸ None.
        """
        query_lower = query.lower()

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð² ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¸
        is_sort_query = any(kw in query_lower for kw in self.SORT_KEYWORDS)
        if not is_sort_query:
            return None

        logger.info(f"[SimpleGPT] Sort action detected: {query}")

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¸
        is_descending = any(kw in query_lower for kw in self.SORT_DESC_KEYWORDS)
        is_ascending = any(kw in query_lower for kw in self.SORT_ASC_KEYWORDS)

        # ÐŸÐ¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ - Ð¿Ð¾ Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚Ð°Ð½Ð¸ÑŽ, ÐµÑÐ»Ð¸ ÑÐ²Ð½Ð¾ Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½Ð¾ ÑƒÐ±Ñ‹Ð²Ð°Ð½Ð¸Ðµ
        sort_order = "DESCENDING" if is_descending and not is_ascending else "ASCENDING"

        # Ð˜Ñ‰ÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð² Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ
        sort_column = None
        sort_column_index = None

        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°
        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‚Ð¾Ñ‡Ð½Ð¾Ðµ Ð²Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ Ð¸Ð»Ð¸ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ðµ
            if col_lower in query_lower or col_name in query:
                sort_column = col_name
                sort_column_index = idx
                logger.info(f"[SimpleGPT] Found sort column: '{col_name}' at index {idx}")
                break

        # Ð•ÑÐ»Ð¸ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¿Ð¾ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ð¼Ñƒ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸ÑŽ
        if not sort_column:
            for idx, col_name in enumerate(column_names):
                # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð½Ð° ÑÐ»Ð¾Ð²Ð°
                col_words = col_name.lower().split()
                for word in col_words:
                    if len(word) > 2 and word in query_lower:
                        sort_column = col_name
                        sort_column_index = idx
                        logger.info(f"[SimpleGPT] Found sort column by partial match: '{col_name}' at index {idx}")
                        break
                if sort_column:
                    break

        if not sort_column:
            logger.warning(f"[SimpleGPT] Sort column not found in query. Available columns: {column_names}")
            return None

        return {
            "action_type": "sort",
            "column_name": sort_column,
            "column_index": sort_column_index,
            "sort_order": sort_order,
            "message": f"Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ ÐºÐ¾Ð»Ð¾Ð½ÐºÐµ '{sort_column}' ({('Ð¿Ð¾ ÑƒÐ±Ñ‹Ð²Ð°Ð½Ð¸ÑŽ' if sort_order == 'DESCENDING' else 'Ð¿Ð¾ Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚Ð°Ð½Ð¸ÑŽ')})"
        }

    def _detect_freeze_action(self, query: str) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ Ð·Ð°Ð¼Ð¾Ñ€Ð¾Ð·ÐºÐ¸ ÑÑ‚Ñ€Ð¾Ðº/ÑÑ‚Ð¾Ð»Ð±Ñ†Ð¾Ð².
        """
        query_lower = query.lower()

        # Check for unfreeze first
        is_unfreeze = any(kw in query_lower for kw in self.UNFREEZE_KEYWORDS)
        if is_unfreeze:
            logger.info(f"[SimpleGPT] Unfreeze action detected: {query}")
            return {
                "action_type": "freeze",
                "freeze_rows": 0,
                "freeze_columns": 0,
                "message": "Ð—Ð°ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸Ðµ ÑÐ½ÑÑ‚Ð¾"
            }

        # Check for freeze
        is_freeze = any(kw in query_lower for kw in self.FREEZE_KEYWORDS)
        if not is_freeze:
            return None

        logger.info(f"[SimpleGPT] Freeze action detected: {query}")

        # Determine what to freeze
        freeze_rows = 0
        freeze_columns = 0

        # Check for row freeze
        if any(word in query_lower for word in ['ÑÑ‚Ñ€Ð¾Ðº', 'ÑÑ‚Ñ€Ð¾ÐºÑƒ', 'row', 'Ð¿ÐµÑ€Ð²ÑƒÑŽ', 'ÑˆÐ°Ð¿ÐºÑƒ', 'Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²']):
            # Try to find number
            import re
            numbers = re.findall(r'(\d+)\s*(?:ÑÑ‚Ñ€Ð¾Ðº|ÑÑ‚Ñ€Ð¾ÐºÑƒ|row)', query_lower)
            if numbers:
                freeze_rows = int(numbers[0])
            else:
                freeze_rows = 1  # Default to 1 row (header)

        # Check for column freeze
        if any(word in query_lower for word in ['ÑÑ‚Ð¾Ð»Ð±', 'ÐºÐ¾Ð»Ð¾Ð½Ðº', 'column', 'Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑÑ‚Ð¾Ð»Ð±', 'Ð¿ÐµÑ€Ð²ÑƒÑŽ ÐºÐ¾Ð»Ð¾Ð½Ðº']):
            import re
            numbers = re.findall(r'(\d+)\s*(?:ÑÑ‚Ð¾Ð»Ð±|ÐºÐ¾Ð»Ð¾Ð½Ðº|column)', query_lower)
            if numbers:
                freeze_columns = int(numbers[0])
            else:
                freeze_columns = 1  # Default to 1 column

        # If nothing specific mentioned, freeze first row
        if freeze_rows == 0 and freeze_columns == 0:
            freeze_rows = 1

        message_parts = []
        if freeze_rows > 0:
            message_parts.append(f"{freeze_rows} ÑÑ‚Ñ€Ð¾Ðº" if freeze_rows > 1 else "Ð¿ÐµÑ€Ð²Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ°")
        if freeze_columns > 0:
            message_parts.append(f"{freeze_columns} ÑÑ‚Ð¾Ð»Ð±Ñ†Ð¾Ð²" if freeze_columns > 1 else "Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑÑ‚Ð¾Ð»Ð±ÐµÑ†")

        return {
            "action_type": "freeze",
            "freeze_rows": freeze_rows,
            "freeze_columns": freeze_columns,
            "message": f"Ð—Ð°ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¾: {', '.join(message_parts)}"
        }

    def _detect_format_action(self, query: str) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.
        """
        query_lower = query.lower()

        # Check for bold formatting
        is_bold = any(kw in query_lower for kw in self.FORMAT_BOLD_KEYWORDS)
        is_header = any(kw in query_lower for kw in self.FORMAT_HEADER_KEYWORDS)

        if not (is_bold or is_header):
            return None

        logger.info(f"[SimpleGPT] Format action detected: {query}")

        # Determine format type
        format_type = "bold_header" if (is_bold and is_header) or is_header else "bold"

        # Check for color
        color = None
        if any(kw in query_lower for kw in self.FORMAT_COLOR_KEYWORDS):
            # Try to detect color
            color_map = {
                'ÐºÑ€Ð°ÑÐ½': '#FF0000', 'red': '#FF0000',
                'ÑÐ¸Ð½Ð¸Ð¹': '#0000FF', 'blue': '#0000FF',
                'Ð·ÐµÐ»ÐµÐ½': '#00FF00', 'green': '#00FF00',
                'Ð¶ÐµÐ»Ñ‚': '#FFFF00', 'yellow': '#FFFF00',
                'Ð¾Ñ€Ð°Ð½Ð¶': '#FFA500', 'orange': '#FFA500',
                'ÑÐµÑ€Ñ‹Ð¹': '#808080', 'ÑÐµÑ€': '#808080', 'gray': '#808080', 'grey': '#808080',
            }
            for color_word, color_code in color_map.items():
                if color_word in query_lower:
                    color = color_code
                    break

        return {
            "action_type": "format",
            "format_type": format_type,
            "target_row": 1,  # First row (header)
            "bold": is_bold or is_header,
            "background_color": color,
            "message": f"Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ð¾Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹" + (f" (Ñ†Ð²ÐµÑ‚: {color})" if color else "")
        }

    def _detect_chart_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñ‹.
        ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð´Ð»Ñ Ð¾ÑÐµÐ¹.
        """
        query_lower = query.lower()

        # Check for chart keywords
        is_chart_query = any(kw in query_lower for kw in self.CHART_KEYWORDS)
        if not is_chart_query:
            return None

        logger.info(f"[SimpleGPT] Chart action detected: {query}")

        # Determine chart type
        chart_type = 'COLUMN'  # Default
        for type_keyword, type_value in self.CHART_TYPES.items():
            if type_keyword in query_lower:
                chart_type = type_value
                logger.info(f"[SimpleGPT] Chart type detected: {type_value}")
                break

        # Analyze columns to find best X and Y axes
        numeric_cols = []
        categorical_cols = []
        date_cols = []

        for idx, col in enumerate(column_names):
            if idx >= len(df.columns):
                continue
            col_data = df.iloc[:, idx]

            # Check if column is numeric
            try:
                numeric_data = pd.to_numeric(col_data, errors='coerce')
                non_null_ratio = numeric_data.notna().sum() / len(numeric_data) if len(numeric_data) > 0 else 0
                if non_null_ratio > 0.5:
                    numeric_cols.append({'name': col, 'index': idx})
                    continue
            except:
                pass

            # Check if column is date-like
            col_lower = col.lower()
            if any(d in col_lower for d in ['Ð´Ð°Ñ‚Ð°', 'date', 'Ð¼ÐµÑÑÑ†', 'month', 'Ð³Ð¾Ð´', 'year', 'Ð´ÐµÐ½ÑŒ', 'day', 'Ð¿ÐµÑ€Ð¸Ð¾Ð´', 'time']):
                date_cols.append({'name': col, 'index': idx})
                continue

            # Otherwise it's categorical
            categorical_cols.append({'name': col, 'index': idx})

        logger.info(f"[SimpleGPT] Column analysis: numeric={[c['name'] for c in numeric_cols]}, "
                   f"categorical={[c['name'] for c in categorical_cols]}, date={[c['name'] for c in date_cols]}")

        # Find columns mentioned in query
        mentioned_cols = []
        for idx, col in enumerate(column_names):
            col_lower = col.lower()
            # Check if column name or any significant word from it is in query
            if col_lower in query_lower or col in query:
                mentioned_cols.append({'name': col, 'index': idx})
            else:
                # Check for partial match
                for word in col_lower.split():
                    if len(word) > 2 and word in query_lower:
                        mentioned_cols.append({'name': col, 'index': idx})
                        break

        logger.info(f"[SimpleGPT] Columns mentioned in query: {[c['name'] for c in mentioned_cols]}")

        # Determine X and Y axes
        x_column = None
        y_columns = []

        # Priority for X axis: mentioned categorical > date > first categorical
        # If user explicitly mentions a categorical column, use it
        for cat in categorical_cols:
            if cat in mentioned_cols:
                x_column = cat
                logger.info(f"[SimpleGPT] Using mentioned categorical column for X axis: {cat['name']}")
                break

        # If no mentioned categorical, use date column for time series
        if not x_column and date_cols:
            x_column = date_cols[0]
            logger.info(f"[SimpleGPT] Using date column for X axis: {x_column['name']}")

        # Fallback to first categorical
        if not x_column and categorical_cols:
            x_column = categorical_cols[0]
            logger.info(f"[SimpleGPT] Using first categorical column for X axis: {x_column['name']}")

        # Y axis: mentioned numeric columns, or all numeric if none mentioned
        for num in numeric_cols:
            if num in mentioned_cols:
                y_columns.append(num)

        if not y_columns and numeric_cols:
            # Take first 1-3 numeric columns
            y_columns = numeric_cols[:3]

        # For PIE charts, we need exactly one Y column and one X column
        if chart_type == 'PIE' and y_columns:
            y_columns = [y_columns[0]]

        # Generate title from query or columns
        title = ""
        if y_columns and x_column:
            y_names = ", ".join([c['name'] for c in y_columns])
            title = f"{y_names} Ð¿Ð¾ {x_column['name']}"
        elif y_columns:
            title = ", ".join([c['name'] for c in y_columns])

        # Build chart spec
        chart_spec = {
            "chart_type": chart_type,
            "title": title,
            "x_column_index": x_column['index'] if x_column else 0,
            "x_column_name": x_column['name'] if x_column else column_names[0],
            "y_column_indices": [c['index'] for c in y_columns] if y_columns else [1] if len(column_names) > 1 else [0],
            "y_column_names": [c['name'] for c in y_columns] if y_columns else [column_names[1] if len(column_names) > 1 else column_names[0]],
            "row_count": len(df),
            "col_count": len(column_names)
        }

        chart_type_names = {
            'LINE': 'Ð»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ð¹ Ð³Ñ€Ð°Ñ„Ð¸Ðº',
            'BAR': 'Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð³Ð¸ÑÑ‚Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ',
            'COLUMN': 'ÑÑ‚Ð¾Ð»Ð±Ñ‡Ð°Ñ‚ÑƒÑŽ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñƒ',
            'PIE': 'ÐºÑ€ÑƒÐ³Ð¾Ð²ÑƒÑŽ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñƒ',
            'AREA': 'Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑÐ¼Ð¸',
            'SCATTER': 'Ñ‚Ð¾Ñ‡ÐµÑ‡Ð½ÑƒÑŽ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñƒ',
            'COMBO': 'ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð³Ñ€Ð°Ñ„Ð¸Ðº'
        }

        message = f"Ð¡Ð¾Ð·Ð´Ð°ÑŽ {chart_type_names.get(chart_type, 'Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñƒ')}: {title}"

        return {
            "action_type": "chart",
            "chart_spec": chart_spec,
            "message": message
        }

    def _detect_conditional_format_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ ÑƒÑÐ»Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        - "Ð²Ñ‹Ð´ÐµÐ»Ð¸ ÐºÑ€Ð°ÑÐ½Ñ‹Ð¼ Ð³Ð´Ðµ ÑÑƒÐ¼Ð¼Ð° Ð±Ð¾Ð»ÑŒÑˆÐµ 10000"
        - "Ð·ÐµÐ»Ñ‘Ð½Ñ‹Ð¼ Ð³Ð´Ðµ Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ"
        - "ÑƒÑÐ»Ð¾Ð²Ð½Ð¾Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ: Ð¶Ñ‘Ð»Ñ‚Ñ‹Ð¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‡ÐµÐ¹ÐºÐ¸"
        """
        query_lower = query.lower()

        # Check for conditional format keywords
        is_conditional = any(kw in query_lower for kw in self.CONDITIONAL_FORMAT_KEYWORDS)
        if not is_conditional:
            return None

        logger.info(f"[SimpleGPT] Conditional format action detected: {query}")

        # Detect color
        format_color = {'red': 1, 'green': 1, 'blue': 0.7}  # Default yellow
        color_name = "Ð¶Ñ‘Ð»Ñ‚Ñ‹Ð¹"
        for color_kw, color_value in self.CONDITION_COLORS.items():
            if color_kw in query_lower:
                format_color = color_value
                color_name = color_kw
                break

        # Find column mentioned in query
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        # If no column found, try to find numeric column
        if not target_column:
            for idx, col_name in enumerate(column_names):
                if idx < len(df.columns):
                    try:
                        numeric_data = pd.to_numeric(df.iloc[:, idx], errors='coerce')
                        if numeric_data.notna().sum() / len(numeric_data) > 0.5:
                            target_column = col_name
                            target_column_index = idx
                            break
                    except:
                        pass

        # Detect condition type and value
        condition_type = "GREATER_THAN"  # Default
        condition_value = None

        # Patterns for conditions
        import re

        # "Ð±Ð¾Ð»ÑŒÑˆÐµ X" / "> X"
        greater_match = re.search(r'(?:Ð±Ð¾Ð»ÑŒÑˆÐµ|>|Ð±Ð¾Ð»ÐµÐµ)\s*(?:Ñ‡ÐµÐ¼\s*)?(\d+(?:[.,]\d+)?)', query_lower)
        if greater_match:
            condition_type = "NUMBER_GREATER"
            condition_value = float(greater_match.group(1).replace(',', '.'))

        # "Ð¼ÐµÐ½ÑŒÑˆÐµ X" / "< X"
        less_match = re.search(r'(?:Ð¼ÐµÐ½ÑŒÑˆÐµ|<|Ð¼ÐµÐ½ÐµÐµ)\s*(?:Ñ‡ÐµÐ¼\s*)?(\d+(?:[.,]\d+)?)', query_lower)
        if less_match:
            condition_type = "NUMBER_LESS"
            condition_value = float(less_match.group(1).replace(',', '.'))

        # "Ñ€Ð°Ð²Ð½Ð¾ X" / "= X"
        equal_match = re.search(r'(?:Ñ€Ð°Ð²Ð½Ð¾|=|Ñ€Ð°Ð²ÐµÐ½)\s*(\d+(?:[.,]\d+)?)', query_lower)
        if equal_match:
            condition_type = "NUMBER_EQ"
            condition_value = float(equal_match.group(1).replace(',', '.'))

        # "Ð¿ÑƒÑÑ‚Ð¾" / "Ð¿ÑƒÑÑ‚Ñ‹Ðµ"
        if any(w in query_lower for w in ['Ð¿ÑƒÑÑ‚', 'empty', 'blank', 'Ð½ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…']):
            condition_type = "BLANK"
            condition_value = None

        # "Ð½Ðµ Ð¿ÑƒÑÑ‚Ð¾" / "Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾"
        if any(w in query_lower for w in ['Ð½Ðµ Ð¿ÑƒÑÑ‚', 'not empty', 'Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½', 'ÐµÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ']):
            condition_type = "NOT_BLANK"
            condition_value = None

        # "Ð¾Ñ‚Ñ€Ð¸Ñ†Ð°Ñ‚ÐµÐ»ÑŒÐ½" / "ÑƒÐ±Ñ‹Ñ‚Ð¾Ðº"
        if any(w in query_lower for w in ['Ð¾Ñ‚Ñ€Ð¸Ñ†Ð°Ñ‚ÐµÐ»ÑŒÐ½', 'ÑƒÐ±Ñ‹Ñ‚', 'negative', 'Ð¼Ð¸Ð½ÑƒÑ']):
            condition_type = "NUMBER_LESS"
            condition_value = 0

        # "Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½" / "Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒ"
        if any(w in query_lower for w in ['Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½', 'Ð¿Ñ€Ð¸Ð±Ñ‹Ð»', 'positive', 'Ð¿Ð»ÑŽÑ']):
            condition_type = "NUMBER_GREATER"
            condition_value = 0

        # Build the conditional format rule
        rule = {
            "column_index": target_column_index if target_column_index is not None else 0,
            "column_name": target_column or column_names[0] if column_names else "A",
            "condition_type": condition_type,
            "condition_value": condition_value,
            "format_color": format_color
        }

        # Generate message
        condition_text = ""
        if condition_type == "NUMBER_GREATER":
            condition_text = f"> {condition_value}"
        elif condition_type == "NUMBER_LESS":
            condition_text = f"< {condition_value}"
        elif condition_type == "NUMBER_EQ":
            condition_text = f"= {condition_value}"
        elif condition_type == "BLANK":
            condition_text = "Ð¿ÑƒÑÑ‚Ñ‹Ðµ"
        elif condition_type == "NOT_BLANK":
            condition_text = "Ð½ÐµÐ¿ÑƒÑÑ‚Ñ‹Ðµ"

        message = f"Ð£ÑÐ»Ð¾Ð²Ð½Ð¾Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ: {target_column or 'ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°'} {condition_text} â†’ {color_name}"

        return {
            "action_type": "conditional_format",
            "rule": rule,
            "message": message
        }

    def _detect_pivot_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐ²Ð¾Ð´Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹.
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        - "ÑÐ²Ð¾Ð´Ð½Ð°Ñ Ð¿Ð¾ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼"
        - "Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¾Ð´Ð°Ð¶ Ð¿Ð¾ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð°Ð¼"
        - "ÑÑƒÐ¼Ð¼Ñ‹ Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼"
        """
        query_lower = query.lower()

        # Check for pivot keywords
        is_pivot = any(kw in query_lower for kw in self.PIVOT_KEYWORDS)
        if not is_pivot:
            return None

        logger.info(f"[SimpleGPT] Pivot action detected: {query}")

        # Analyze columns
        numeric_cols = []
        categorical_cols = []

        for idx, col_name in enumerate(column_names):
            if idx >= len(df.columns):
                continue
            col_data = df.iloc[:, idx]

            # Check if numeric
            try:
                numeric_data = pd.to_numeric(col_data, errors='coerce')
                non_null_ratio = numeric_data.notna().sum() / len(numeric_data) if len(numeric_data) > 0 else 0
                if non_null_ratio > 0.5:
                    numeric_cols.append({'name': col_name, 'index': idx})
                    continue
            except:
                pass

            # Otherwise categorical
            unique_count = col_data.nunique()
            if unique_count <= len(col_data) * 0.7:  # Up to 70% unique = categorical
                categorical_cols.append({'name': col_name, 'index': idx})

        logger.info(f"[SimpleGPT] Columns: numeric={[c['name'] for c in numeric_cols]}, categorical={[c['name'] for c in categorical_cols]}")

        # Find grouping column (categorical mentioned in query)
        group_column = None
        for cat in categorical_cols:
            cat_lower = cat['name'].lower()
            if cat_lower in query_lower or cat['name'] in query:
                group_column = cat
                break
            # Partial match - check if column name stem is in query
            for word in cat_lower.split():
                if len(word) > 2 and word in query_lower:
                    group_column = cat
                    break
                # Check if word stem (first 4+ chars) is in query for Russian word forms
                if len(word) >= 4:
                    word_stem = word[:max(4, len(word) - 2)]  # Get stem (at least 4 chars)
                    if word_stem in query_lower:
                        group_column = cat
                        logger.info(f"[SimpleGPT] Found pivot column by stem: '{word_stem}' in '{word}' -> {cat['name']}")
                        break
            if group_column:
                break

        # If not found, use first categorical
        if not group_column and categorical_cols:
            group_column = categorical_cols[0]

        # Find value column (numeric mentioned in query or via synonyms)
        value_column = None

        # First, try direct match
        for num in numeric_cols:
            num_lower = num['name'].lower()
            if num_lower in query_lower or num['name'] in query:
                value_column = num
                logger.info(f"[SimpleGPT] Found value column by direct match: {num['name']}")
                break
            for word in num_lower.split():
                if len(word) > 2 and word in query_lower:
                    value_column = num
                    logger.info(f"[SimpleGPT] Found value column by word match: {num['name']}")
                    break
            if value_column:
                break

        # If not found, try synonyms (e.g., "Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð¸" -> "Ð¡ÑƒÐ¼Ð¼Ð°")
        if not value_column:
            for query_term, synonyms in self.VALUE_COLUMN_SYNONYMS.items():
                if query_term in query_lower:
                    # Found a business term in query, look for matching column
                    for num in numeric_cols:
                        num_lower = num['name'].lower()
                        for syn in synonyms:
                            if syn in num_lower or num_lower in syn:
                                value_column = num
                                logger.info(f"[SimpleGPT] Found value column via synonym '{query_term}' -> '{num['name']}'")
                                break
                        if value_column:
                            break
                    if value_column:
                        break

        # Fallback to first numeric column (but skip ID columns)
        if not value_column and numeric_cols:
            for num in numeric_cols:
                # Skip columns that look like IDs
                if num['name'].lower() not in ['id', 'Ð¸Ð´', 'Ð½Ð¾Ð¼ÐµÑ€', 'ÐºÐ¾Ð´', 'index']:
                    value_column = num
                    logger.info(f"[SimpleGPT] Using first non-ID numeric column: {num['name']}")
                    break
            # If all are ID-like, use first one anyway
            if not value_column:
                value_column = numeric_cols[0]

        # Detect aggregation function
        agg_func = 'sum'  # Default
        for kw, func in self.AGG_FUNCTIONS.items():
            if kw in query_lower:
                agg_func = func
                break

        if not group_column or not value_column:
            logger.warning(f"[SimpleGPT] Cannot create pivot: group_column={group_column}, value_column={value_column}")
            return None

        # Create pivot table using pandas
        try:
            pivot_df = df.groupby(df.iloc[:, group_column['index']]).agg({
                df.columns[value_column['index']]: agg_func
            }).reset_index()

            # Rename columns
            pivot_df.columns = [group_column['name'], f"{agg_func.upper()}({value_column['name']})"]

            # Sort by value descending
            pivot_df = pivot_df.sort_values(by=pivot_df.columns[1], ascending=False)

            # Convert to structured data
            pivot_data = {
                "headers": list(pivot_df.columns),
                "rows": pivot_df.to_dict(orient='records')
            }

            agg_names = {
                'sum': 'Ð¡ÑƒÐ¼Ð¼Ð°',
                'mean': 'Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ',
                'count': 'ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾',
                'max': 'ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼',
                'min': 'ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼'
            }

            # Avoid duplication like "Ð¡ÑƒÐ¼Ð¼Ð° Ð¡ÑƒÐ¼Ð¼Ð°" when column name matches agg function name
            agg_name = agg_names.get(agg_func, agg_func)
            col_name = value_column['name']
            if agg_name.lower() in col_name.lower() or col_name.lower() in agg_name.lower():
                message = f"Ð¡Ð²Ð¾Ð´Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°: {col_name} Ð¿Ð¾ {group_column['name']}"
            else:
                message = f"Ð¡Ð²Ð¾Ð´Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°: {agg_name} {col_name} Ð¿Ð¾ {group_column['name']}"

            return {
                "action_type": "pivot_table",
                "pivot_data": pivot_data,
                "group_column": group_column['name'],
                "value_column": value_column['name'],
                "agg_func": agg_func,
                "message": message
            }

        except Exception as e:
            logger.error(f"[SimpleGPT] Error creating pivot: {e}")
            return None

    def _detect_csv_split_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ Ñ€Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ñ CSV/Ñ‚ÐµÐºÑÑ‚Ð° Ð¿Ð¾ ÑÑ‡ÐµÐ¹ÐºÐ°Ð¼.
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        - "Ñ€Ð°Ð·Ð±ÐµÐ¹ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ ÑÑ‡ÐµÐ¹ÐºÐ°Ð¼"
        - "Ñ€Ð°Ð·Ð´ÐµÐ»Ð¸ csv Ð¿Ð¾ ÑÑ‚Ð¾Ð»Ð±Ñ†Ð°Ð¼"
        - "text to columns"
        """
        query_lower = query.lower()
        
        # Check for CSV split keywords
        is_csv_split = any(kw in query_lower for kw in self.CSV_SPLIT_KEYWORDS)
        if not is_csv_split:
            return None
        
        logger.info(f"[SimpleGPT] CSV split action detected: {query}")
        logger.info(f"[SimpleGPT] CSV split - df.columns: {df.columns.tolist()}")
        logger.info(f"[SimpleGPT] CSV split - df.shape: {df.shape}")
        logger.info(f"[SimpleGPT] CSV split - df first row: {df.iloc[0].tolist() if len(df) > 0 else 'empty'}")
        
        # Detect delimiter from data
        delimiter = None
        first_row = df.iloc[0, 0] if len(df) > 0 and len(df.columns) > 0 else ''
        first_row_str = str(first_row)
        
        # Check common delimiters
        if ';' in first_row_str:
            delimiter = ';'
        elif ',' in first_row_str:
            delimiter = ','
        elif '	' in first_row_str:
            delimiter = '	'
        elif '|' in first_row_str:
            delimiter = '|'
        
        if not delimiter:
            logger.warning(f"[SimpleGPT] Could not detect delimiter in data")
            return None
        
        logger.info(f"[SimpleGPT] Detected delimiter: '{delimiter}'")
        
        # Split data
        try:
            import io
            # IMPORTANT: column_names contains the FIRST row from Google Sheets
            # which was separated as "headers" by frontend. We need to include it!
            
            # Start with first row (column_names) - it's actually the first data row
            all_data = []
            if column_names and len(column_names) == 1 and delimiter in str(column_names[0]):
                # First row is CSV text in single cell - add it
                all_data.append(str(column_names[0]))
            
            # Add remaining rows from df
            for idx, row in df.iterrows():
                row_str = str(row.iloc[0]) if len(row) > 0 else ''
                all_data.append(row_str)
            
            logger.info(f"[SimpleGPT] CSV split - total rows including header: {len(all_data)}")
            csv_text = chr(10).join(all_data)

            # Parse CSV - first row becomes headers
            split_df = pd.read_csv(io.StringIO(csv_text), sep=delimiter, header=0, dtype=str)

            # Use pandas-extracted headers
            headers = split_df.columns.tolist()
            logger.info(f"[SimpleGPT] CSV split - headers: {headers}")

            # All remaining rows are data
            rows = split_df.fillna('').to_dict('records')
            logger.info(f"[SimpleGPT] CSV split - data rows: {len(rows)}")
            
            structured_data = {
                'headers': headers,
                'rows': rows
            }
            
            message = f"""**âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ Ñ€Ð°Ð·Ð±Ð¸Ñ‚Ñ‹ Ð¿Ð¾ ÑÑ‡ÐµÐ¹ÐºÐ°Ð¼**

ðŸ“‹ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
â€¢ ÐšÐ¾Ð»Ð¾Ð½Ð¾Ðº: {len(headers)}
â€¢ Ð¡Ñ‚Ñ€Ð¾Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ…: {len(rows)}
â€¢ Ð Ð°Ð·Ð´ÐµÐ»Ð¸Ñ‚ÐµÐ»ÑŒ: '{delimiter}'
â€¢ ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸: {', '.join(headers[:5])}{'...' if len(headers) > 5 else ''}

ðŸ’¡ ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ ÐºÐ½Ð¾Ð¿ÐºÑƒ Ð½Ð¸Ð¶Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ."""
            
            return {
                'structured_data': structured_data,
                'original_rows': len(df),
                'new_rows': len(rows),
                'new_cols': len(headers),
                'delimiter': delimiter,
                'message': message
            }
            
        except Exception as e:
            logger.error(f"[SimpleGPT] CSV split error: {e}")
            return None

    def _detect_clean_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ….
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        - "ÑƒÐ´Ð°Ð»Ð¸ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹"
        - "ÑƒÐ´Ð°Ð»Ð¸ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸"
        - "Ð·Ð°Ð¿Ð¾Ð»Ð½Ð¸ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‡ÐµÐ¹ÐºÐ¸ Ð½ÑƒÐ»ÑÐ¼Ð¸"
        - "Ð¾Ñ‡Ð¸ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ"
        """
        query_lower = query.lower()

        # Check for clean keywords
        is_clean = any(kw in query_lower for kw in self.CLEAN_KEYWORDS)
        if not is_clean:
            return None

        logger.info(f"[SimpleGPT] Clean action detected: {query}")

        # Determine operation type
        operations = []

        # Check for duplicate removal
        if any(kw in query_lower for kw in self.CLEAN_OPERATIONS['duplicate']):
            operations.append('remove_duplicates')

        # Check for empty row removal
        if any(kw in query_lower for kw in self.CLEAN_OPERATIONS['empty_rows']):
            # Distinguish between "ÑƒÐ´Ð°Ð»Ð¸ Ð¿ÑƒÑÑ‚Ñ‹Ðµ" vs "Ð·Ð°Ð¿Ð¾Ð»Ð½Ð¸ Ð¿ÑƒÑÑ‚Ñ‹Ðµ"
            if any(w in query_lower for w in ['ÑƒÐ´Ð°Ð»Ð¸', 'ÑƒÐ±ÐµÑ€Ð¸', 'remove', 'delete']):
                operations.append('remove_empty_rows')
            elif any(w in query_lower for w in ['Ð·Ð°Ð¿Ð¾Ð»Ð½', 'fill', 'Ð·Ð°Ð¼ÐµÐ½']):
                operations.append('fill_empty')

        # Check for trimming whitespace
        if any(kw in query_lower for kw in self.CLEAN_OPERATIONS['trim']):
            operations.append('trim_whitespace')

        # Check for fill operation (if not already detected)
        if 'fill_empty' not in operations and any(kw in query_lower for kw in self.CLEAN_OPERATIONS['fill']):
            operations.append('fill_empty')

        # Default to all common operations if just "Ð¾Ñ‡Ð¸ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ"
        if not operations and any(w in query_lower for w in ['Ð¾Ñ‡Ð¸ÑÑ‚Ð¸', 'clean']):
            operations = ['remove_duplicates', 'remove_empty_rows', 'trim_whitespace']

        if not operations:
            return None

        # Detect fill value if applicable
        fill_value = None
        if 'fill_empty' in operations:
            # Check for specific fill values
            import re

            # "Ð½ÑƒÐ»ÑÐ¼Ð¸" / "0" / "zeros"
            if any(w in query_lower for w in ['Ð½ÑƒÐ»', 'zero', '0']):
                fill_value = 0
            # "Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹" / ""
            elif any(w in query_lower for w in ['ÑÑ‚Ñ€Ð¾Ðº', 'string', 'Ñ‚ÐµÐºÑÑ‚']):
                fill_value = ""
            # "ÑÑ€ÐµÐ´Ð½Ð¸Ð¼" / "mean" / "average"
            elif any(w in query_lower for w in ['ÑÑ€ÐµÐ´Ð½', 'mean', 'average', 'avg']):
                fill_value = "mean"
            # "Ð¼ÐµÐ´Ð¸Ð°Ð½Ð¾Ð¹" / "median"
            elif any(w in query_lower for w in ['Ð¼ÐµÐ´Ð¸Ð°Ð½', 'median']):
                fill_value = "median"
            # "Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¼" / "forward fill"
            elif any(w in query_lower for w in ['Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰', 'forward', 'ffill', 'Ð¿Ð¾ÑÐ»ÐµÐ´Ð½']):
                fill_value = "ffill"
            # Specific number
            number_match = re.search(r'(\d+(?:[.,]\d+)?)', query_lower)
            if number_match and fill_value is None:
                fill_value = float(number_match.group(1).replace(',', '.'))

            # Default to 0 if not specified
            if fill_value is None:
                fill_value = 0

        # Find target column if specified
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        # Execute cleaning and get preview
        try:
            cleaned_df = df.copy()
            original_rows = len(cleaned_df)
            changes = []

            for op in operations:
                if op == 'remove_duplicates':
                    before = len(cleaned_df)
                    if target_column:
                        cleaned_df = cleaned_df.drop_duplicates(subset=[cleaned_df.columns[target_column_index]])
                    else:
                        cleaned_df = cleaned_df.drop_duplicates()
                    removed = before - len(cleaned_df)
                    if removed > 0:
                        changes.append(f"ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {removed} Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²")

                elif op == 'remove_empty_rows':
                    before = len(cleaned_df)
                    if target_column:
                        cleaned_df = cleaned_df.dropna(subset=[cleaned_df.columns[target_column_index]])
                    else:
                        cleaned_df = cleaned_df.dropna(how='all')
                    removed = before - len(cleaned_df)
                    if removed > 0:
                        changes.append(f"ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {removed} Ð¿ÑƒÑÑ‚Ñ‹Ñ… ÑÑ‚Ñ€Ð¾Ðº")

                elif op == 'trim_whitespace':
                    # Trim string columns
                    str_cols = cleaned_df.select_dtypes(include=['object']).columns
                    for col in str_cols:
                        cleaned_df[col] = cleaned_df[col].apply(
                            lambda x: x.strip() if isinstance(x, str) else x
                        )
                    if len(str_cols) > 0:
                        changes.append(f"ÑƒÐ±Ñ€Ð°Ð½Ñ‹ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ Ð² {len(str_cols)} ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°Ñ…")

                elif op == 'fill_empty':
                    if target_column:
                        col = cleaned_df.columns[target_column_index]
                        empty_count = cleaned_df[col].isna().sum()
                        if fill_value == "mean":
                            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mean())
                        elif fill_value == "median":
                            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())
                        elif fill_value == "ffill":
                            cleaned_df[col] = cleaned_df[col].fillna(method='ffill')
                        else:
                            cleaned_df[col] = cleaned_df[col].fillna(fill_value)
                        if empty_count > 0:
                            changes.append(f"Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾ {empty_count} Ð¿ÑƒÑÑ‚Ñ‹Ñ… ÑÑ‡ÐµÐµÐº Ð² '{target_column}'")
                    else:
                        # Fill all numeric columns
                        num_cols = cleaned_df.select_dtypes(include=[np.number]).columns
                        total_filled = 0
                        for col in num_cols:
                            empty_count = cleaned_df[col].isna().sum()
                            total_filled += empty_count
                            if fill_value == "mean":
                                cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mean())
                            elif fill_value == "median":
                                cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())
                            elif fill_value == "ffill":
                                cleaned_df[col] = cleaned_df[col].fillna(method='ffill')
                            else:
                                cleaned_df[col] = cleaned_df[col].fillna(fill_value)
                        if total_filled > 0:
                            changes.append(f"Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾ {total_filled} Ð¿ÑƒÑÑ‚Ñ‹Ñ… ÑÑ‡ÐµÐµÐº")

            final_rows = len(cleaned_df)

            # Prepare result data
            cleaned_data = {
                "headers": list(cleaned_df.columns),
                "rows": cleaned_df.to_dict(orient='records')
            }

            # Build message
            if changes:
                message = "ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…: " + ", ".join(changes)
            else:
                message = "Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑƒÐ¶Ðµ Ñ‡Ð¸ÑÑ‚Ñ‹Ðµ, Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ"

            return {
                "action_type": "clean_data",
                "operations": operations,
                "fill_value": fill_value,
                "target_column": target_column,
                "original_rows": original_rows,
                "final_rows": final_rows,
                "cleaned_data": cleaned_data,
                "changes": changes,
                "message": message
            }

        except Exception as e:
            logger.error(f"[SimpleGPT] Error cleaning data: {e}")
            return None

    def _detect_validation_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð²Ñ‹Ð¿Ð°Ð´Ð°ÑŽÑ‰Ð¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº).
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        - "ÑÐ¾Ð·Ð´Ð°Ð¹ Ð²Ñ‹Ð¿Ð°Ð´Ð°ÑŽÑ‰Ð¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð² ÐºÐ¾Ð»Ð¾Ð½ÐºÐµ Ð¡Ñ‚Ð°Ñ‚ÑƒÑ"
        - "Ð´Ð¾Ð±Ð°Ð²ÑŒ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ: Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð”Ð°/ÐÐµÑ‚"
        - "Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð² ÐºÐ¾Ð»Ð¾Ð½ÐºÐµ ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ"
        """
        query_lower = query.lower()

        # Check for validation keywords
        is_validation = any(kw in query_lower for kw in self.VALIDATION_KEYWORDS)
        if not is_validation:
            return None

        logger.info(f"[SimpleGPT] Validation action detected: {query}")

        # Find target column
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        # Extract allowed values from query
        allowed_values = []

        # Pattern 1: "Ñ‚Ð¾Ð»ÑŒÐºÐ¾ X/Y/Z" or "Ñ‚Ð¾Ð»ÑŒÐºÐ¾ X, Y, Z"
        import re
        only_match = re.search(r'Ñ‚Ð¾Ð»ÑŒÐºÐ¾\s+([^.!?]+)', query_lower)
        if only_match:
            values_str = only_match.group(1)
            # Split by / or , or "Ð¸Ð»Ð¸"
            values = re.split(r'[/,]|\sÐ¸Ð»Ð¸\s|\sor\s', values_str)
            allowed_values = [v.strip() for v in values if v.strip()]

        # Pattern 2: "Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ: X, Y, Z" or "Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹: X, Y, Z"
        values_match = re.search(r'(?:Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ|Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹|options|values)[:\s]+([^.!?]+)', query_lower)
        if values_match and not allowed_values:
            values_str = values_match.group(1)
            values = re.split(r'[/,]|\sÐ¸Ð»Ð¸\s|\sor\s', values_str)
            allowed_values = [v.strip() for v in values if v.strip()]

        # Pattern 3: "Ð”Ð°/ÐÐµÑ‚" style in query
        if not allowed_values:
            # Look for slash-separated values
            slash_match = re.search(r'([Ð°-ÑÑ‘a-z0-9]+(?:/[Ð°-ÑÑ‘a-z0-9]+)+)', query_lower)
            if slash_match:
                allowed_values = slash_match.group(1).split('/')

        # If still no values and we have a target column, extract unique values from data
        if not allowed_values and target_column and target_column_index is not None:
            try:
                unique_values = df.iloc[:, target_column_index].dropna().unique()
                # Only use if reasonable number of unique values (< 20)
                if len(unique_values) <= 20:
                    allowed_values = [str(v) for v in unique_values]
                    logger.info(f"[SimpleGPT] Auto-extracted {len(allowed_values)} unique values from column")
            except Exception as e:
                logger.warning(f"[SimpleGPT] Could not extract unique values: {e}")

        if not target_column:
            # Try to find first categorical column
            for idx, col_name in enumerate(column_names):
                if idx < len(df.columns):
                    try:
                        unique_count = df.iloc[:, idx].nunique()
                        total_count = len(df)
                        # Categorical if less than 50% unique values and <= 20 unique
                        if unique_count <= 20 and unique_count < total_count * 0.5:
                            target_column = col_name
                            target_column_index = idx
                            if not allowed_values:
                                allowed_values = [str(v) for v in df.iloc[:, idx].dropna().unique()]
                            break
                    except:
                        pass

        if not target_column:
            logger.warning(f"[SimpleGPT] No target column found for validation")
            return None

        if not allowed_values:
            logger.warning(f"[SimpleGPT] No allowed values found for validation")
            return None

        # Capitalize first letter of each value for display
        allowed_values = [v.strip().capitalize() if v.strip() else v for v in allowed_values]

        # Build validation rule
        rule = {
            "column_index": target_column_index,
            "column_name": target_column,
            "validation_type": "ONE_OF_LIST",
            "allowed_values": allowed_values,
            "show_dropdown": True,
            "strict": True  # Reject invalid input
        }

        values_preview = ", ".join(allowed_values[:5])
        if len(allowed_values) > 5:
            values_preview += f" (+{len(allowed_values) - 5})"

        message = f"Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ '{target_column}': {values_preview}"

        return {
            "action_type": "data_validation",
            "rule": rule,
            "message": message
        }

    def _detect_highlight_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¸Ñ ÑÑ‚Ñ€Ð¾Ðº Ñ†Ð²ÐµÑ‚Ð¾Ð¼.
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        - "Ð²Ñ‹Ð´ÐµÐ»Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð³Ð´Ðµ Ð¡ÑƒÐ¼Ð¼Ð° > 100000"
        - "Ð¿Ð¾Ð´ÑÐ²ÐµÑ‚ÑŒ ÐºÑ€Ð°ÑÐ½Ñ‹Ð¼ Ð¾Ñ‚Ð¼ÐµÐ½Ñ‘Ð½Ð½Ñ‹Ðµ Ð·Ð°ÐºÐ°Ð·Ñ‹"
        - "Ð¾Ñ‚Ð¼ÐµÑ‚ÑŒ Ð·ÐµÐ»Ñ‘Ð½Ñ‹Ð¼ Ð¾Ð¿Ð»Ð°Ñ‡ÐµÐ½Ð½Ñ‹Ðµ"
        """
        query_lower = query.lower()

        # Check for highlight keywords FIRST
        is_highlight = any(kw in query_lower for kw in self.HIGHLIGHT_KEYWORDS)
        if not is_highlight:
            return None

        logger.info(f"[SimpleGPT] Highlight action detected: {query}")

        # Detect color from query
        highlight_color = '#FFFF00'  # Default yellow
        for color_key, color_hex in self.HIGHLIGHT_COLORS.items():
            if color_key in query_lower:
                highlight_color = color_hex
                logger.info(f"[SimpleGPT] Highlight color detected: {color_key} -> {color_hex}")
                break

        # Find target column and condition
        target_column = None
        target_column_index = None
        filter_value = None
        operator = '=='

        # Look for column mentions
        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        # Detect operator
        for op, patterns in self.FILTER_OPERATORS.items():
            for pattern in patterns:
                if pattern in query_lower:
                    operator = op
                    break
            if operator != '==':
                break

        # Extract value
        import re
        if operator not in ['empty', 'not_empty']:
            # Try to extract numeric value
            number_match = re.search(r'(\d+(?:[.,]\d+)?)', query_lower)
            if number_match:
                filter_value = float(number_match.group(1).replace(',', '.'))
            else:
                # Try to find text value (e.g., status names)
                status_words = ['Ð¾Ð¿Ð»Ð°Ñ‡ÐµÐ½', 'Ð¾Ñ‚Ð¼ÐµÐ½ÐµÐ½', 'Ð¾Ñ‚Ð¼ÐµÐ½Ñ‘Ð½', 'Ð´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½', 'Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚', 'Ð°ÐºÑ‚Ð¸Ð²Ð½',
                                'Ð½ÐµÐ°ÐºÑ‚Ð¸Ð²Ð½', 'Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½', 'Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½', 'Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½', 'Ð¾Ð¶Ð¸Ð´Ð°Ð½', 'vip']
                for status in status_words:
                    if status in query_lower:
                        filter_value = status
                        operator = 'contains'
                        break

        # Execute filter to find rows
        try:
            if target_column is None:
                # Try to find by status-like column
                for idx, col in enumerate(column_names):
                    col_lower = col.lower()
                    if any(x in col_lower for x in ['ÑÑ‚Ð°Ñ‚ÑƒÑ', 'status', 'ÑÐ¾ÑÑ‚Ð¾ÑÐ½']):
                        target_column = col
                        target_column_index = idx
                        break

            if target_column is None:
                logger.warning(f"[SimpleGPT] No target column found for highlight")
                return None

            filtered_df = df.copy()
            col = filtered_df.columns[target_column_index]

            if operator == 'empty':
                mask = filtered_df[col].isna() | (filtered_df[col] == '')
            elif operator == 'not_empty':
                mask = filtered_df[col].notna() & (filtered_df[col] != '')
            elif operator == 'contains' and filter_value:
                mask = filtered_df[col].astype(str).str.lower().str.contains(str(filter_value).lower(), na=False)
            elif filter_value is not None:
                try:
                    numeric_val = float(filter_value) if isinstance(filter_value, (int, float, str)) and str(filter_value).replace('.', '').replace('-', '').isdigit() else None
                    if numeric_val is not None:
                        col_numeric = pd.to_numeric(filtered_df[col], errors='coerce')
                        if operator == '>':
                            mask = col_numeric > numeric_val
                        elif operator == '<':
                            mask = col_numeric < numeric_val
                        elif operator == '>=':
                            mask = col_numeric >= numeric_val
                        elif operator == '<=':
                            mask = col_numeric <= numeric_val
                        elif operator == '!=':
                            mask = col_numeric != numeric_val
                        else:
                            mask = col_numeric == numeric_val
                    else:
                        str_col = filtered_df[col].astype(str).str.lower()
                        str_val = str(filter_value).lower()
                        if operator == '!=':
                            mask = str_col != str_val
                        else:
                            mask = str_col == str_val
                except:
                    str_col = filtered_df[col].astype(str).str.lower()
                    str_val = str(filter_value).lower()
                    mask = str_col.str.contains(str_val, na=False)
            else:
                logger.warning(f"[SimpleGPT] No filter condition for highlight")
                return None

            # Get row indices (1-indexed for spreadsheet)
            highlight_rows = [i + 2 for i in filtered_df[mask].index.tolist()]  # +2 for header + 1-indexed

            if not highlight_rows:
                logger.warning(f"[SimpleGPT] No rows matched highlight condition")
                return None

            # Build condition string
            op_display = {
                '==': '=', '!=': 'â‰ ', '>': '>', '<': '<', '>=': 'â‰¥', '<=': 'â‰¤',
                'contains': 'ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚', 'empty': 'Ð¿ÑƒÑÑ‚Ð¾', 'not_empty': 'Ð½Ðµ Ð¿ÑƒÑÑ‚Ð¾'
            }
            if operator in ['empty', 'not_empty']:
                condition_str = f"{target_column} {op_display.get(operator, operator)}"
            else:
                condition_str = f"{target_column} {op_display.get(operator, operator)} {filter_value}"

            message = f"Ð’Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¾ {len(highlight_rows)} ÑÑ‚Ñ€Ð¾Ðº Ð³Ð´Ðµ {condition_str}"

            return {
                "action_type": "highlight",
                "highlight_rows": highlight_rows,
                "highlight_color": highlight_color,
                "highlight_count": len(highlight_rows),
                "condition_str": condition_str,
                "message": message
            }

        except Exception as e:
            logger.error(f"[SimpleGPT] Error detecting highlight rows: {e}")
            return None

    def _detect_filter_action(self, query: str, column_names: List[str], df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ….
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        - "Ð¿Ð¾ÐºÐ°Ð¶Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð³Ð´Ðµ Ð¡Ñ‚Ð°Ñ‚ÑƒÑ = ÐÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹"
        - "Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐ¹ Ð¿Ð¾ Ð¦ÐµÐ½Ð° > 1000"
        - "Ð½Ð°Ð¹Ð´Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð³Ð´Ðµ Ð”Ð°Ñ‚Ð° Ð¿ÑƒÑÑ‚Ð°Ñ"
        """
        query_lower = query.lower()

        # Check for filter keywords
        is_filter = any(kw in query_lower for kw in self.FILTER_KEYWORDS)
        if not is_filter:
            return None

        logger.info(f"[SimpleGPT] Filter action detected: {query}")

        # Find target column
        target_column = None
        target_column_index = None

        for idx, col_name in enumerate(column_names):
            col_lower = col_name.lower()
            if col_lower in query_lower or col_name in query:
                target_column = col_name
                target_column_index = idx
                break
            # Partial match
            for word in col_lower.split():
                if len(word) > 2 and word in query_lower:
                    target_column = col_name
                    target_column_index = idx
                    break
            if target_column:
                break

        if not target_column:
            logger.warning(f"[SimpleGPT] No target column found for filter")
            return None

        # Detect operator and value
        import re
        operator = '=='
        filter_value = None

        # Check operators in order of specificity (longer patterns first)
        for op, patterns in self.FILTER_OPERATORS.items():
            for pattern in patterns:
                if pattern in query_lower:
                    operator = op
                    break
            if operator != '==':
                break

        # Extract value based on operator
        if operator in ['empty', 'not_empty']:
            filter_value = None
        else:
            # Try to extract numeric value
            number_match = re.search(r'(\d+(?:[.,]\d+)?)', query_lower)
            if number_match:
                filter_value = float(number_match.group(1).replace(',', '.'))
            else:
                # Try to extract text value after operator patterns
                value_patterns = [
                    r'(?:Ñ€Ð°Ð²Ð½Ð¾|=|Ñ€Ð°Ð²ÐµÐ½|is)\s+["\']?([^"\'.,!?]+)["\']?',
                    r'(?:ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚|contains)\s+["\']?([^"\'.,!?]+)["\']?',
                    r'(?:Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ|starts)\s+(?:Ñ|with)?\s*["\']?([^"\'.,!?]+)["\']?',
                ]
                for vp in value_patterns:
                    value_match = re.search(vp, query_lower)
                    if value_match:
                        filter_value = value_match.group(1).strip()
                        break

                # If still no value, try to find value after column name
                if filter_value is None:
                    col_pattern = re.escape(target_column.lower())
                    after_col_match = re.search(
                        rf'{col_pattern}\s*(?:[=<>!]+|Ñ€Ð°Ð²Ð½Ð¾|Ð±Ð¾Ð»ÑŒÑˆÐµ|Ð¼ÐµÐ½ÑŒÑˆÐµ|ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚)\s*["\']?([^\s"\'.,!?]+)',
                        query_lower
                    )
                    if after_col_match:
                        filter_value = after_col_match.group(1).strip()

        # Execute filter and get preview
        try:
            filtered_df = df.copy()
            original_rows = len(filtered_df)
            col = filtered_df.columns[target_column_index]

            if operator == 'empty':
                filtered_df = filtered_df[filtered_df[col].isna() | (filtered_df[col] == '')]
            elif operator == 'not_empty':
                filtered_df = filtered_df[filtered_df[col].notna() & (filtered_df[col] != '')]
            elif operator == 'contains' and filter_value:
                filtered_df = filtered_df[
                    filtered_df[col].astype(str).str.lower().str.contains(str(filter_value).lower(), na=False)
                ]
            elif operator == 'startswith' and filter_value:
                filtered_df = filtered_df[
                    filtered_df[col].astype(str).str.lower().str.startswith(str(filter_value).lower())
                ]
            elif operator == 'endswith' and filter_value:
                filtered_df = filtered_df[
                    filtered_df[col].astype(str).str.lower().str.endswith(str(filter_value).lower())
                ]
            elif filter_value is not None:
                # Numeric or exact match
                try:
                    numeric_val = float(filter_value) if isinstance(filter_value, (int, float, str)) and str(filter_value).replace('.', '').replace('-', '').isdigit() else None
                    if numeric_val is not None:
                        col_numeric = pd.to_numeric(filtered_df[col], errors='coerce')
                        if operator == '>':
                            filtered_df = filtered_df[col_numeric > numeric_val]
                        elif operator == '<':
                            filtered_df = filtered_df[col_numeric < numeric_val]
                        elif operator == '>=':
                            filtered_df = filtered_df[col_numeric >= numeric_val]
                        elif operator == '<=':
                            filtered_df = filtered_df[col_numeric <= numeric_val]
                        elif operator == '!=':
                            filtered_df = filtered_df[col_numeric != numeric_val]
                        else:  # ==
                            filtered_df = filtered_df[col_numeric == numeric_val]
                    else:
                        # String comparison
                        str_col = filtered_df[col].astype(str).str.lower()
                        str_val = str(filter_value).lower()
                        if operator == '!=':
                            filtered_df = filtered_df[str_col != str_val]
                        else:
                            filtered_df = filtered_df[str_col == str_val]
                except Exception as e:
                    logger.warning(f"[SimpleGPT] Filter comparison error: {e}")
                    # Fallback to string comparison
                    str_col = filtered_df[col].astype(str).str.lower()
                    str_val = str(filter_value).lower()
                    filtered_df = filtered_df[str_col == str_val]

            filtered_rows = len(filtered_df)

            # Prepare result data
            filtered_data = {
                "headers": list(filtered_df.columns),
                "rows": filtered_df.to_dict(orient='records')
            }

            # Build operator display
            op_display = {
                '==': '=', '!=': 'â‰ ', '>': '>', '<': '<', '>=': 'â‰¥', '<=': 'â‰¤',
                'contains': 'ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚', 'startswith': 'Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ñ', 'endswith': 'Ð·Ð°ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð½Ð°',
                'empty': 'Ð¿ÑƒÑÑ‚Ð¾', 'not_empty': 'Ð½Ðµ Ð¿ÑƒÑÑ‚Ð¾'
            }

            if operator in ['empty', 'not_empty']:
                condition_str = f"{target_column} {op_display.get(operator, operator)}"
            else:
                condition_str = f"{target_column} {op_display.get(operator, operator)} {filter_value}"

            message = f"Ð¤Ð¸Ð»ÑŒÑ‚Ñ€: {condition_str} â†’ {filtered_rows} Ð¸Ð· {original_rows} ÑÑ‚Ñ€Ð¾Ðº"

            return {
                "action_type": "filter_data",
                "column_name": target_column,
                "column_index": target_column_index,
                "operator": operator,
                "filter_value": filter_value,
                "original_rows": original_rows,
                "filtered_rows": filtered_rows,
                "filtered_data": filtered_data,
                "condition_str": condition_str,
                "message": message
            }

        except Exception as e:
            logger.error(f"[SimpleGPT] Error filtering data: {e}")
            return None

    async def process(
        self,
        query: str,
        df: pd.DataFrame,
        column_names: List[str],
        custom_context: Optional[str] = None,
        history: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°.
        """
        start_time = time.time()

        try:
            # 0. Check for direct actions (sort, format, etc.) - no GPT needed
            sort_action = self._detect_sort_action(query, column_names)
            if sort_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning sort action: {sort_action}")
                return {
                    "success": True,
                    "action_type": "sort",
                    "result_type": "action",
                    "sort_column": sort_action["column_name"],
                    "sort_column_index": sort_action["column_index"],
                    "sort_order": sort_action["sort_order"],
                    "summary": sort_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for freeze action
            freeze_action = self._detect_freeze_action(query)
            if freeze_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning freeze action: {freeze_action}")
                return {
                    "success": True,
                    "action_type": "freeze",
                    "result_type": "action",
                    "freeze_rows": freeze_action["freeze_rows"],
                    "freeze_columns": freeze_action["freeze_columns"],
                    "summary": freeze_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for format action
            format_action = self._detect_format_action(query)
            if format_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning format action: {format_action}")
                return {
                    "success": True,
                    "action_type": "format",
                    "result_type": "action",
                    "format_type": format_action["format_type"],
                    "target_row": format_action["target_row"],
                    "bold": format_action["bold"],
                    "background_color": format_action["background_color"],
                    "summary": format_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for chart action (needs df for column analysis)
            chart_action = self._detect_chart_action(query, column_names, df)
            if chart_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Chart action detected: {chart_action}")
                chart_result = {
                    "success": True,
                    "action_type": "chart",
                    "result_type": "action",
                    "chart_spec": chart_action["chart_spec"],
                    "summary": chart_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }
                logger.info(f"[SimpleGPT] Returning chart result with keys: {list(chart_result.keys())}")
                logger.info(f"[SimpleGPT] chart_result['chart_spec']: {chart_result.get('chart_spec')}")
                return chart_result

            # Check for conditional formatting action
            conditional_action = self._detect_conditional_format_action(query, column_names, df)
            if conditional_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning conditional format action: {conditional_action}")
                return {
                    "success": True,
                    "action_type": "conditional_format",
                    "result_type": "action",
                    "rule": conditional_action["rule"],
                    "summary": conditional_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for pivot table action
            pivot_action = self._detect_pivot_action(query, column_names, df)
            if pivot_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning pivot table action: {pivot_action}")
                return {
                    "success": True,
                    "action_type": "pivot_table",
                    "result_type": "action",
                    "pivot_data": pivot_action["pivot_data"],
                    "group_column": pivot_action["group_column"],
                    "value_column": pivot_action["value_column"],
                    "agg_func": pivot_action["agg_func"],
                    "summary": pivot_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for CSV split action (text to columns)
            csv_split_action = self._detect_csv_split_action(query, column_names, df)
            if csv_split_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning CSV split action")
                return {
                    "success": True,
                    "action_type": "csv_split",
                    "result_type": "action",
                    "structured_data": csv_split_action["structured_data"],
                    "original_rows": csv_split_action["original_rows"],
                    "new_rows": csv_split_action["new_rows"],
                    "new_cols": csv_split_action["new_cols"],
                    "delimiter": csv_split_action["delimiter"],
                    "summary": csv_split_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for data cleaning action
            clean_action = self._detect_clean_action(query, column_names, df)
            if clean_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning clean data action: {clean_action}")
                return {
                    "success": True,
                    "action_type": "clean_data",
                    "result_type": "action",
                    "operations": clean_action["operations"],
                    "fill_value": clean_action["fill_value"],
                    "target_column": clean_action["target_column"],
                    "original_rows": clean_action["original_rows"],
                    "final_rows": clean_action["final_rows"],
                    "cleaned_data": clean_action["cleaned_data"],
                    "changes": clean_action["changes"],
                    "summary": clean_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for data validation action
            validation_action = self._detect_validation_action(query, column_names, df)
            if validation_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning data validation action: {validation_action}")
                return {
                    "success": True,
                    "action_type": "data_validation",
                    "result_type": "action",
                    "rule": validation_action["rule"],
                    "summary": validation_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for highlight action BEFORE filter (to handle "Ð²Ñ‹Ð´ÐµÐ»Ð¸ Ð³Ð´Ðµ..." queries)
            highlight_action = self._detect_highlight_action(query, column_names, df)
            if highlight_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning highlight action: {highlight_action}")
                return {
                    "success": True,
                    "action_type": "highlight",
                    "result_type": "action",
                    "highlight_rows": highlight_action["highlight_rows"],
                    "highlight_color": highlight_action["highlight_color"],
                    "highlight_count": highlight_action["highlight_count"],
                    "highlight_message": highlight_action["message"],
                    "summary": highlight_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # Check for filter action
            filter_action = self._detect_filter_action(query, column_names, df)
            if filter_action:
                elapsed = time.time() - start_time
                logger.info(f"[SimpleGPT] Returning filter action: {filter_action}")
                return {
                    "success": True,
                    "action_type": "filter_data",
                    "result_type": "action",
                    "column_name": filter_action["column_name"],
                    "column_index": filter_action["column_index"],
                    "operator": filter_action["operator"],
                    "filter_value": filter_action["filter_value"],
                    "original_rows": filter_action["original_rows"],
                    "filtered_rows": filter_action["filtered_rows"],
                    "filtered_data": filter_action["filtered_data"],
                    "condition_str": filter_action["condition_str"],
                    "summary": filter_action["message"],
                    "processing_time": f"{elapsed:.2f}s",
                    "processor": "SimpleGPT v1.0 (direct action)"
                }

            # 1. Schema extraction
            logger.info(f"[SimpleGPT] Processing: {query[:50]}...")
            schema = self.schema_extractor.extract_schema(df)
            schema_prompt = self.schema_extractor.schema_to_prompt(schema)
            logger.info(f"[SimpleGPT] Schema: {schema['column_count']} cols, {schema['row_count']} rows")

            # 2. Generate and execute code (with retries)
            result = await self._generate_and_execute(
                query=query,
                df=df,
                schema_prompt=schema_prompt,
                custom_context=custom_context,
                history=history
            )

            if not result["success"]:
                return self._create_error_response(result.get("error", "Unknown error"), time.time() - start_time)

            # 3. Post-validation
            validation = await self._validate_result(query, result["result"])

            if validation == "BAD":
                logger.warning(f"[SimpleGPT] Post-validation failed, retrying with clarification...")
                # Retry with explicit clarification
                result = await self._generate_and_execute(
                    query=query,
                    df=df,
                    schema_prompt=schema_prompt,
                    custom_context=custom_context,
                    history=history,
                    clarification="ÐŸÑ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð½Ðµ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ð» Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ. Ð£Ð±ÐµÐ´Ð¸ÑÑŒ Ñ‡Ñ‚Ð¾ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑˆÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ð´Ð°Ð½Ð½Ñ‹Ñ…: ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð»Ñ 'ÐºÐ°ÐºÐ¸Ðµ', Ñ‡Ð¸ÑÐ»Ð¾ Ð´Ð»Ñ 'ÑÐºÐ¾Ð»ÑŒÐºÐ¾', DataFrame Ð´Ð»Ñ 'Ð¿Ð¾ÐºÐ°Ð¶Ð¸'."
                )

            # 4. Format response
            elapsed = time.time() - start_time
            formatted_result = self._format_result(result["result"])
            result_type = self._get_result_type(result["result"])

            # Use explanation from code if available, otherwise generate summary
            explanation = result.get("explanation", "")
            if explanation:
                summary = explanation
                logger.info(f"[SimpleGPT] Using explanation from code: {explanation[:100]}...")
            else:
                summary = self._generate_summary(result["result"], result_type, query)

            response = {
                "success": True,
                "result": formatted_result,
                "result_type": result_type,
                "summary": summary,
                "code": result.get("code"),
                "processing_time": f"{elapsed:.2f}s",
                "processor": "SimpleGPT v1.0",
                "validation": validation
            }

            # Check if this is a highlight query
            query_lower = query.lower()
            is_highlight_query = any(kw in query_lower for kw in ['Ð²Ñ‹Ð´ÐµÐ»Ð¸', 'Ð²Ñ‹Ð´ÐµÐ»Ð¸Ñ‚ÑŒ', 'Ð¿Ð¾Ð´ÑÐ²ÐµÑ‚Ð¸', 'Ð¿Ð¾Ð´ÑÐ²ÐµÑ‚Ð¸Ñ‚ÑŒ', 'highlight', 'mark'])

            if is_highlight_query:
                logger.info(f"[SimpleGPT] Highlight query detected: {query[:50]}")
                # Extract row indices from the result for highlighting
                highlight_rows = self._extract_highlight_rows(result["result"])
                if highlight_rows:
                    response["highlight_rows"] = highlight_rows
                    response["highlighted_count"] = len(highlight_rows)
                    response["highlight_color"] = "#FFFF00"  # Yellow
                    response["highlight_message"] = f"Ð’Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¾ {len(highlight_rows)} ÑÑ‚Ñ€Ð¾Ðº"
                    response["result_type"] = "highlight"
                    logger.info(f"[SimpleGPT] Generated highlight_rows: {highlight_rows[:10]}... (total: {len(highlight_rows)})")

            # Add structured_data for tables/lists (only if NOT highlight query)
            if not is_highlight_query and result_type == "table" and isinstance(formatted_result, list):
                # Extract headers from first row keys (rows are dicts from DataFrame)
                headers = list(formatted_result[0].keys()) if formatted_result else []
                response["structured_data"] = {
                    "headers": headers,
                    "rows": formatted_result,
                    "display_mode": "sidebar_only" if len(formatted_result) <= 20 else "create_sheet"
                }
            elif result_type == "list" and isinstance(formatted_result, list):
                response["key_findings"] = formatted_result

            return response

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"[SimpleGPT] Error: {str(e)}")
            return self._create_error_response(str(e), elapsed)

    async def _generate_and_execute(
        self,
        query: str,
        df: pd.DataFrame,
        schema_prompt: str,
        custom_context: Optional[str] = None,
        history: List[Dict[str, Any]] = None,
        clarification: Optional[str] = None,
        previous_error: Optional[str] = None
    ) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ ÐºÐ¾Ð´ Ñ retry."""

        for attempt in range(self.MAX_RETRIES + 1):
            # Generate code
            code = await self._generate_code(
                query=query,
                schema_prompt=schema_prompt,
                custom_context=custom_context,
                history=history,
                clarification=clarification,
                previous_error=previous_error
            )

            if not code:
                return {"success": False, "error": "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð´"}

            # Validate code safety
            is_safe, safety_error = self._validate_code_safety(code)
            if not is_safe:
                previous_error = f"ÐÐµÐ±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ ÐºÐ¾Ð´: {safety_error}"
                continue

            # Execute code
            try:
                exec_result = self._execute_code(code, df)
                return {"success": True, "result": exec_result['result'], "explanation": exec_result.get('explanation', ''), "code": code}
            except Exception as e:
                previous_error = f"{type(e).__name__}: {str(e)}"
                logger.warning(f"[SimpleGPT] Attempt {attempt + 1} failed: {previous_error}")
                continue

        return {"success": False, "error": previous_error}

    async def _generate_code(
        self,
        query: str,
        schema_prompt: str,
        custom_context: Optional[str] = None,
        history: List[Dict[str, Any]] = None,
        clarification: Optional[str] = None,
        previous_error: Optional[str] = None
    ) -> Optional[str]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Pandas ÐºÐ¾Ð´ Ñ‡ÐµÑ€ÐµÐ· GPT-4o."""

        user_prompt = f"""Ð¡Ð¥Ð•ÐœÐ Ð”ÐÐÐÐ«Ð¥:
{schema_prompt}

Ð—ÐÐŸÐ ÐžÐ¡: {query}
"""

        # Build history context if available
        history_context = ""
        if history and len(history) > 0:
            history_context = "\nÐ˜Ð¡Ð¢ÐžÐ Ð˜Ð¯ Ð ÐÐ—Ð“ÐžÐ’ÐžÐ Ð (Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹):\n"
            for i, item in enumerate(history[-5:], 1):
                prev_query = item.get('query', '')
                prev_response = item.get('response', '')
                if prev_query:
                    history_context += f"{i}. Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {prev_query}\n"
                    if prev_response:
                        resp_str = str(prev_response)
                        history_context += f"   ÐžÑ‚Ð²ÐµÑ‚: {resp_str[:150]}...\n" if len(resp_str) > 150 else f"   ÐžÑ‚Ð²ÐµÑ‚: {resp_str}\n"
            history_context += "Ð’ÐÐ–ÐÐž: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð½ÑÑ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ñ‚Ð¸Ð¿Ð° 'Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ?' Ð¸Ð»Ð¸ 'Ð° ÐŸÐµÑ‚Ñ€Ð¾Ð²?'\n"
            logger.info(f"[SimpleGPT] Added conversation history: {len(history)} messages")

        user_prompt = f"""Ð¡Ð¥Ð•ÐœÐ Ð”ÐÐÐÐ«Ð¥:
{schema_prompt}
{history_context}
Ð—ÐÐŸÐ ÐžÐ¡: {query}
"""
        if custom_context:
            user_prompt += f"""
Ð ÐžÐ›Ð¬ ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¯: {custom_context}
Ð’ÐÐ–ÐÐž: Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ð¹ Ñ€Ð¾Ð»ÑŒ Ð² explanation! Ð¤Ð¾ÐºÑƒÑÐ¸Ñ€ÑƒÐ¹ÑÑ Ð½Ð° Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ñ… Ð²Ð°Ð¶Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÑ‚Ð¾Ð¹ Ñ€Ð¾Ð»Ð¸.
"""

        if clarification:
            user_prompt += f"\nÐ’ÐÐ–ÐÐž: {clarification}\n"

        if previous_error:
            user_prompt += f"\nÐŸÐ Ð•Ð”Ð«Ð”Ð£Ð©ÐÐ¯ ÐžÐ¨Ð˜Ð‘ÐšÐ (Ð¸Ð·Ð±ÐµÐ³Ð°Ð¹ ÐµÑ‘): {previous_error}\n"

        try:
            response = await self.client.chat.completions.create(
                model=self.MODEL,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )

            content = response.choices[0].message.content

            # Extract code from markdown
            code_match = re.search(r'```python\s*(.*?)\s*```', content, re.DOTALL)
            if code_match:
                return code_match.group(1).strip()

            # Try without markdown
            if 'result' in content and '=' in content:
                return content.strip()

            return None

        except Exception as e:
            logger.error(f"[SimpleGPT] Code generation error: {e}")
            return None

    async def _validate_result(self, query: str, result: Any) -> str:
        """Post-validation: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°."""

        # Format result for validation
        result_str = self._format_for_validation(result)

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",  # Cheaper model for validation
                messages=[
                    {"role": "user", "content": self.VALIDATION_PROMPT.format(
                        query=query,
                        result=result_str
                    )}
                ],
                temperature=0,
                max_tokens=10
            )

            answer = response.choices[0].message.content.strip().upper()
            return "OK" if "OK" in answer else "BAD"

        except Exception as e:
            logger.warning(f"[SimpleGPT] Validation error: {e}")
            return "OK"  # Default to OK if validation fails

    def _validate_code_safety(self, code: str) -> tuple:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ ÐºÐ¾Ð´Ð°."""
        for pattern in self.FORBIDDEN_PATTERNS:
            if re.search(pattern, code, re.IGNORECASE):
                return False, f"Forbidden pattern: {pattern}"

        # Check AST
        try:
            ast.parse(code)
        except SyntaxError as e:
            return False, f"Syntax error: {e}"

        return True, None

    def _execute_code(self, code: str, df: pd.DataFrame) -> dict:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ ÐºÐ¾Ð´ Ð² sandbox. Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ dict Ñ result Ð¸ explanation."""

        # Create safe namespace
        namespace = {
            'df': df.copy(),
            'pd': pd,
            'np': np,
            'result': None,
            'explanation': None,
            'datetime': __import__('datetime'),
            'timedelta': __import__('datetime').timedelta,
            're': __import__('re'),
            'math': __import__('math'),
        }

        exec(code, namespace)

        result = namespace.get('result')
        if result is None:
            raise ValueError("ÐšÐ¾Ð´ Ð½Ðµ Ð²ÐµÑ€Ð½ÑƒÐ» Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ (result = None)")

        explanation = namespace.get('explanation', '')

        return {'result': result, 'explanation': explanation}

    def _format_result(self, result: Any) -> Any:
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð´Ð»Ñ JSON."""

        if isinstance(result, pd.DataFrame):
            # Convert to list of dicts
            return result.to_dict(orient='records')
        elif isinstance(result, pd.Series):
            return result.tolist()
        elif isinstance(result, np.ndarray):
            return result.tolist()
        elif isinstance(result, (np.integer, np.floating)):
            return float(result)
        elif isinstance(result, list):
            return result
        else:
            return result

    def _format_for_validation(self, result: Any) -> str:
        """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸."""

        if isinstance(result, pd.DataFrame):
            if len(result) > 5:
                return f"DataFrame Ñ {len(result)} ÑÑ‚Ñ€Ð¾ÐºÐ°Ð¼Ð¸. ÐŸÐµÑ€Ð²Ñ‹Ðµ 3: {result.head(3).to_dict(orient='records')}"
            return str(result.to_dict(orient='records'))
        elif isinstance(result, (list, pd.Series)):
            items = list(result)[:10]
            return f"Ð¡Ð¿Ð¸ÑÐ¾Ðº: {items}" + (f" (Ð²ÑÐµÐ³Ð¾ {len(result)})" if len(result) > 10 else "")
        elif isinstance(result, (int, float, np.integer, np.floating)):
            return f"Ð§Ð¸ÑÐ»Ð¾: {result}"
        else:
            return str(result)[:500]

    def _get_result_type(self, result: Any) -> str:
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ Ñ‚Ð¸Ð¿ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°."""

        if isinstance(result, pd.DataFrame):
            return "table"
        elif isinstance(result, (list, pd.Series)):
            return "list"
        elif isinstance(result, (int, float, np.integer, np.floating)):
            return "number"
        else:
            return "text"

    def _generate_summary(self, result: Any, result_type: str, query: str) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾-Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ð¾Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°."""

        if result_type == "number":
            # Ð”Ð»Ñ Ñ‡Ð¸ÑÐµÐ» - Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ
            if isinstance(result, float):
                return f"{result:,.2f}".replace(",", " ")
            return str(result)

        elif result_type == "list":
            # Ð”Ð»Ñ ÑÐ¿Ð¸ÑÐºÐ¾Ð² - Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            items = list(result) if isinstance(result, pd.Series) else result
            if len(items) == 0:
                return "ÐÐ¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾"
            elif len(items) <= 5:
                return ", ".join(str(item) for item in items)
            else:
                first_items = ", ".join(str(item) for item in items[:5])
                return f"{first_items} (Ð¸ ÐµÑ‰Ñ‘ {len(items) - 5})"

        elif result_type == "table":
            # Ð”Ð»Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ† - ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð¾Ðº
            if isinstance(result, pd.DataFrame):
                return f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(result)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
            elif isinstance(result, list):
                return f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(result)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
            return "Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…"

        else:
            # Ð¢ÐµÐºÑÑ‚
            return str(result)[:200] if result else "Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½"

    def _extract_highlight_rows(self, result: Any) -> List[int]:
        """
        Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð½Ð¾Ð¼ÐµÑ€Ð° ÑÑ‚Ñ€Ð¾Ðº Ð´Ð»Ñ Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°.
        Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ list[int] Ñ Ð½Ð¾Ð¼ÐµÑ€Ð°Ð¼Ð¸ ÑÑ‚Ñ€Ð¾Ðº (1-based Ð´Ð»Ñ Google Sheets, +1 Ð´Ð»Ñ header).
        """
        try:
            if isinstance(result, pd.DataFrame):
                # Get original DataFrame indices and convert to Google Sheets row numbers
                # +2 because: +1 for 1-based indexing, +1 for header row
                indices = result.index.tolist()
                row_numbers = [int(idx) + 2 for idx in indices]
                logger.info(f"[SimpleGPT] Extracted {len(row_numbers)} row indices from DataFrame")
                return row_numbers
            elif isinstance(result, pd.Series):
                # Series with row indices
                indices = result.index.tolist()
                row_numbers = [int(idx) + 2 for idx in indices]
                return row_numbers
            elif isinstance(result, list):
                # If result is a list of row numbers
                if all(isinstance(x, (int, np.integer)) for x in result):
                    return [int(x) + 2 for x in result]
                # If result is list of dicts (from DataFrame.to_dict), can't extract indices
                return []
            else:
                return []
        except Exception as e:
            logger.error(f"[SimpleGPT] Error extracting highlight rows: {e}")
            return []

    def _create_error_response(self, error: str, elapsed: float) -> Dict[str, Any]:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ð± Ð¾ÑˆÐ¸Ð±ÐºÐµ."""
        return {
            "success": False,
            "error": error,
            "processing_time": f"{elapsed:.2f}s",
            "processor": "SimpleGPT v1.0"
        }


# Singleton
_processor = None

def get_simple_gpt_processor() -> SimpleGPTProcessor:
    global _processor
    if _processor is None:
        _processor = SimpleGPTProcessor()
    return _processor
